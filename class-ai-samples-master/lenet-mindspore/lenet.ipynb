{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc48f5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import mindspore.dataset as ds\n",
    "import mindspore.nn as nn\n",
    "from mindspore.train import Model\n",
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor, TimeMonitor\n",
    "from mindspore.train.serialization import load_checkpoint, load_param_into_net\n",
    "from mindspore.common.initializer import Normal\n",
    "from mindspore import context\n",
    "import mindspore.dataset.vision.c_transforms as CV\n",
    "import mindspore.dataset.transforms.c_transforms as C\n",
    "from mindspore.dataset.vision import Inter\n",
    "from mindspore.nn import Accuracy\n",
    "from mindspore.common import dtype as mstype\n",
    "from mindspore.nn import SoftmaxCrossEntropyWithLogits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e125b1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data_path, batch_size=32, repeat_size=1,\n",
    "                   num_parallel_workers=1):\n",
    "    \"\"\" create dataset for train or test\n",
    "    Args:\n",
    "        data_path: Data path\n",
    "        batch_size: The number of data records in each group\n",
    "        repeat_size: The number of replicated data records\n",
    "        num_parallel_workers: The number of parallel workers\n",
    "    \"\"\"\n",
    "    # define dataset\n",
    "    mnist_ds = ds.MnistDataset(data_path)\n",
    "\n",
    "    # define operation parameters\n",
    "    resize_height, resize_width = 32, 32\n",
    "    rescale = 1.0 / 255.0\n",
    "    shift = 0.0\n",
    "    rescale_nml = 1 / 0.3081\n",
    "    shift_nml = -1 * 0.1307 / 0.3081\n",
    "\n",
    "    # define map operations\n",
    "    resize_op = CV.Resize((resize_height, resize_width), interpolation=Inter.LINEAR)  # Resize images to (32, 32)\n",
    "    rescale_nml_op = CV.Rescale(rescale_nml, shift_nml) # normalize images\n",
    "    rescale_op = CV.Rescale(rescale, shift) # rescale images\n",
    "    hwc2chw_op = CV.HWC2CHW() # change shape from (height, width, channel) to (channel, height, width) to fit network.\n",
    "    type_cast_op = C.TypeCast(mstype.int32) # change data type of label to int32 to fit network\n",
    "\n",
    "    # apply map operations on images\n",
    "    mnist_ds = mnist_ds.map(operations=type_cast_op, input_columns=\"label\", num_parallel_workers=num_parallel_workers)\n",
    "    mnist_ds = mnist_ds.map(operations=resize_op, input_columns=\"image\", num_parallel_workers=num_parallel_workers)\n",
    "    mnist_ds = mnist_ds.map(operations=rescale_op, input_columns=\"image\", num_parallel_workers=num_parallel_workers)\n",
    "    mnist_ds = mnist_ds.map(operations=rescale_nml_op, input_columns=\"image\", num_parallel_workers=num_parallel_workers)\n",
    "    mnist_ds = mnist_ds.map(operations=hwc2chw_op, input_columns=\"image\", num_parallel_workers=num_parallel_workers)\n",
    "\n",
    "    # apply DatasetOps\n",
    "    buffer_size = 10000\n",
    "    mnist_ds = mnist_ds.shuffle(buffer_size=buffer_size)  # 10000 as in LeNet train script\n",
    "    mnist_ds = mnist_ds.batch(batch_size, drop_remainder=True)\n",
    "    mnist_ds = mnist_ds.repeat(repeat_size)\n",
    "\n",
    "    return mnist_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1094d3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Cell):\n",
    "    \"\"\"Lenet network structure.\"\"\"\n",
    "    # define the operator required\n",
    "    def __init__(self, num_class=10, num_channel=1):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_channel, 6, 5, pad_mode='valid')\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, pad_mode='valid')\n",
    "        self.fc1 = nn.Dense(16 * 5 * 5, 120, weight_init=Normal(0.01))\n",
    "        self.dp1 = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Dense(120, 84, weight_init=Normal(0.01))\n",
    "        self.dp2 = nn.Dropout(p=0.5)\n",
    "        self.fc3 = nn.Dense(84, num_class, weight_init=Normal(0.01))\n",
    "        self.dp3 = nn.Dropout(p=0.1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    # use the preceding operators to construct networks\n",
    "    def construct(self, x):\n",
    "        x = self.max_pool2d(self.relu(self.conv1(x)))\n",
    "        x = self.max_pool2d(self.relu(self.conv2(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7bd97d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(network_model, epoch_size, data_path, repeat_size, ckpoint_cb, sink_mode):\n",
    "    \"\"\"Define the training method.\"\"\"\n",
    "    print(\"============== Starting Training ==============\")\n",
    "    # load training dataset\n",
    "    ds_train = create_dataset(os.path.join(data_path, \"train\"), 32, repeat_size)\n",
    "    network_model.train(epoch_size, ds_train, callbacks=[ckpoint_cb, LossMonitor()], dataset_sink_mode=sink_mode)\n",
    "\n",
    "\n",
    "def test_net(network, network_model, data_path):\n",
    "    \"\"\"Define the evaluation method.\"\"\"\n",
    "    print(\"============== Starting Testing ==============\")\n",
    "    # load the saved model for evaluation\n",
    "    param_dict = load_checkpoint(\"checkpoint_lenet-1_1875.ckpt\")\n",
    "    # load parameter to the network\n",
    "    load_param_into_net(network, param_dict)\n",
    "    # load testing dataset\n",
    "    ds_eval = create_dataset(os.path.join(data_path, \"test\"))\n",
    "    acc = network_model.eval(ds_eval, dataset_sink_mode=False)\n",
    "    print(\"============== Accuracy:{} ==============\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dce7c334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(8186:8296894272,MainProcess):2024-10-06-18:32:37.973.87 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(8186:8296894272,MainProcess):2024-10-06-18:32:37.976.90 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(8186:8296894272,MainProcess):2024-10-06-18:32:37.979.07 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(8186:8296894272,MainProcess):2024-10-06-18:32:37.981.57 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(8186:8296894272,MainProcess):2024-10-06-18:32:37.985.80 [mindspore/dataset/core/validator_helpers.py:744] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Starting Training ==============\n",
      "epoch: 1 step: 1, loss is 2.3088252544403076\n",
      "epoch: 1 step: 2, loss is 2.2897584438323975\n",
      "epoch: 1 step: 3, loss is 2.3080296516418457\n",
      "epoch: 1 step: 4, loss is 2.304595470428467\n",
      "epoch: 1 step: 5, loss is 2.2742886543273926\n",
      "epoch: 1 step: 6, loss is 2.3123981952667236\n",
      "epoch: 1 step: 7, loss is 2.3051247596740723\n",
      "epoch: 1 step: 8, loss is 2.3067409992218018\n",
      "epoch: 1 step: 9, loss is 2.2939302921295166\n",
      "epoch: 1 step: 10, loss is 2.2891926765441895\n",
      "epoch: 1 step: 11, loss is 2.309209108352661\n",
      "epoch: 1 step: 12, loss is 2.2934112548828125\n",
      "epoch: 1 step: 13, loss is 2.3137929439544678\n",
      "epoch: 1 step: 14, loss is 2.3012139797210693\n",
      "epoch: 1 step: 15, loss is 2.299746036529541\n",
      "epoch: 1 step: 16, loss is 2.315131425857544\n",
      "epoch: 1 step: 17, loss is 2.303494930267334\n",
      "epoch: 1 step: 18, loss is 2.307297706604004\n",
      "epoch: 1 step: 19, loss is 2.2991225719451904\n",
      "epoch: 1 step: 20, loss is 2.2987964153289795\n",
      "epoch: 1 step: 21, loss is 2.3101563453674316\n",
      "epoch: 1 step: 22, loss is 2.2877399921417236\n",
      "epoch: 1 step: 23, loss is 2.3044216632843018\n",
      "epoch: 1 step: 24, loss is 2.2742578983306885\n",
      "epoch: 1 step: 25, loss is 2.3001019954681396\n",
      "epoch: 1 step: 26, loss is 2.3125016689300537\n",
      "epoch: 1 step: 27, loss is 2.2901055812835693\n",
      "epoch: 1 step: 28, loss is 2.308908700942993\n",
      "epoch: 1 step: 29, loss is 2.310936450958252\n",
      "epoch: 1 step: 30, loss is 2.3072829246520996\n",
      "epoch: 1 step: 31, loss is 2.3075387477874756\n",
      "epoch: 1 step: 32, loss is 2.307187080383301\n",
      "epoch: 1 step: 33, loss is 2.2998204231262207\n",
      "epoch: 1 step: 34, loss is 2.3086941242218018\n",
      "epoch: 1 step: 35, loss is 2.2895262241363525\n",
      "epoch: 1 step: 36, loss is 2.3100030422210693\n",
      "epoch: 1 step: 37, loss is 2.3151843547821045\n",
      "epoch: 1 step: 38, loss is 2.3007969856262207\n",
      "epoch: 1 step: 39, loss is 2.307227611541748\n",
      "epoch: 1 step: 40, loss is 2.2964425086975098\n",
      "epoch: 1 step: 41, loss is 2.303203582763672\n",
      "epoch: 1 step: 42, loss is 2.290327787399292\n",
      "epoch: 1 step: 43, loss is 2.2870664596557617\n",
      "epoch: 1 step: 44, loss is 2.3052563667297363\n",
      "epoch: 1 step: 45, loss is 2.294072151184082\n",
      "epoch: 1 step: 46, loss is 2.307734251022339\n",
      "epoch: 1 step: 47, loss is 2.3072550296783447\n",
      "epoch: 1 step: 48, loss is 2.2990193367004395\n",
      "epoch: 1 step: 49, loss is 2.2962496280670166\n",
      "epoch: 1 step: 50, loss is 2.2937142848968506\n",
      "epoch: 1 step: 51, loss is 2.30648136138916\n",
      "epoch: 1 step: 52, loss is 2.297285795211792\n",
      "epoch: 1 step: 53, loss is 2.302544355392456\n",
      "epoch: 1 step: 54, loss is 2.310554265975952\n",
      "epoch: 1 step: 55, loss is 2.315476417541504\n",
      "epoch: 1 step: 56, loss is 2.3068761825561523\n",
      "epoch: 1 step: 57, loss is 2.290168523788452\n",
      "epoch: 1 step: 58, loss is 2.2905986309051514\n",
      "epoch: 1 step: 59, loss is 2.303537368774414\n",
      "epoch: 1 step: 60, loss is 2.3038406372070312\n",
      "epoch: 1 step: 61, loss is 2.305978775024414\n",
      "epoch: 1 step: 62, loss is 2.292711019515991\n",
      "epoch: 1 step: 63, loss is 2.3251891136169434\n",
      "epoch: 1 step: 64, loss is 2.2966086864471436\n",
      "epoch: 1 step: 65, loss is 2.307189702987671\n",
      "epoch: 1 step: 66, loss is 2.3068885803222656\n",
      "epoch: 1 step: 67, loss is 2.2940104007720947\n",
      "epoch: 1 step: 68, loss is 2.3071987628936768\n",
      "epoch: 1 step: 69, loss is 2.3119640350341797\n",
      "epoch: 1 step: 70, loss is 2.3012561798095703\n",
      "epoch: 1 step: 71, loss is 2.304791212081909\n",
      "epoch: 1 step: 72, loss is 2.287170171737671\n",
      "epoch: 1 step: 73, loss is 2.2966315746307373\n",
      "epoch: 1 step: 74, loss is 2.297553062438965\n",
      "epoch: 1 step: 75, loss is 2.2938382625579834\n",
      "epoch: 1 step: 76, loss is 2.2948596477508545\n",
      "epoch: 1 step: 77, loss is 2.308913469314575\n",
      "epoch: 1 step: 78, loss is 2.2918970584869385\n",
      "epoch: 1 step: 79, loss is 2.2983875274658203\n",
      "epoch: 1 step: 80, loss is 2.3030686378479004\n",
      "epoch: 1 step: 81, loss is 2.3088595867156982\n",
      "epoch: 1 step: 82, loss is 2.301910638809204\n",
      "epoch: 1 step: 83, loss is 2.3058934211730957\n",
      "epoch: 1 step: 84, loss is 2.3090288639068604\n",
      "epoch: 1 step: 85, loss is 2.2996740341186523\n",
      "epoch: 1 step: 86, loss is 2.307758331298828\n",
      "epoch: 1 step: 87, loss is 2.306612014770508\n",
      "epoch: 1 step: 88, loss is 2.3042376041412354\n",
      "epoch: 1 step: 89, loss is 2.3106954097747803\n",
      "epoch: 1 step: 90, loss is 2.2985928058624268\n",
      "epoch: 1 step: 91, loss is 2.3010001182556152\n",
      "epoch: 1 step: 92, loss is 2.3005611896514893\n",
      "epoch: 1 step: 93, loss is 2.3115289211273193\n",
      "epoch: 1 step: 94, loss is 2.288832902908325\n",
      "epoch: 1 step: 95, loss is 2.2972683906555176\n",
      "epoch: 1 step: 96, loss is 2.313241481781006\n",
      "epoch: 1 step: 97, loss is 2.292677164077759\n",
      "epoch: 1 step: 98, loss is 2.300677537918091\n",
      "epoch: 1 step: 99, loss is 2.3165624141693115\n",
      "epoch: 1 step: 100, loss is 2.2953407764434814\n",
      "epoch: 1 step: 101, loss is 2.29803204536438\n",
      "epoch: 1 step: 102, loss is 2.3046512603759766\n",
      "epoch: 1 step: 103, loss is 2.3161797523498535\n",
      "epoch: 1 step: 104, loss is 2.2890655994415283\n",
      "epoch: 1 step: 105, loss is 2.299975872039795\n",
      "epoch: 1 step: 106, loss is 2.3025972843170166\n",
      "epoch: 1 step: 107, loss is 2.306049346923828\n",
      "epoch: 1 step: 108, loss is 2.3083581924438477\n",
      "epoch: 1 step: 109, loss is 2.309586524963379\n",
      "epoch: 1 step: 110, loss is 2.3087830543518066\n",
      "epoch: 1 step: 111, loss is 2.3030591011047363\n",
      "epoch: 1 step: 112, loss is 2.3065333366394043\n",
      "epoch: 1 step: 113, loss is 2.277735948562622\n",
      "epoch: 1 step: 114, loss is 2.3042399883270264\n",
      "epoch: 1 step: 115, loss is 2.289066791534424\n",
      "epoch: 1 step: 116, loss is 2.291191816329956\n",
      "epoch: 1 step: 117, loss is 2.298858404159546\n",
      "epoch: 1 step: 118, loss is 2.301435708999634\n",
      "epoch: 1 step: 119, loss is 2.3058314323425293\n",
      "epoch: 1 step: 120, loss is 2.306480646133423\n",
      "epoch: 1 step: 121, loss is 2.2732133865356445\n",
      "epoch: 1 step: 122, loss is 2.300598382949829\n",
      "epoch: 1 step: 123, loss is 2.299525260925293\n",
      "epoch: 1 step: 124, loss is 2.303725242614746\n",
      "epoch: 1 step: 125, loss is 2.2858903408050537\n",
      "epoch: 1 step: 126, loss is 2.3065600395202637\n",
      "epoch: 1 step: 127, loss is 2.2812747955322266\n",
      "epoch: 1 step: 128, loss is 2.3079206943511963\n",
      "epoch: 1 step: 129, loss is 2.2866311073303223\n",
      "epoch: 1 step: 130, loss is 2.320984363555908\n",
      "epoch: 1 step: 131, loss is 2.3072705268859863\n",
      "epoch: 1 step: 132, loss is 2.2843029499053955\n",
      "epoch: 1 step: 133, loss is 2.274110794067383\n",
      "epoch: 1 step: 134, loss is 2.298548936843872\n",
      "epoch: 1 step: 135, loss is 2.293698310852051\n",
      "epoch: 1 step: 136, loss is 2.283820152282715\n",
      "epoch: 1 step: 137, loss is 2.286022424697876\n",
      "epoch: 1 step: 138, loss is 2.3034918308258057\n",
      "epoch: 1 step: 139, loss is 2.2963294982910156\n",
      "epoch: 1 step: 140, loss is 2.2938356399536133\n",
      "epoch: 1 step: 141, loss is 2.307171583175659\n",
      "epoch: 1 step: 142, loss is 2.300459146499634\n",
      "epoch: 1 step: 143, loss is 2.304204225540161\n",
      "epoch: 1 step: 144, loss is 2.3029720783233643\n",
      "epoch: 1 step: 145, loss is 2.307429313659668\n",
      "epoch: 1 step: 146, loss is 2.3120784759521484\n",
      "epoch: 1 step: 147, loss is 2.2969911098480225\n",
      "epoch: 1 step: 148, loss is 2.2885212898254395\n",
      "epoch: 1 step: 149, loss is 2.307213306427002\n",
      "epoch: 1 step: 150, loss is 2.3024120330810547\n",
      "epoch: 1 step: 151, loss is 2.2928731441497803\n",
      "epoch: 1 step: 152, loss is 2.298494338989258\n",
      "epoch: 1 step: 153, loss is 2.2915000915527344\n",
      "epoch: 1 step: 154, loss is 2.3092591762542725\n",
      "epoch: 1 step: 155, loss is 2.3140780925750732\n",
      "epoch: 1 step: 156, loss is 2.259946346282959\n",
      "epoch: 1 step: 157, loss is 2.2776732444763184\n",
      "epoch: 1 step: 158, loss is 2.2813706398010254\n",
      "epoch: 1 step: 159, loss is 2.28829026222229\n",
      "epoch: 1 step: 160, loss is 2.3008270263671875\n",
      "epoch: 1 step: 161, loss is 2.2993288040161133\n",
      "epoch: 1 step: 162, loss is 2.302166700363159\n",
      "epoch: 1 step: 163, loss is 2.296346426010132\n",
      "epoch: 1 step: 164, loss is 2.314960241317749\n",
      "epoch: 1 step: 165, loss is 2.3011972904205322\n",
      "epoch: 1 step: 166, loss is 2.305684804916382\n",
      "epoch: 1 step: 167, loss is 2.30703067779541\n",
      "epoch: 1 step: 168, loss is 2.2812044620513916\n",
      "epoch: 1 step: 169, loss is 2.322904109954834\n",
      "epoch: 1 step: 170, loss is 2.322932004928589\n",
      "epoch: 1 step: 171, loss is 2.3118326663970947\n",
      "epoch: 1 step: 172, loss is 2.294213056564331\n",
      "epoch: 1 step: 173, loss is 2.2984778881073\n",
      "epoch: 1 step: 174, loss is 2.2905325889587402\n",
      "epoch: 1 step: 175, loss is 2.2959141731262207\n",
      "epoch: 1 step: 176, loss is 2.2731575965881348\n",
      "epoch: 1 step: 177, loss is 2.2880990505218506\n",
      "epoch: 1 step: 178, loss is 2.3030622005462646\n",
      "epoch: 1 step: 179, loss is 2.284611701965332\n",
      "epoch: 1 step: 180, loss is 2.302929401397705\n",
      "epoch: 1 step: 181, loss is 2.3206918239593506\n",
      "epoch: 1 step: 182, loss is 2.2892906665802\n",
      "epoch: 1 step: 183, loss is 2.301546335220337\n",
      "epoch: 1 step: 184, loss is 2.2786121368408203\n",
      "epoch: 1 step: 185, loss is 2.304851770401001\n",
      "epoch: 1 step: 186, loss is 2.309260368347168\n",
      "epoch: 1 step: 187, loss is 2.3032734394073486\n",
      "epoch: 1 step: 188, loss is 2.318066120147705\n",
      "epoch: 1 step: 189, loss is 2.292214870452881\n",
      "epoch: 1 step: 190, loss is 2.277017593383789\n",
      "epoch: 1 step: 191, loss is 2.3061795234680176\n",
      "epoch: 1 step: 192, loss is 2.2736213207244873\n",
      "epoch: 1 step: 193, loss is 2.3069934844970703\n",
      "epoch: 1 step: 194, loss is 2.289127826690674\n",
      "epoch: 1 step: 195, loss is 2.3059914112091064\n",
      "epoch: 1 step: 196, loss is 2.2799887657165527\n",
      "epoch: 1 step: 197, loss is 2.279986619949341\n",
      "epoch: 1 step: 198, loss is 2.2704503536224365\n",
      "epoch: 1 step: 199, loss is 2.2957069873809814\n",
      "epoch: 1 step: 200, loss is 2.2949328422546387\n",
      "epoch: 1 step: 201, loss is 2.2880234718322754\n",
      "epoch: 1 step: 202, loss is 2.2944390773773193\n",
      "epoch: 1 step: 203, loss is 2.272676467895508\n",
      "epoch: 1 step: 204, loss is 2.2890639305114746\n",
      "epoch: 1 step: 205, loss is 2.2546451091766357\n",
      "epoch: 1 step: 206, loss is 2.2819271087646484\n",
      "epoch: 1 step: 207, loss is 2.2928719520568848\n",
      "epoch: 1 step: 208, loss is 2.294860601425171\n",
      "epoch: 1 step: 209, loss is 2.294569492340088\n",
      "epoch: 1 step: 210, loss is 2.2921743392944336\n",
      "epoch: 1 step: 211, loss is 2.308718204498291\n",
      "epoch: 1 step: 212, loss is 2.312495470046997\n",
      "epoch: 1 step: 213, loss is 2.2762744426727295\n",
      "epoch: 1 step: 214, loss is 2.244441509246826\n",
      "epoch: 1 step: 215, loss is 2.2761354446411133\n",
      "epoch: 1 step: 216, loss is 2.2698397636413574\n",
      "epoch: 1 step: 217, loss is 2.318178415298462\n",
      "epoch: 1 step: 218, loss is 2.255958080291748\n",
      "epoch: 1 step: 219, loss is 2.2687056064605713\n",
      "epoch: 1 step: 220, loss is 2.265995502471924\n",
      "epoch: 1 step: 221, loss is 2.2777247428894043\n",
      "epoch: 1 step: 222, loss is 2.268505811691284\n",
      "epoch: 1 step: 223, loss is 2.264347791671753\n",
      "epoch: 1 step: 224, loss is 2.275110960006714\n",
      "epoch: 1 step: 225, loss is 2.30349063873291\n",
      "epoch: 1 step: 226, loss is 2.2411482334136963\n",
      "epoch: 1 step: 227, loss is 2.263171434402466\n",
      "epoch: 1 step: 228, loss is 2.254047155380249\n",
      "epoch: 1 step: 229, loss is 2.2284231185913086\n",
      "epoch: 1 step: 230, loss is 2.259136199951172\n",
      "epoch: 1 step: 231, loss is 2.2402093410491943\n",
      "epoch: 1 step: 232, loss is 2.2710700035095215\n",
      "epoch: 1 step: 233, loss is 2.1644582748413086\n",
      "epoch: 1 step: 234, loss is 2.2662367820739746\n",
      "epoch: 1 step: 235, loss is 2.2250912189483643\n",
      "epoch: 1 step: 236, loss is 2.2697536945343018\n",
      "epoch: 1 step: 237, loss is 2.147939920425415\n",
      "epoch: 1 step: 238, loss is 2.2099995613098145\n",
      "epoch: 1 step: 239, loss is 2.1837570667266846\n",
      "epoch: 1 step: 240, loss is 2.179075002670288\n",
      "epoch: 1 step: 241, loss is 2.198704719543457\n",
      "epoch: 1 step: 242, loss is 2.179227828979492\n",
      "epoch: 1 step: 243, loss is 2.103471279144287\n",
      "epoch: 1 step: 244, loss is 2.2308173179626465\n",
      "epoch: 1 step: 245, loss is 2.1494295597076416\n",
      "epoch: 1 step: 246, loss is 2.153390407562256\n",
      "epoch: 1 step: 247, loss is 1.9742730855941772\n",
      "epoch: 1 step: 248, loss is 1.9684255123138428\n",
      "epoch: 1 step: 249, loss is 2.023998260498047\n",
      "epoch: 1 step: 250, loss is 1.9647659063339233\n",
      "epoch: 1 step: 251, loss is 2.0138041973114014\n",
      "epoch: 1 step: 252, loss is 2.1390180587768555\n",
      "epoch: 1 step: 253, loss is 2.1612601280212402\n",
      "epoch: 1 step: 254, loss is 1.7899953126907349\n",
      "epoch: 1 step: 255, loss is 2.0561163425445557\n",
      "epoch: 1 step: 256, loss is 2.055018424987793\n",
      "epoch: 1 step: 257, loss is 1.9273210763931274\n",
      "epoch: 1 step: 258, loss is 2.033668279647827\n",
      "epoch: 1 step: 259, loss is 1.8323512077331543\n",
      "epoch: 1 step: 260, loss is 1.8164067268371582\n",
      "epoch: 1 step: 261, loss is 1.8971189260482788\n",
      "epoch: 1 step: 262, loss is 1.955042839050293\n",
      "epoch: 1 step: 263, loss is 1.8312052488327026\n",
      "epoch: 1 step: 264, loss is 1.730744481086731\n",
      "epoch: 1 step: 265, loss is 1.941074252128601\n",
      "epoch: 1 step: 266, loss is 1.5930577516555786\n",
      "epoch: 1 step: 267, loss is 1.8293942213058472\n",
      "epoch: 1 step: 268, loss is 1.7318761348724365\n",
      "epoch: 1 step: 269, loss is 1.5888971090316772\n",
      "epoch: 1 step: 270, loss is 1.7610830068588257\n",
      "epoch: 1 step: 271, loss is 1.9029775857925415\n",
      "epoch: 1 step: 272, loss is 1.4765375852584839\n",
      "epoch: 1 step: 273, loss is 1.5067574977874756\n",
      "epoch: 1 step: 274, loss is 1.3981062173843384\n",
      "epoch: 1 step: 275, loss is 1.398898959159851\n",
      "epoch: 1 step: 276, loss is 1.7207388877868652\n",
      "epoch: 1 step: 277, loss is 1.4118363857269287\n",
      "epoch: 1 step: 278, loss is 1.7077776193618774\n",
      "epoch: 1 step: 279, loss is 1.586692452430725\n",
      "epoch: 1 step: 280, loss is 1.2816530466079712\n",
      "epoch: 1 step: 281, loss is 1.2644243240356445\n",
      "epoch: 1 step: 282, loss is 1.3513035774230957\n",
      "epoch: 1 step: 283, loss is 1.2661147117614746\n",
      "epoch: 1 step: 284, loss is 1.677425742149353\n",
      "epoch: 1 step: 285, loss is 1.513746738433838\n",
      "epoch: 1 step: 286, loss is 1.3281009197235107\n",
      "epoch: 1 step: 287, loss is 1.4068242311477661\n",
      "epoch: 1 step: 288, loss is 1.2214008569717407\n",
      "epoch: 1 step: 289, loss is 1.5962016582489014\n",
      "epoch: 1 step: 290, loss is 1.5288848876953125\n",
      "epoch: 1 step: 291, loss is 1.267882227897644\n",
      "epoch: 1 step: 292, loss is 1.6112436056137085\n",
      "epoch: 1 step: 293, loss is 1.0759066343307495\n",
      "epoch: 1 step: 294, loss is 1.3807207345962524\n",
      "epoch: 1 step: 295, loss is 1.2830188274383545\n",
      "epoch: 1 step: 296, loss is 1.1697920560836792\n",
      "epoch: 1 step: 297, loss is 1.2727961540222168\n",
      "epoch: 1 step: 298, loss is 0.9069486856460571\n",
      "epoch: 1 step: 299, loss is 0.9304875731468201\n",
      "epoch: 1 step: 300, loss is 1.2933393716812134\n",
      "epoch: 1 step: 301, loss is 0.858218252658844\n",
      "epoch: 1 step: 302, loss is 0.8701322078704834\n",
      "epoch: 1 step: 303, loss is 0.9576447606086731\n",
      "epoch: 1 step: 304, loss is 0.9563692808151245\n",
      "epoch: 1 step: 305, loss is 1.0239558219909668\n",
      "epoch: 1 step: 306, loss is 1.1079885959625244\n",
      "epoch: 1 step: 307, loss is 1.7648519277572632\n",
      "epoch: 1 step: 308, loss is 1.125197410583496\n",
      "epoch: 1 step: 309, loss is 0.9548307061195374\n",
      "epoch: 1 step: 310, loss is 1.2803380489349365\n",
      "epoch: 1 step: 311, loss is 0.7435316443443298\n",
      "epoch: 1 step: 312, loss is 0.8967411518096924\n",
      "epoch: 1 step: 313, loss is 0.3373659551143646\n",
      "epoch: 1 step: 314, loss is 1.1583856344223022\n",
      "epoch: 1 step: 315, loss is 1.0722999572753906\n",
      "epoch: 1 step: 316, loss is 0.8616359233856201\n",
      "epoch: 1 step: 317, loss is 0.9765185713768005\n",
      "epoch: 1 step: 318, loss is 0.7299550771713257\n",
      "epoch: 1 step: 319, loss is 0.7969080805778503\n",
      "epoch: 1 step: 320, loss is 0.7932347655296326\n",
      "epoch: 1 step: 321, loss is 0.9897705912590027\n",
      "epoch: 1 step: 322, loss is 0.8401029706001282\n",
      "epoch: 1 step: 323, loss is 0.9501879811286926\n",
      "epoch: 1 step: 324, loss is 0.7406326532363892\n",
      "epoch: 1 step: 325, loss is 0.8091892004013062\n",
      "epoch: 1 step: 326, loss is 1.1136021614074707\n",
      "epoch: 1 step: 327, loss is 1.1852126121520996\n",
      "epoch: 1 step: 328, loss is 0.9204393625259399\n",
      "epoch: 1 step: 329, loss is 1.2714289426803589\n",
      "epoch: 1 step: 330, loss is 0.8962709307670593\n",
      "epoch: 1 step: 331, loss is 0.6677216291427612\n",
      "epoch: 1 step: 332, loss is 0.7204903364181519\n",
      "epoch: 1 step: 333, loss is 0.650558590888977\n",
      "epoch: 1 step: 334, loss is 0.6909817457199097\n",
      "epoch: 1 step: 335, loss is 1.0787570476531982\n",
      "epoch: 1 step: 336, loss is 0.8529075980186462\n",
      "epoch: 1 step: 337, loss is 0.7958667874336243\n",
      "epoch: 1 step: 338, loss is 0.7180119752883911\n",
      "epoch: 1 step: 339, loss is 0.7044186592102051\n",
      "epoch: 1 step: 340, loss is 0.681587278842926\n",
      "epoch: 1 step: 341, loss is 0.8517749905586243\n",
      "epoch: 1 step: 342, loss is 0.9434900879859924\n",
      "epoch: 1 step: 343, loss is 0.9470328688621521\n",
      "epoch: 1 step: 344, loss is 0.6026003360748291\n",
      "epoch: 1 step: 345, loss is 0.7732653617858887\n",
      "epoch: 1 step: 346, loss is 0.7173418998718262\n",
      "epoch: 1 step: 347, loss is 0.7555949091911316\n",
      "epoch: 1 step: 348, loss is 0.9048782587051392\n",
      "epoch: 1 step: 349, loss is 0.7881668210029602\n",
      "epoch: 1 step: 350, loss is 0.7295504808425903\n",
      "epoch: 1 step: 351, loss is 1.2917160987854004\n",
      "epoch: 1 step: 352, loss is 1.107515811920166\n",
      "epoch: 1 step: 353, loss is 0.501802921295166\n",
      "epoch: 1 step: 354, loss is 0.9398590326309204\n",
      "epoch: 1 step: 355, loss is 0.746354341506958\n",
      "epoch: 1 step: 356, loss is 0.7851202487945557\n",
      "epoch: 1 step: 357, loss is 0.7068289518356323\n",
      "epoch: 1 step: 358, loss is 0.6149641275405884\n",
      "epoch: 1 step: 359, loss is 0.4372740685939789\n",
      "epoch: 1 step: 360, loss is 0.6731146574020386\n",
      "epoch: 1 step: 361, loss is 0.27175095677375793\n",
      "epoch: 1 step: 362, loss is 0.2904432415962219\n",
      "epoch: 1 step: 363, loss is 0.5141564607620239\n",
      "epoch: 1 step: 364, loss is 0.9141446352005005\n",
      "epoch: 1 step: 365, loss is 0.3129643201828003\n",
      "epoch: 1 step: 366, loss is 0.7488610148429871\n",
      "epoch: 1 step: 367, loss is 0.38329416513442993\n",
      "epoch: 1 step: 368, loss is 0.6365708112716675\n",
      "epoch: 1 step: 369, loss is 0.6753060817718506\n",
      "epoch: 1 step: 370, loss is 0.29714590311050415\n",
      "epoch: 1 step: 371, loss is 0.4822643995285034\n",
      "epoch: 1 step: 372, loss is 0.6546379923820496\n",
      "epoch: 1 step: 373, loss is 0.462291955947876\n",
      "epoch: 1 step: 374, loss is 0.6215528249740601\n",
      "epoch: 1 step: 375, loss is 0.5136353969573975\n",
      "epoch: 1 step: 376, loss is 0.806808590888977\n",
      "epoch: 1 step: 377, loss is 0.682482898235321\n",
      "epoch: 1 step: 378, loss is 0.7764397859573364\n",
      "epoch: 1 step: 379, loss is 0.43507087230682373\n",
      "epoch: 1 step: 380, loss is 0.44125795364379883\n",
      "epoch: 1 step: 381, loss is 0.49742209911346436\n",
      "epoch: 1 step: 382, loss is 0.3575311601161957\n",
      "epoch: 1 step: 383, loss is 0.5528024435043335\n",
      "epoch: 1 step: 384, loss is 0.446981817483902\n",
      "epoch: 1 step: 385, loss is 0.5529546141624451\n",
      "epoch: 1 step: 386, loss is 0.5888735055923462\n",
      "epoch: 1 step: 387, loss is 0.48664167523384094\n",
      "epoch: 1 step: 388, loss is 0.30859407782554626\n",
      "epoch: 1 step: 389, loss is 0.49058201909065247\n",
      "epoch: 1 step: 390, loss is 0.5429295301437378\n",
      "epoch: 1 step: 391, loss is 0.7567619681358337\n",
      "epoch: 1 step: 392, loss is 0.39111754298210144\n",
      "epoch: 1 step: 393, loss is 0.4103764593601227\n",
      "epoch: 1 step: 394, loss is 0.33839279413223267\n",
      "epoch: 1 step: 395, loss is 0.6488182544708252\n",
      "epoch: 1 step: 396, loss is 0.34183189272880554\n",
      "epoch: 1 step: 397, loss is 0.6411921381950378\n",
      "epoch: 1 step: 398, loss is 0.5869840979576111\n",
      "epoch: 1 step: 399, loss is 0.709286630153656\n",
      "epoch: 1 step: 400, loss is 0.450185090303421\n",
      "epoch: 1 step: 401, loss is 0.5143252611160278\n",
      "epoch: 1 step: 402, loss is 0.5452401638031006\n",
      "epoch: 1 step: 403, loss is 0.4026294946670532\n",
      "epoch: 1 step: 404, loss is 0.41092434525489807\n",
      "epoch: 1 step: 405, loss is 0.32073622941970825\n",
      "epoch: 1 step: 406, loss is 0.30673179030418396\n",
      "epoch: 1 step: 407, loss is 0.530680239200592\n",
      "epoch: 1 step: 408, loss is 0.32031476497650146\n",
      "epoch: 1 step: 409, loss is 0.11789265275001526\n",
      "epoch: 1 step: 410, loss is 0.2254946231842041\n",
      "epoch: 1 step: 411, loss is 0.4629074037075043\n",
      "epoch: 1 step: 412, loss is 0.3168513774871826\n",
      "epoch: 1 step: 413, loss is 0.22946159541606903\n",
      "epoch: 1 step: 414, loss is 0.30235204100608826\n",
      "epoch: 1 step: 415, loss is 0.31037068367004395\n",
      "epoch: 1 step: 416, loss is 0.23997418582439423\n",
      "epoch: 1 step: 417, loss is 0.34644851088523865\n",
      "epoch: 1 step: 418, loss is 0.2697697579860687\n",
      "epoch: 1 step: 419, loss is 0.3316473662853241\n",
      "epoch: 1 step: 420, loss is 0.10478965193033218\n",
      "epoch: 1 step: 421, loss is 0.1976955682039261\n",
      "epoch: 1 step: 422, loss is 0.36368951201438904\n",
      "epoch: 1 step: 423, loss is 0.5299737453460693\n",
      "epoch: 1 step: 424, loss is 0.14558447897434235\n",
      "epoch: 1 step: 425, loss is 0.5831152200698853\n",
      "epoch: 1 step: 426, loss is 0.6156436800956726\n",
      "epoch: 1 step: 427, loss is 0.6058842539787292\n",
      "epoch: 1 step: 428, loss is 0.565112292766571\n",
      "epoch: 1 step: 429, loss is 0.29022154211997986\n",
      "epoch: 1 step: 430, loss is 0.17873141169548035\n",
      "epoch: 1 step: 431, loss is 0.9457321166992188\n",
      "epoch: 1 step: 432, loss is 0.7290535569190979\n",
      "epoch: 1 step: 433, loss is 0.4560469686985016\n",
      "epoch: 1 step: 434, loss is 0.5075349807739258\n",
      "epoch: 1 step: 435, loss is 0.23653434216976166\n",
      "epoch: 1 step: 436, loss is 0.5057867169380188\n",
      "epoch: 1 step: 437, loss is 0.4796901345252991\n",
      "epoch: 1 step: 438, loss is 0.46534469723701477\n",
      "epoch: 1 step: 439, loss is 0.42416101694107056\n",
      "epoch: 1 step: 440, loss is 0.44371071457862854\n",
      "epoch: 1 step: 441, loss is 0.507915198802948\n",
      "epoch: 1 step: 442, loss is 0.3699881136417389\n",
      "epoch: 1 step: 443, loss is 0.2843821346759796\n",
      "epoch: 1 step: 444, loss is 0.46581771969795227\n",
      "epoch: 1 step: 445, loss is 0.5442425012588501\n",
      "epoch: 1 step: 446, loss is 0.17302434146404266\n",
      "epoch: 1 step: 447, loss is 0.449444979429245\n",
      "epoch: 1 step: 448, loss is 0.36438503861427307\n",
      "epoch: 1 step: 449, loss is 0.2273482233285904\n",
      "epoch: 1 step: 450, loss is 0.1596570461988449\n",
      "epoch: 1 step: 451, loss is 0.2024514079093933\n",
      "epoch: 1 step: 452, loss is 0.26465582847595215\n",
      "epoch: 1 step: 453, loss is 0.324135422706604\n",
      "epoch: 1 step: 454, loss is 0.5786317586898804\n",
      "epoch: 1 step: 455, loss is 0.45953065156936646\n",
      "epoch: 1 step: 456, loss is 0.311290442943573\n",
      "epoch: 1 step: 457, loss is 0.27769583463668823\n",
      "epoch: 1 step: 458, loss is 0.6913138031959534\n",
      "epoch: 1 step: 459, loss is 0.3106911778450012\n",
      "epoch: 1 step: 460, loss is 0.3698959946632385\n",
      "epoch: 1 step: 461, loss is 0.7233113050460815\n",
      "epoch: 1 step: 462, loss is 0.5067330002784729\n",
      "epoch: 1 step: 463, loss is 0.13905549049377441\n",
      "epoch: 1 step: 464, loss is 0.53526771068573\n",
      "epoch: 1 step: 465, loss is 0.4699840545654297\n",
      "epoch: 1 step: 466, loss is 0.26853692531585693\n",
      "epoch: 1 step: 467, loss is 0.6474863886833191\n",
      "epoch: 1 step: 468, loss is 0.45267343521118164\n",
      "epoch: 1 step: 469, loss is 0.2721782326698303\n",
      "epoch: 1 step: 470, loss is 0.4937705397605896\n",
      "epoch: 1 step: 471, loss is 0.5780807733535767\n",
      "epoch: 1 step: 472, loss is 0.5894080400466919\n",
      "epoch: 1 step: 473, loss is 0.18605956435203552\n",
      "epoch: 1 step: 474, loss is 0.18996737897396088\n",
      "epoch: 1 step: 475, loss is 0.46469658613204956\n",
      "epoch: 1 step: 476, loss is 0.2895575761795044\n",
      "epoch: 1 step: 477, loss is 0.5991829633712769\n",
      "epoch: 1 step: 478, loss is 0.2923520803451538\n",
      "epoch: 1 step: 479, loss is 0.41408559679985046\n",
      "epoch: 1 step: 480, loss is 0.1620561182498932\n",
      "epoch: 1 step: 481, loss is 0.6108221411705017\n",
      "epoch: 1 step: 482, loss is 0.5311442613601685\n",
      "epoch: 1 step: 483, loss is 0.3922688066959381\n",
      "epoch: 1 step: 484, loss is 0.25486910343170166\n",
      "epoch: 1 step: 485, loss is 0.2536291778087616\n",
      "epoch: 1 step: 486, loss is 0.42442870140075684\n",
      "epoch: 1 step: 487, loss is 0.827576220035553\n",
      "epoch: 1 step: 488, loss is 0.2580536901950836\n",
      "epoch: 1 step: 489, loss is 0.5782480239868164\n",
      "epoch: 1 step: 490, loss is 0.3428788185119629\n",
      "epoch: 1 step: 491, loss is 0.19538800418376923\n",
      "epoch: 1 step: 492, loss is 0.21063123643398285\n",
      "epoch: 1 step: 493, loss is 0.3267654478549957\n",
      "epoch: 1 step: 494, loss is 0.3506109416484833\n",
      "epoch: 1 step: 495, loss is 0.38899338245391846\n",
      "epoch: 1 step: 496, loss is 0.4547381103038788\n",
      "epoch: 1 step: 497, loss is 0.291936993598938\n",
      "epoch: 1 step: 498, loss is 0.27646926045417786\n",
      "epoch: 1 step: 499, loss is 0.45286449790000916\n",
      "epoch: 1 step: 500, loss is 0.13663484156131744\n",
      "epoch: 1 step: 501, loss is 0.1259550154209137\n",
      "epoch: 1 step: 502, loss is 0.15260209143161774\n",
      "epoch: 1 step: 503, loss is 0.2691350281238556\n",
      "epoch: 1 step: 504, loss is 0.1340511441230774\n",
      "epoch: 1 step: 505, loss is 0.1090259924530983\n",
      "epoch: 1 step: 506, loss is 0.13694502413272858\n",
      "epoch: 1 step: 507, loss is 0.261473149061203\n",
      "epoch: 1 step: 508, loss is 0.21821679174900055\n",
      "epoch: 1 step: 509, loss is 0.22505146265029907\n",
      "epoch: 1 step: 510, loss is 0.10614099353551865\n",
      "epoch: 1 step: 511, loss is 0.4144408404827118\n",
      "epoch: 1 step: 512, loss is 0.1334744095802307\n",
      "epoch: 1 step: 513, loss is 0.1744583547115326\n",
      "epoch: 1 step: 514, loss is 0.07708548754453659\n",
      "epoch: 1 step: 515, loss is 0.25397366285324097\n",
      "epoch: 1 step: 516, loss is 0.5618014335632324\n",
      "epoch: 1 step: 517, loss is 0.19408224523067474\n",
      "epoch: 1 step: 518, loss is 0.21552486717700958\n",
      "epoch: 1 step: 519, loss is 0.11082233488559723\n",
      "epoch: 1 step: 520, loss is 0.16681364178657532\n",
      "epoch: 1 step: 521, loss is 0.4856988787651062\n",
      "epoch: 1 step: 522, loss is 0.2862556576728821\n",
      "epoch: 1 step: 523, loss is 0.26811346411705017\n",
      "epoch: 1 step: 524, loss is 0.2516609728336334\n",
      "epoch: 1 step: 525, loss is 0.40884336829185486\n",
      "epoch: 1 step: 526, loss is 0.16339807212352753\n",
      "epoch: 1 step: 527, loss is 0.18179461359977722\n",
      "epoch: 1 step: 528, loss is 0.1057535782456398\n",
      "epoch: 1 step: 529, loss is 0.10700217634439468\n",
      "epoch: 1 step: 530, loss is 0.23146292567253113\n",
      "epoch: 1 step: 531, loss is 0.425893634557724\n",
      "epoch: 1 step: 532, loss is 0.09507712721824646\n",
      "epoch: 1 step: 533, loss is 0.16886714100837708\n",
      "epoch: 1 step: 534, loss is 0.08585955202579498\n",
      "epoch: 1 step: 535, loss is 0.43394342064857483\n",
      "epoch: 1 step: 536, loss is 0.22821132838726044\n",
      "epoch: 1 step: 537, loss is 0.15442770719528198\n",
      "epoch: 1 step: 538, loss is 0.22011291980743408\n",
      "epoch: 1 step: 539, loss is 0.17301912605762482\n",
      "epoch: 1 step: 540, loss is 0.7500737905502319\n",
      "epoch: 1 step: 541, loss is 0.049494773149490356\n",
      "epoch: 1 step: 542, loss is 0.07202701270580292\n",
      "epoch: 1 step: 543, loss is 0.09711974114179611\n",
      "epoch: 1 step: 544, loss is 0.13449092209339142\n",
      "epoch: 1 step: 545, loss is 0.11723466962575912\n",
      "epoch: 1 step: 546, loss is 0.11706437170505524\n",
      "epoch: 1 step: 547, loss is 0.3096804916858673\n",
      "epoch: 1 step: 548, loss is 0.06337174773216248\n",
      "epoch: 1 step: 549, loss is 0.11740823835134506\n",
      "epoch: 1 step: 550, loss is 0.1757604479789734\n",
      "epoch: 1 step: 551, loss is 0.31707537174224854\n",
      "epoch: 1 step: 552, loss is 0.22766508162021637\n",
      "epoch: 1 step: 553, loss is 0.14630591869354248\n",
      "epoch: 1 step: 554, loss is 0.23044608533382416\n",
      "epoch: 1 step: 555, loss is 0.3290272057056427\n",
      "epoch: 1 step: 556, loss is 0.3601571023464203\n",
      "epoch: 1 step: 557, loss is 0.1514623910188675\n",
      "epoch: 1 step: 558, loss is 0.5051459670066833\n",
      "epoch: 1 step: 559, loss is 0.16731759905815125\n",
      "epoch: 1 step: 560, loss is 0.03066108375787735\n",
      "epoch: 1 step: 561, loss is 0.08586064726114273\n",
      "epoch: 1 step: 562, loss is 0.24077284336090088\n",
      "epoch: 1 step: 563, loss is 0.5172154903411865\n",
      "epoch: 1 step: 564, loss is 0.242719829082489\n",
      "epoch: 1 step: 565, loss is 0.2526131272315979\n",
      "epoch: 1 step: 566, loss is 0.07116616517305374\n",
      "epoch: 1 step: 567, loss is 0.0956207737326622\n",
      "epoch: 1 step: 568, loss is 0.16553722321987152\n",
      "epoch: 1 step: 569, loss is 0.3233053982257843\n",
      "epoch: 1 step: 570, loss is 0.11966697871685028\n",
      "epoch: 1 step: 571, loss is 0.17644789814949036\n",
      "epoch: 1 step: 572, loss is 0.0564739964902401\n",
      "epoch: 1 step: 573, loss is 0.2884933352470398\n",
      "epoch: 1 step: 574, loss is 0.0439193956553936\n",
      "epoch: 1 step: 575, loss is 0.2124115377664566\n",
      "epoch: 1 step: 576, loss is 0.1404484659433365\n",
      "epoch: 1 step: 577, loss is 0.14090155065059662\n",
      "epoch: 1 step: 578, loss is 0.2508050799369812\n",
      "epoch: 1 step: 579, loss is 0.3995395600795746\n",
      "epoch: 1 step: 580, loss is 0.047687411308288574\n",
      "epoch: 1 step: 581, loss is 0.2984614372253418\n",
      "epoch: 1 step: 582, loss is 0.07893272489309311\n",
      "epoch: 1 step: 583, loss is 0.2142471969127655\n",
      "epoch: 1 step: 584, loss is 0.689147412776947\n",
      "epoch: 1 step: 585, loss is 0.22174072265625\n",
      "epoch: 1 step: 586, loss is 0.1256110817193985\n",
      "epoch: 1 step: 587, loss is 0.25761696696281433\n",
      "epoch: 1 step: 588, loss is 0.1314634382724762\n",
      "epoch: 1 step: 589, loss is 0.5230224132537842\n",
      "epoch: 1 step: 590, loss is 0.27661943435668945\n",
      "epoch: 1 step: 591, loss is 0.09884844720363617\n",
      "epoch: 1 step: 592, loss is 0.05476070195436478\n",
      "epoch: 1 step: 593, loss is 0.06562073528766632\n",
      "epoch: 1 step: 594, loss is 0.3402823805809021\n",
      "epoch: 1 step: 595, loss is 0.15982449054718018\n",
      "epoch: 1 step: 596, loss is 0.20549409091472626\n",
      "epoch: 1 step: 597, loss is 0.1616799235343933\n",
      "epoch: 1 step: 598, loss is 0.15076322853565216\n",
      "epoch: 1 step: 599, loss is 0.14716781675815582\n",
      "epoch: 1 step: 600, loss is 0.25055912137031555\n",
      "epoch: 1 step: 601, loss is 0.18698500096797943\n",
      "epoch: 1 step: 602, loss is 0.13906987011432648\n",
      "epoch: 1 step: 603, loss is 0.13475830852985382\n",
      "epoch: 1 step: 604, loss is 0.08285700529813766\n",
      "epoch: 1 step: 605, loss is 0.05613318830728531\n",
      "epoch: 1 step: 606, loss is 0.44787588715553284\n",
      "epoch: 1 step: 607, loss is 0.1136566624045372\n",
      "epoch: 1 step: 608, loss is 0.27820247411727905\n",
      "epoch: 1 step: 609, loss is 0.08189693093299866\n",
      "epoch: 1 step: 610, loss is 0.4782050848007202\n",
      "epoch: 1 step: 611, loss is 0.17379561066627502\n",
      "epoch: 1 step: 612, loss is 0.2431311458349228\n",
      "epoch: 1 step: 613, loss is 0.07078196853399277\n",
      "epoch: 1 step: 614, loss is 0.08999760448932648\n",
      "epoch: 1 step: 615, loss is 0.1586875021457672\n",
      "epoch: 1 step: 616, loss is 0.23806852102279663\n",
      "epoch: 1 step: 617, loss is 0.12184054404497147\n",
      "epoch: 1 step: 618, loss is 0.2920811176300049\n",
      "epoch: 1 step: 619, loss is 0.04218311607837677\n",
      "epoch: 1 step: 620, loss is 0.28064388036727905\n",
      "epoch: 1 step: 621, loss is 0.3456723988056183\n",
      "epoch: 1 step: 622, loss is 0.059821631759405136\n",
      "epoch: 1 step: 623, loss is 0.12477962672710419\n",
      "epoch: 1 step: 624, loss is 0.6335103511810303\n",
      "epoch: 1 step: 625, loss is 0.12719695270061493\n",
      "epoch: 1 step: 626, loss is 0.15979935228824615\n",
      "epoch: 1 step: 627, loss is 0.2716434597969055\n",
      "epoch: 1 step: 628, loss is 0.1442481279373169\n",
      "epoch: 1 step: 629, loss is 0.1369292438030243\n",
      "epoch: 1 step: 630, loss is 0.10873069614171982\n",
      "epoch: 1 step: 631, loss is 0.26369747519493103\n",
      "epoch: 1 step: 632, loss is 0.13142350316047668\n",
      "epoch: 1 step: 633, loss is 0.1729237288236618\n",
      "epoch: 1 step: 634, loss is 0.20780858397483826\n",
      "epoch: 1 step: 635, loss is 0.033151693642139435\n",
      "epoch: 1 step: 636, loss is 0.2853573262691498\n",
      "epoch: 1 step: 637, loss is 0.09150922298431396\n",
      "epoch: 1 step: 638, loss is 0.10811813920736313\n",
      "epoch: 1 step: 639, loss is 0.16953307390213013\n",
      "epoch: 1 step: 640, loss is 0.12360195070505142\n",
      "epoch: 1 step: 641, loss is 0.23521867394447327\n",
      "epoch: 1 step: 642, loss is 0.2468545138835907\n",
      "epoch: 1 step: 643, loss is 0.07700866460800171\n",
      "epoch: 1 step: 644, loss is 0.18199372291564941\n",
      "epoch: 1 step: 645, loss is 0.2719402015209198\n",
      "epoch: 1 step: 646, loss is 0.05652189254760742\n",
      "epoch: 1 step: 647, loss is 0.020622659474611282\n",
      "epoch: 1 step: 648, loss is 0.08479149639606476\n",
      "epoch: 1 step: 649, loss is 0.07315593212842941\n",
      "epoch: 1 step: 650, loss is 0.15376152098178864\n",
      "epoch: 1 step: 651, loss is 0.08503653854131699\n",
      "epoch: 1 step: 652, loss is 0.23395198583602905\n",
      "epoch: 1 step: 653, loss is 0.1541619896888733\n",
      "epoch: 1 step: 654, loss is 0.10148114711046219\n",
      "epoch: 1 step: 655, loss is 0.2148204892873764\n",
      "epoch: 1 step: 656, loss is 0.06292527914047241\n",
      "epoch: 1 step: 657, loss is 0.18973229825496674\n",
      "epoch: 1 step: 658, loss is 0.08342534303665161\n",
      "epoch: 1 step: 659, loss is 0.1318403035402298\n",
      "epoch: 1 step: 660, loss is 0.14881794154644012\n",
      "epoch: 1 step: 661, loss is 0.36560559272766113\n",
      "epoch: 1 step: 662, loss is 0.2973797023296356\n",
      "epoch: 1 step: 663, loss is 0.2819278836250305\n",
      "epoch: 1 step: 664, loss is 0.2837546467781067\n",
      "epoch: 1 step: 665, loss is 0.06249845027923584\n",
      "epoch: 1 step: 666, loss is 0.23979097604751587\n",
      "epoch: 1 step: 667, loss is 0.2127627432346344\n",
      "epoch: 1 step: 668, loss is 0.3769589960575104\n",
      "epoch: 1 step: 669, loss is 0.18970689177513123\n",
      "epoch: 1 step: 670, loss is 0.2559713125228882\n",
      "epoch: 1 step: 671, loss is 0.2552734613418579\n",
      "epoch: 1 step: 672, loss is 0.06187545135617256\n",
      "epoch: 1 step: 673, loss is 0.17466194927692413\n",
      "epoch: 1 step: 674, loss is 0.5806146264076233\n",
      "epoch: 1 step: 675, loss is 0.14885683357715607\n",
      "epoch: 1 step: 676, loss is 0.1820022612810135\n",
      "epoch: 1 step: 677, loss is 0.34135356545448303\n",
      "epoch: 1 step: 678, loss is 0.19104531407356262\n",
      "epoch: 1 step: 679, loss is 0.19092562794685364\n",
      "epoch: 1 step: 680, loss is 0.3533426523208618\n",
      "epoch: 1 step: 681, loss is 0.21482615172863007\n",
      "epoch: 1 step: 682, loss is 0.3398033380508423\n",
      "epoch: 1 step: 683, loss is 0.2591135799884796\n",
      "epoch: 1 step: 684, loss is 0.20491382479667664\n",
      "epoch: 1 step: 685, loss is 0.047203414142131805\n",
      "epoch: 1 step: 686, loss is 0.18002010881900787\n",
      "epoch: 1 step: 687, loss is 0.1474820375442505\n",
      "epoch: 1 step: 688, loss is 0.30153393745422363\n",
      "epoch: 1 step: 689, loss is 0.13932347297668457\n",
      "epoch: 1 step: 690, loss is 0.1772315949201584\n",
      "epoch: 1 step: 691, loss is 0.049923092126846313\n",
      "epoch: 1 step: 692, loss is 0.22998420894145966\n",
      "epoch: 1 step: 693, loss is 0.27677157521247864\n",
      "epoch: 1 step: 694, loss is 0.5041919350624084\n",
      "epoch: 1 step: 695, loss is 0.056312404572963715\n",
      "epoch: 1 step: 696, loss is 0.15043114125728607\n",
      "epoch: 1 step: 697, loss is 0.17015337944030762\n",
      "epoch: 1 step: 698, loss is 0.09222808480262756\n",
      "epoch: 1 step: 699, loss is 0.542254626750946\n",
      "epoch: 1 step: 700, loss is 0.2946353852748871\n",
      "epoch: 1 step: 701, loss is 0.2162739783525467\n",
      "epoch: 1 step: 702, loss is 0.19562040269374847\n",
      "epoch: 1 step: 703, loss is 0.1495606154203415\n",
      "epoch: 1 step: 704, loss is 0.05782173201441765\n",
      "epoch: 1 step: 705, loss is 0.16195254027843475\n",
      "epoch: 1 step: 706, loss is 0.2519797384738922\n",
      "epoch: 1 step: 707, loss is 0.5335734486579895\n",
      "epoch: 1 step: 708, loss is 0.07403087615966797\n",
      "epoch: 1 step: 709, loss is 0.060762569308280945\n",
      "epoch: 1 step: 710, loss is 0.07294254750013351\n",
      "epoch: 1 step: 711, loss is 0.20741809904575348\n",
      "epoch: 1 step: 712, loss is 0.15027382969856262\n",
      "epoch: 1 step: 713, loss is 0.061393603682518005\n",
      "epoch: 1 step: 714, loss is 0.13067889213562012\n",
      "epoch: 1 step: 715, loss is 0.17822568118572235\n",
      "epoch: 1 step: 716, loss is 0.1036960780620575\n",
      "epoch: 1 step: 717, loss is 0.028503471985459328\n",
      "epoch: 1 step: 718, loss is 0.19212539494037628\n",
      "epoch: 1 step: 719, loss is 0.045727137476205826\n",
      "epoch: 1 step: 720, loss is 0.20721052587032318\n",
      "epoch: 1 step: 721, loss is 0.15124934911727905\n",
      "epoch: 1 step: 722, loss is 0.16121269762516022\n",
      "epoch: 1 step: 723, loss is 0.13487309217453003\n",
      "epoch: 1 step: 724, loss is 0.4212854504585266\n",
      "epoch: 1 step: 725, loss is 0.27434179186820984\n",
      "epoch: 1 step: 726, loss is 0.1580202877521515\n",
      "epoch: 1 step: 727, loss is 0.29556554555892944\n",
      "epoch: 1 step: 728, loss is 0.07305631786584854\n",
      "epoch: 1 step: 729, loss is 0.34828001260757446\n",
      "epoch: 1 step: 730, loss is 0.16746361553668976\n",
      "epoch: 1 step: 731, loss is 0.27944299578666687\n",
      "epoch: 1 step: 732, loss is 0.20636600255966187\n",
      "epoch: 1 step: 733, loss is 0.27177608013153076\n",
      "epoch: 1 step: 734, loss is 0.1411837935447693\n",
      "epoch: 1 step: 735, loss is 0.04993263632059097\n",
      "epoch: 1 step: 736, loss is 0.1640140563249588\n",
      "epoch: 1 step: 737, loss is 0.18590131402015686\n",
      "epoch: 1 step: 738, loss is 0.1051405519247055\n",
      "epoch: 1 step: 739, loss is 0.06812333315610886\n",
      "epoch: 1 step: 740, loss is 0.08803962916135788\n",
      "epoch: 1 step: 741, loss is 0.15523190796375275\n",
      "epoch: 1 step: 742, loss is 0.1347215622663498\n",
      "epoch: 1 step: 743, loss is 0.1998237669467926\n",
      "epoch: 1 step: 744, loss is 0.21205781400203705\n",
      "epoch: 1 step: 745, loss is 0.11682999134063721\n",
      "epoch: 1 step: 746, loss is 0.12559491395950317\n",
      "epoch: 1 step: 747, loss is 0.084632009267807\n",
      "epoch: 1 step: 748, loss is 0.1159764975309372\n",
      "epoch: 1 step: 749, loss is 0.25695183873176575\n",
      "epoch: 1 step: 750, loss is 0.06934484839439392\n",
      "epoch: 1 step: 751, loss is 0.06111110746860504\n",
      "epoch: 1 step: 752, loss is 0.05966193601489067\n",
      "epoch: 1 step: 753, loss is 0.18990899622440338\n",
      "epoch: 1 step: 754, loss is 0.04644547775387764\n",
      "epoch: 1 step: 755, loss is 0.4567900598049164\n",
      "epoch: 1 step: 756, loss is 0.08613448590040207\n",
      "epoch: 1 step: 757, loss is 0.14151862263679504\n",
      "epoch: 1 step: 758, loss is 0.05817018449306488\n",
      "epoch: 1 step: 759, loss is 0.38598254323005676\n",
      "epoch: 1 step: 760, loss is 0.16023477911949158\n",
      "epoch: 1 step: 761, loss is 0.061984509229660034\n",
      "epoch: 1 step: 762, loss is 0.09485819190740585\n",
      "epoch: 1 step: 763, loss is 0.15367387235164642\n",
      "epoch: 1 step: 764, loss is 0.23391123116016388\n",
      "epoch: 1 step: 765, loss is 0.16677846014499664\n",
      "epoch: 1 step: 766, loss is 0.3854384422302246\n",
      "epoch: 1 step: 767, loss is 0.04219458997249603\n",
      "epoch: 1 step: 768, loss is 0.06345368176698685\n",
      "epoch: 1 step: 769, loss is 0.03207012265920639\n",
      "epoch: 1 step: 770, loss is 0.18253599107265472\n",
      "epoch: 1 step: 771, loss is 0.03267271816730499\n",
      "epoch: 1 step: 772, loss is 0.027907690033316612\n",
      "epoch: 1 step: 773, loss is 0.05582892894744873\n",
      "epoch: 1 step: 774, loss is 0.09607388079166412\n",
      "epoch: 1 step: 775, loss is 0.17143379151821136\n",
      "epoch: 1 step: 776, loss is 0.20231682062149048\n",
      "epoch: 1 step: 777, loss is 0.08208572119474411\n",
      "epoch: 1 step: 778, loss is 0.015865232795476913\n",
      "epoch: 1 step: 779, loss is 0.01439722254872322\n",
      "epoch: 1 step: 780, loss is 0.15736335515975952\n",
      "epoch: 1 step: 781, loss is 0.1751355081796646\n",
      "epoch: 1 step: 782, loss is 0.4477187991142273\n",
      "epoch: 1 step: 783, loss is 0.2874007523059845\n",
      "epoch: 1 step: 784, loss is 0.040224604308605194\n",
      "epoch: 1 step: 785, loss is 0.276521235704422\n",
      "epoch: 1 step: 786, loss is 0.04697714000940323\n",
      "epoch: 1 step: 787, loss is 0.2985549867153168\n",
      "epoch: 1 step: 788, loss is 0.052348170429468155\n",
      "epoch: 1 step: 789, loss is 0.11868391185998917\n",
      "epoch: 1 step: 790, loss is 0.3233397305011749\n",
      "epoch: 1 step: 791, loss is 0.3933010697364807\n",
      "epoch: 1 step: 792, loss is 0.19634179770946503\n",
      "epoch: 1 step: 793, loss is 0.03616613894701004\n",
      "epoch: 1 step: 794, loss is 0.22534307837486267\n",
      "epoch: 1 step: 795, loss is 0.050085604190826416\n",
      "epoch: 1 step: 796, loss is 0.027427785098552704\n",
      "epoch: 1 step: 797, loss is 0.2460775375366211\n",
      "epoch: 1 step: 798, loss is 0.08479663729667664\n",
      "epoch: 1 step: 799, loss is 0.09663375467061996\n",
      "epoch: 1 step: 800, loss is 0.10875696688890457\n",
      "epoch: 1 step: 801, loss is 0.3332958221435547\n",
      "epoch: 1 step: 802, loss is 0.11934013664722443\n",
      "epoch: 1 step: 803, loss is 0.2385345995426178\n",
      "epoch: 1 step: 804, loss is 0.0783676728606224\n",
      "epoch: 1 step: 805, loss is 0.30140629410743713\n",
      "epoch: 1 step: 806, loss is 0.2352037876844406\n",
      "epoch: 1 step: 807, loss is 0.03127685934305191\n",
      "epoch: 1 step: 808, loss is 0.24292483925819397\n",
      "epoch: 1 step: 809, loss is 0.3599114716053009\n",
      "epoch: 1 step: 810, loss is 0.09943981468677521\n",
      "epoch: 1 step: 811, loss is 0.1878688633441925\n",
      "epoch: 1 step: 812, loss is 0.29466935992240906\n",
      "epoch: 1 step: 813, loss is 0.23649613559246063\n",
      "epoch: 1 step: 814, loss is 0.0645136833190918\n",
      "epoch: 1 step: 815, loss is 0.0893440693616867\n",
      "epoch: 1 step: 816, loss is 0.21769924461841583\n",
      "epoch: 1 step: 817, loss is 0.2444067746400833\n",
      "epoch: 1 step: 818, loss is 0.1222541332244873\n",
      "epoch: 1 step: 819, loss is 0.19308403134346008\n",
      "epoch: 1 step: 820, loss is 0.5083915591239929\n",
      "epoch: 1 step: 821, loss is 0.08740341663360596\n",
      "epoch: 1 step: 822, loss is 0.13681188225746155\n",
      "epoch: 1 step: 823, loss is 0.06889276206493378\n",
      "epoch: 1 step: 824, loss is 0.1297406256198883\n",
      "epoch: 1 step: 825, loss is 0.0894356518983841\n",
      "epoch: 1 step: 826, loss is 0.15598534047603607\n",
      "epoch: 1 step: 827, loss is 0.2353293001651764\n",
      "epoch: 1 step: 828, loss is 0.23437175154685974\n",
      "epoch: 1 step: 829, loss is 0.039655908942222595\n",
      "epoch: 1 step: 830, loss is 0.01542914193123579\n",
      "epoch: 1 step: 831, loss is 0.08158392459154129\n",
      "epoch: 1 step: 832, loss is 0.21502910554409027\n",
      "epoch: 1 step: 833, loss is 0.04418589547276497\n",
      "epoch: 1 step: 834, loss is 0.05486162379384041\n",
      "epoch: 1 step: 835, loss is 0.14682133495807648\n",
      "epoch: 1 step: 836, loss is 0.1944047063589096\n",
      "epoch: 1 step: 837, loss is 0.01068906206637621\n",
      "epoch: 1 step: 838, loss is 0.13983643054962158\n",
      "epoch: 1 step: 839, loss is 0.14046385884284973\n",
      "epoch: 1 step: 840, loss is 0.017821740359067917\n",
      "epoch: 1 step: 841, loss is 0.05398855358362198\n",
      "epoch: 1 step: 842, loss is 0.2674552798271179\n",
      "epoch: 1 step: 843, loss is 0.1291860193014145\n",
      "epoch: 1 step: 844, loss is 0.05955486372113228\n",
      "epoch: 1 step: 845, loss is 0.1147480458021164\n",
      "epoch: 1 step: 846, loss is 0.011837024241685867\n",
      "epoch: 1 step: 847, loss is 0.08762949705123901\n",
      "epoch: 1 step: 848, loss is 0.565548300743103\n",
      "epoch: 1 step: 849, loss is 0.0657610073685646\n",
      "epoch: 1 step: 850, loss is 0.05680471286177635\n",
      "epoch: 1 step: 851, loss is 0.02198835276067257\n",
      "epoch: 1 step: 852, loss is 0.15291686356067657\n",
      "epoch: 1 step: 853, loss is 0.13836030662059784\n",
      "epoch: 1 step: 854, loss is 0.07068336755037308\n",
      "epoch: 1 step: 855, loss is 0.19695642590522766\n",
      "epoch: 1 step: 856, loss is 0.18876908719539642\n",
      "epoch: 1 step: 857, loss is 0.05091537535190582\n",
      "epoch: 1 step: 858, loss is 0.0848541259765625\n",
      "epoch: 1 step: 859, loss is 0.10580795258283615\n",
      "epoch: 1 step: 860, loss is 0.027830885723233223\n",
      "epoch: 1 step: 861, loss is 0.23352310061454773\n",
      "epoch: 1 step: 862, loss is 0.18920797109603882\n",
      "epoch: 1 step: 863, loss is 0.11177413910627365\n",
      "epoch: 1 step: 864, loss is 0.06048610806465149\n",
      "epoch: 1 step: 865, loss is 0.10993917286396027\n",
      "epoch: 1 step: 866, loss is 0.09336727112531662\n",
      "epoch: 1 step: 867, loss is 0.14842812716960907\n",
      "epoch: 1 step: 868, loss is 0.21771423518657684\n",
      "epoch: 1 step: 869, loss is 0.30558082461357117\n",
      "epoch: 1 step: 870, loss is 0.24838608503341675\n",
      "epoch: 1 step: 871, loss is 0.300173819065094\n",
      "epoch: 1 step: 872, loss is 0.11226452887058258\n",
      "epoch: 1 step: 873, loss is 0.22257912158966064\n",
      "epoch: 1 step: 874, loss is 0.05874725058674812\n",
      "epoch: 1 step: 875, loss is 0.2669564187526703\n",
      "epoch: 1 step: 876, loss is 0.21929635107517242\n",
      "epoch: 1 step: 877, loss is 0.38820400834083557\n",
      "epoch: 1 step: 878, loss is 0.04433387145400047\n",
      "epoch: 1 step: 879, loss is 0.11364929378032684\n",
      "epoch: 1 step: 880, loss is 0.06849367916584015\n",
      "epoch: 1 step: 881, loss is 0.08842860162258148\n",
      "epoch: 1 step: 882, loss is 0.1476617306470871\n",
      "epoch: 1 step: 883, loss is 0.08643252402544022\n",
      "epoch: 1 step: 884, loss is 0.09425167739391327\n",
      "epoch: 1 step: 885, loss is 0.16182586550712585\n",
      "epoch: 1 step: 886, loss is 0.1952337771654129\n",
      "epoch: 1 step: 887, loss is 0.10578475892543793\n",
      "epoch: 1 step: 888, loss is 0.055989205837249756\n",
      "epoch: 1 step: 889, loss is 0.18734000623226166\n",
      "epoch: 1 step: 890, loss is 0.062236037105321884\n",
      "epoch: 1 step: 891, loss is 0.2152750939130783\n",
      "epoch: 1 step: 892, loss is 0.020836202427744865\n",
      "epoch: 1 step: 893, loss is 0.14689260721206665\n",
      "epoch: 1 step: 894, loss is 0.08073453605175018\n",
      "epoch: 1 step: 895, loss is 0.126471146941185\n",
      "epoch: 1 step: 896, loss is 0.031098604202270508\n",
      "epoch: 1 step: 897, loss is 0.024415548890829086\n",
      "epoch: 1 step: 898, loss is 0.202339306473732\n",
      "epoch: 1 step: 899, loss is 0.03055650368332863\n",
      "epoch: 1 step: 900, loss is 0.2627221345901489\n",
      "epoch: 1 step: 901, loss is 0.11959249526262283\n",
      "epoch: 1 step: 902, loss is 0.0888209268450737\n",
      "epoch: 1 step: 903, loss is 0.04657363146543503\n",
      "epoch: 1 step: 904, loss is 0.020182151347398758\n",
      "epoch: 1 step: 905, loss is 0.17384390532970428\n",
      "epoch: 1 step: 906, loss is 0.08143824338912964\n",
      "epoch: 1 step: 907, loss is 0.17470219731330872\n",
      "epoch: 1 step: 908, loss is 0.2090587168931961\n",
      "epoch: 1 step: 909, loss is 0.09358146786689758\n",
      "epoch: 1 step: 910, loss is 0.2234175056219101\n",
      "epoch: 1 step: 911, loss is 0.1876348853111267\n",
      "epoch: 1 step: 912, loss is 0.037511855363845825\n",
      "epoch: 1 step: 913, loss is 0.15176384150981903\n",
      "epoch: 1 step: 914, loss is 0.06056934967637062\n",
      "epoch: 1 step: 915, loss is 0.024692393839359283\n",
      "epoch: 1 step: 916, loss is 0.1356223225593567\n",
      "epoch: 1 step: 917, loss is 0.14937318861484528\n",
      "epoch: 1 step: 918, loss is 0.22646799683570862\n",
      "epoch: 1 step: 919, loss is 0.025648707523941994\n",
      "epoch: 1 step: 920, loss is 0.005893217399716377\n",
      "epoch: 1 step: 921, loss is 0.331074059009552\n",
      "epoch: 1 step: 922, loss is 0.16522356867790222\n",
      "epoch: 1 step: 923, loss is 0.03543265536427498\n",
      "epoch: 1 step: 924, loss is 0.16040635108947754\n",
      "epoch: 1 step: 925, loss is 0.3432280123233795\n",
      "epoch: 1 step: 926, loss is 0.16766498982906342\n",
      "epoch: 1 step: 927, loss is 0.2811770737171173\n",
      "epoch: 1 step: 928, loss is 0.1132395938038826\n",
      "epoch: 1 step: 929, loss is 0.1228284239768982\n",
      "epoch: 1 step: 930, loss is 0.4568036198616028\n",
      "epoch: 1 step: 931, loss is 0.08654821664094925\n",
      "epoch: 1 step: 932, loss is 0.11882636696100235\n",
      "epoch: 1 step: 933, loss is 0.24559201300144196\n",
      "epoch: 1 step: 934, loss is 0.19186583161354065\n",
      "epoch: 1 step: 935, loss is 0.17783260345458984\n",
      "epoch: 1 step: 936, loss is 0.03811399266123772\n",
      "epoch: 1 step: 937, loss is 0.04693489149212837\n",
      "epoch: 1 step: 938, loss is 0.1068134605884552\n",
      "epoch: 1 step: 939, loss is 0.31356361508369446\n",
      "epoch: 1 step: 940, loss is 0.2753585875034332\n",
      "epoch: 1 step: 941, loss is 0.4880230128765106\n",
      "epoch: 1 step: 942, loss is 0.0985974445939064\n",
      "epoch: 1 step: 943, loss is 0.1745026558637619\n",
      "epoch: 1 step: 944, loss is 0.0729832574725151\n",
      "epoch: 1 step: 945, loss is 0.021434031426906586\n",
      "epoch: 1 step: 946, loss is 0.32492804527282715\n",
      "epoch: 1 step: 947, loss is 0.22488048672676086\n",
      "epoch: 1 step: 948, loss is 0.10740718245506287\n",
      "epoch: 1 step: 949, loss is 0.02760128863155842\n",
      "epoch: 1 step: 950, loss is 0.15661075711250305\n",
      "epoch: 1 step: 951, loss is 0.3847653269767761\n",
      "epoch: 1 step: 952, loss is 0.09474025666713715\n",
      "epoch: 1 step: 953, loss is 0.1537848860025406\n",
      "epoch: 1 step: 954, loss is 0.060464464128017426\n",
      "epoch: 1 step: 955, loss is 0.2297605574131012\n",
      "epoch: 1 step: 956, loss is 0.03926685452461243\n",
      "epoch: 1 step: 957, loss is 0.23272129893302917\n",
      "epoch: 1 step: 958, loss is 0.06469392031431198\n",
      "epoch: 1 step: 959, loss is 0.08839090168476105\n",
      "epoch: 1 step: 960, loss is 0.08450708538293839\n",
      "epoch: 1 step: 961, loss is 0.13598063588142395\n",
      "epoch: 1 step: 962, loss is 0.19339005649089813\n",
      "epoch: 1 step: 963, loss is 0.13054588437080383\n",
      "epoch: 1 step: 964, loss is 0.13099777698516846\n",
      "epoch: 1 step: 965, loss is 0.17381858825683594\n",
      "epoch: 1 step: 966, loss is 0.17376843094825745\n",
      "epoch: 1 step: 967, loss is 0.13554434478282928\n",
      "epoch: 1 step: 968, loss is 0.1119488850235939\n",
      "epoch: 1 step: 969, loss is 0.19494318962097168\n",
      "epoch: 1 step: 970, loss is 0.07426375895738602\n",
      "epoch: 1 step: 971, loss is 0.19875529408454895\n",
      "epoch: 1 step: 972, loss is 0.04845001921057701\n",
      "epoch: 1 step: 973, loss is 0.04577549174427986\n",
      "epoch: 1 step: 974, loss is 0.267063707113266\n",
      "epoch: 1 step: 975, loss is 0.19548428058624268\n",
      "epoch: 1 step: 976, loss is 0.07477094978094101\n",
      "epoch: 1 step: 977, loss is 0.08772990852594376\n",
      "epoch: 1 step: 978, loss is 0.36512482166290283\n",
      "epoch: 1 step: 979, loss is 0.017856065183877945\n",
      "epoch: 1 step: 980, loss is 0.21220339834690094\n",
      "epoch: 1 step: 981, loss is 0.32815438508987427\n",
      "epoch: 1 step: 982, loss is 0.32831263542175293\n",
      "epoch: 1 step: 983, loss is 0.23696483671665192\n",
      "epoch: 1 step: 984, loss is 0.24047566950321198\n",
      "epoch: 1 step: 985, loss is 0.17620724439620972\n",
      "epoch: 1 step: 986, loss is 0.04731190577149391\n",
      "epoch: 1 step: 987, loss is 0.3541662395000458\n",
      "epoch: 1 step: 988, loss is 0.04281853511929512\n",
      "epoch: 1 step: 989, loss is 0.027474990114569664\n",
      "epoch: 1 step: 990, loss is 0.6042101383209229\n",
      "epoch: 1 step: 991, loss is 0.07566915452480316\n",
      "epoch: 1 step: 992, loss is 0.2996595799922943\n",
      "epoch: 1 step: 993, loss is 0.14266633987426758\n",
      "epoch: 1 step: 994, loss is 0.25437527894973755\n",
      "epoch: 1 step: 995, loss is 0.1598494052886963\n",
      "epoch: 1 step: 996, loss is 0.07035881280899048\n",
      "epoch: 1 step: 997, loss is 0.14816099405288696\n",
      "epoch: 1 step: 998, loss is 0.07117190212011337\n",
      "epoch: 1 step: 999, loss is 0.0221824049949646\n",
      "epoch: 1 step: 1000, loss is 0.09479691088199615\n",
      "epoch: 1 step: 1001, loss is 0.08802517503499985\n",
      "epoch: 1 step: 1002, loss is 0.10367770493030548\n",
      "epoch: 1 step: 1003, loss is 0.1731491982936859\n",
      "epoch: 1 step: 1004, loss is 0.08567795157432556\n",
      "epoch: 1 step: 1005, loss is 0.06838249415159225\n",
      "epoch: 1 step: 1006, loss is 0.16179418563842773\n",
      "epoch: 1 step: 1007, loss is 0.05949755012989044\n",
      "epoch: 1 step: 1008, loss is 0.21774832904338837\n",
      "epoch: 1 step: 1009, loss is 0.13577060401439667\n",
      "epoch: 1 step: 1010, loss is 0.14186231791973114\n",
      "epoch: 1 step: 1011, loss is 0.05444097891449928\n",
      "epoch: 1 step: 1012, loss is 0.1948847770690918\n",
      "epoch: 1 step: 1013, loss is 0.06561706960201263\n",
      "epoch: 1 step: 1014, loss is 0.30476251244544983\n",
      "epoch: 1 step: 1015, loss is 0.09974770992994308\n",
      "epoch: 1 step: 1016, loss is 0.052632275968790054\n",
      "epoch: 1 step: 1017, loss is 0.27659064531326294\n",
      "epoch: 1 step: 1018, loss is 0.03440503031015396\n",
      "epoch: 1 step: 1019, loss is 0.19769145548343658\n",
      "epoch: 1 step: 1020, loss is 0.17479605972766876\n",
      "epoch: 1 step: 1021, loss is 0.12726148962974548\n",
      "epoch: 1 step: 1022, loss is 0.009473122656345367\n",
      "epoch: 1 step: 1023, loss is 0.0016013681888580322\n",
      "epoch: 1 step: 1024, loss is 0.021925518289208412\n",
      "epoch: 1 step: 1025, loss is 0.05034615099430084\n",
      "epoch: 1 step: 1026, loss is 0.15491612255573273\n",
      "epoch: 1 step: 1027, loss is 0.008937274105846882\n",
      "epoch: 1 step: 1028, loss is 0.12994104623794556\n",
      "epoch: 1 step: 1029, loss is 0.07020706683397293\n",
      "epoch: 1 step: 1030, loss is 0.1637634038925171\n",
      "epoch: 1 step: 1031, loss is 0.045847341418266296\n",
      "epoch: 1 step: 1032, loss is 0.12002021074295044\n",
      "epoch: 1 step: 1033, loss is 0.06535810977220535\n",
      "epoch: 1 step: 1034, loss is 0.15497496724128723\n",
      "epoch: 1 step: 1035, loss is 0.01294687483459711\n",
      "epoch: 1 step: 1036, loss is 0.324995756149292\n",
      "epoch: 1 step: 1037, loss is 0.5787203907966614\n",
      "epoch: 1 step: 1038, loss is 0.3838057219982147\n",
      "epoch: 1 step: 1039, loss is 0.11383524537086487\n",
      "epoch: 1 step: 1040, loss is 0.09727311134338379\n",
      "epoch: 1 step: 1041, loss is 0.09279931336641312\n",
      "epoch: 1 step: 1042, loss is 0.027766384184360504\n",
      "epoch: 1 step: 1043, loss is 0.045312605798244476\n",
      "epoch: 1 step: 1044, loss is 0.037201911211013794\n",
      "epoch: 1 step: 1045, loss is 0.2287818342447281\n",
      "epoch: 1 step: 1046, loss is 0.1987026184797287\n",
      "epoch: 1 step: 1047, loss is 0.019908934831619263\n",
      "epoch: 1 step: 1048, loss is 0.16255103051662445\n",
      "epoch: 1 step: 1049, loss is 0.14165520668029785\n",
      "epoch: 1 step: 1050, loss is 0.11309446394443512\n",
      "epoch: 1 step: 1051, loss is 0.1275760680437088\n",
      "epoch: 1 step: 1052, loss is 0.21122294664382935\n",
      "epoch: 1 step: 1053, loss is 0.11102400720119476\n",
      "epoch: 1 step: 1054, loss is 0.10946176201105118\n",
      "epoch: 1 step: 1055, loss is 0.05235595256090164\n",
      "epoch: 1 step: 1056, loss is 0.06204327568411827\n",
      "epoch: 1 step: 1057, loss is 0.0863836482167244\n",
      "epoch: 1 step: 1058, loss is 0.07527002692222595\n",
      "epoch: 1 step: 1059, loss is 0.012069456279277802\n",
      "epoch: 1 step: 1060, loss is 0.03236367926001549\n",
      "epoch: 1 step: 1061, loss is 0.04059353470802307\n",
      "epoch: 1 step: 1062, loss is 0.09311802685260773\n",
      "epoch: 1 step: 1063, loss is 0.03208118677139282\n",
      "epoch: 1 step: 1064, loss is 0.03980159014463425\n",
      "epoch: 1 step: 1065, loss is 0.040564119815826416\n",
      "epoch: 1 step: 1066, loss is 0.25486037135124207\n",
      "epoch: 1 step: 1067, loss is 0.18140578269958496\n",
      "epoch: 1 step: 1068, loss is 0.06721343100070953\n",
      "epoch: 1 step: 1069, loss is 0.13010604679584503\n",
      "epoch: 1 step: 1070, loss is 0.05563252791762352\n",
      "epoch: 1 step: 1071, loss is 0.1557563692331314\n",
      "epoch: 1 step: 1072, loss is 0.014009248465299606\n",
      "epoch: 1 step: 1073, loss is 0.011448166333138943\n",
      "epoch: 1 step: 1074, loss is 0.24680741131305695\n",
      "epoch: 1 step: 1075, loss is 0.006844672374427319\n",
      "epoch: 1 step: 1076, loss is 0.06773120164871216\n",
      "epoch: 1 step: 1077, loss is 0.4241085648536682\n",
      "epoch: 1 step: 1078, loss is 0.040749259293079376\n",
      "epoch: 1 step: 1079, loss is 0.13037872314453125\n",
      "epoch: 1 step: 1080, loss is 0.10260215401649475\n",
      "epoch: 1 step: 1081, loss is 0.012159273028373718\n",
      "epoch: 1 step: 1082, loss is 0.20038862526416779\n",
      "epoch: 1 step: 1083, loss is 0.11852431297302246\n",
      "epoch: 1 step: 1084, loss is 0.10851644724607468\n",
      "epoch: 1 step: 1085, loss is 0.022087223827838898\n",
      "epoch: 1 step: 1086, loss is 0.04395050182938576\n",
      "epoch: 1 step: 1087, loss is 0.08402115851640701\n",
      "epoch: 1 step: 1088, loss is 0.1789635866880417\n",
      "epoch: 1 step: 1089, loss is 0.01834728941321373\n",
      "epoch: 1 step: 1090, loss is 0.16826865077018738\n",
      "epoch: 1 step: 1091, loss is 0.14750295877456665\n",
      "epoch: 1 step: 1092, loss is 0.4052964448928833\n",
      "epoch: 1 step: 1093, loss is 0.039628319442272186\n",
      "epoch: 1 step: 1094, loss is 0.18682798743247986\n",
      "epoch: 1 step: 1095, loss is 0.10941092669963837\n",
      "epoch: 1 step: 1096, loss is 0.01920570246875286\n",
      "epoch: 1 step: 1097, loss is 0.06163136288523674\n",
      "epoch: 1 step: 1098, loss is 0.1526300460100174\n",
      "epoch: 1 step: 1099, loss is 0.7420663237571716\n",
      "epoch: 1 step: 1100, loss is 0.166277676820755\n",
      "epoch: 1 step: 1101, loss is 0.08308455348014832\n",
      "epoch: 1 step: 1102, loss is 0.04406319558620453\n",
      "epoch: 1 step: 1103, loss is 0.08707279711961746\n",
      "epoch: 1 step: 1104, loss is 0.5973227620124817\n",
      "epoch: 1 step: 1105, loss is 0.11369894444942474\n",
      "epoch: 1 step: 1106, loss is 0.04872683808207512\n",
      "epoch: 1 step: 1107, loss is 0.020138366147875786\n",
      "epoch: 1 step: 1108, loss is 0.09366214275360107\n",
      "epoch: 1 step: 1109, loss is 0.21496301889419556\n",
      "epoch: 1 step: 1110, loss is 0.06611309945583344\n",
      "epoch: 1 step: 1111, loss is 0.18863248825073242\n",
      "epoch: 1 step: 1112, loss is 0.11335527896881104\n",
      "epoch: 1 step: 1113, loss is 0.05798890069127083\n",
      "epoch: 1 step: 1114, loss is 0.052474912256002426\n",
      "epoch: 1 step: 1115, loss is 0.3580433428287506\n",
      "epoch: 1 step: 1116, loss is 0.18582366406917572\n",
      "epoch: 1 step: 1117, loss is 0.08801328390836716\n",
      "epoch: 1 step: 1118, loss is 0.0643056184053421\n",
      "epoch: 1 step: 1119, loss is 0.22278530895709991\n",
      "epoch: 1 step: 1120, loss is 0.3071773648262024\n",
      "epoch: 1 step: 1121, loss is 0.10561583936214447\n",
      "epoch: 1 step: 1122, loss is 0.05310669541358948\n",
      "epoch: 1 step: 1123, loss is 0.018407320603728294\n",
      "epoch: 1 step: 1124, loss is 0.010051448829472065\n",
      "epoch: 1 step: 1125, loss is 0.07602588832378387\n",
      "epoch: 1 step: 1126, loss is 0.08171127736568451\n",
      "epoch: 1 step: 1127, loss is 0.3389241695404053\n",
      "epoch: 1 step: 1128, loss is 0.22443847358226776\n",
      "epoch: 1 step: 1129, loss is 0.019753646105527878\n",
      "epoch: 1 step: 1130, loss is 0.11741414666175842\n",
      "epoch: 1 step: 1131, loss is 0.02944885566830635\n",
      "epoch: 1 step: 1132, loss is 0.04708964377641678\n",
      "epoch: 1 step: 1133, loss is 0.36264336109161377\n",
      "epoch: 1 step: 1134, loss is 0.018296392634510994\n",
      "epoch: 1 step: 1135, loss is 0.0804104134440422\n",
      "epoch: 1 step: 1136, loss is 0.04469740763306618\n",
      "epoch: 1 step: 1137, loss is 0.05053245276212692\n",
      "epoch: 1 step: 1138, loss is 0.12222626805305481\n",
      "epoch: 1 step: 1139, loss is 0.15251056849956512\n",
      "epoch: 1 step: 1140, loss is 0.08849729597568512\n",
      "epoch: 1 step: 1141, loss is 0.13470187783241272\n",
      "epoch: 1 step: 1142, loss is 0.0951809361577034\n",
      "epoch: 1 step: 1143, loss is 0.04922204092144966\n",
      "epoch: 1 step: 1144, loss is 0.10540992766618729\n",
      "epoch: 1 step: 1145, loss is 0.01655842736363411\n",
      "epoch: 1 step: 1146, loss is 0.05231563746929169\n",
      "epoch: 1 step: 1147, loss is 0.030296780169010162\n",
      "epoch: 1 step: 1148, loss is 0.0036294166930019855\n",
      "epoch: 1 step: 1149, loss is 0.016403036192059517\n",
      "epoch: 1 step: 1150, loss is 0.0071882689371705055\n",
      "epoch: 1 step: 1151, loss is 0.05808870494365692\n",
      "epoch: 1 step: 1152, loss is 0.14737944304943085\n",
      "epoch: 1 step: 1153, loss is 0.04440341144800186\n",
      "epoch: 1 step: 1154, loss is 0.02654682844877243\n",
      "epoch: 1 step: 1155, loss is 0.03498521447181702\n",
      "epoch: 1 step: 1156, loss is 0.11096677929162979\n",
      "epoch: 1 step: 1157, loss is 0.01620558835566044\n",
      "epoch: 1 step: 1158, loss is 0.028864959254860878\n",
      "epoch: 1 step: 1159, loss is 0.0502006970345974\n",
      "epoch: 1 step: 1160, loss is 0.010089448653161526\n",
      "epoch: 1 step: 1161, loss is 0.06910578906536102\n",
      "epoch: 1 step: 1162, loss is 0.006929770112037659\n",
      "epoch: 1 step: 1163, loss is 0.18270665407180786\n",
      "epoch: 1 step: 1164, loss is 0.15931645035743713\n",
      "epoch: 1 step: 1165, loss is 0.010309385135769844\n",
      "epoch: 1 step: 1166, loss is 0.08770693093538284\n",
      "epoch: 1 step: 1167, loss is 0.022996854037046432\n",
      "epoch: 1 step: 1168, loss is 0.011866339482367039\n",
      "epoch: 1 step: 1169, loss is 0.047521136701107025\n",
      "epoch: 1 step: 1170, loss is 0.015597634017467499\n",
      "epoch: 1 step: 1171, loss is 0.015008015558123589\n",
      "epoch: 1 step: 1172, loss is 0.40626850724220276\n",
      "epoch: 1 step: 1173, loss is 0.20677168667316437\n",
      "epoch: 1 step: 1174, loss is 0.016967542469501495\n",
      "epoch: 1 step: 1175, loss is 0.1287679225206375\n",
      "epoch: 1 step: 1176, loss is 0.2743050158023834\n",
      "epoch: 1 step: 1177, loss is 0.0036655303556472063\n",
      "epoch: 1 step: 1178, loss is 0.10631715506315231\n",
      "epoch: 1 step: 1179, loss is 0.03425797075033188\n",
      "epoch: 1 step: 1180, loss is 0.32825928926467896\n",
      "epoch: 1 step: 1181, loss is 0.23051095008850098\n",
      "epoch: 1 step: 1182, loss is 0.04901246726512909\n",
      "epoch: 1 step: 1183, loss is 0.11997165530920029\n",
      "epoch: 1 step: 1184, loss is 0.03180919215083122\n",
      "epoch: 1 step: 1185, loss is 0.5091477036476135\n",
      "epoch: 1 step: 1186, loss is 0.25999006628990173\n",
      "epoch: 1 step: 1187, loss is 0.08945713937282562\n",
      "epoch: 1 step: 1188, loss is 0.399044007062912\n",
      "epoch: 1 step: 1189, loss is 0.0042226267978549\n",
      "epoch: 1 step: 1190, loss is 0.21580825746059418\n",
      "epoch: 1 step: 1191, loss is 0.04305366054177284\n",
      "epoch: 1 step: 1192, loss is 0.20897673070430756\n",
      "epoch: 1 step: 1193, loss is 0.5392404198646545\n",
      "epoch: 1 step: 1194, loss is 0.02748377062380314\n",
      "epoch: 1 step: 1195, loss is 0.08997304737567902\n",
      "epoch: 1 step: 1196, loss is 0.024838028475642204\n",
      "epoch: 1 step: 1197, loss is 0.00955576729029417\n",
      "epoch: 1 step: 1198, loss is 0.16342693567276\n",
      "epoch: 1 step: 1199, loss is 0.032631102949380875\n",
      "epoch: 1 step: 1200, loss is 0.34959131479263306\n",
      "epoch: 1 step: 1201, loss is 0.029469111934304237\n",
      "epoch: 1 step: 1202, loss is 0.02076965756714344\n",
      "epoch: 1 step: 1203, loss is 0.3869680166244507\n",
      "epoch: 1 step: 1204, loss is 0.042175471782684326\n",
      "epoch: 1 step: 1205, loss is 0.4522894024848938\n",
      "epoch: 1 step: 1206, loss is 0.039344124495983124\n",
      "epoch: 1 step: 1207, loss is 0.10643596947193146\n",
      "epoch: 1 step: 1208, loss is 0.039815597236156464\n",
      "epoch: 1 step: 1209, loss is 0.13958163559436798\n",
      "epoch: 1 step: 1210, loss is 0.06513764709234238\n",
      "epoch: 1 step: 1211, loss is 0.17642538249492645\n",
      "epoch: 1 step: 1212, loss is 0.094134621322155\n",
      "epoch: 1 step: 1213, loss is 0.027400650084018707\n",
      "epoch: 1 step: 1214, loss is 0.049296144396066666\n",
      "epoch: 1 step: 1215, loss is 0.026468981057405472\n",
      "epoch: 1 step: 1216, loss is 0.28433099389076233\n",
      "epoch: 1 step: 1217, loss is 0.1401684731245041\n",
      "epoch: 1 step: 1218, loss is 0.021030372008681297\n",
      "epoch: 1 step: 1219, loss is 0.07020372897386551\n",
      "epoch: 1 step: 1220, loss is 0.08784529566764832\n",
      "epoch: 1 step: 1221, loss is 0.016758354380726814\n",
      "epoch: 1 step: 1222, loss is 0.061604082584381104\n",
      "epoch: 1 step: 1223, loss is 0.10127606242895126\n",
      "epoch: 1 step: 1224, loss is 0.3142133057117462\n",
      "epoch: 1 step: 1225, loss is 0.03155287355184555\n",
      "epoch: 1 step: 1226, loss is 0.04228401929140091\n",
      "epoch: 1 step: 1227, loss is 0.2718713581562042\n",
      "epoch: 1 step: 1228, loss is 0.060920778661966324\n",
      "epoch: 1 step: 1229, loss is 0.08358436822891235\n",
      "epoch: 1 step: 1230, loss is 0.11818969249725342\n",
      "epoch: 1 step: 1231, loss is 0.058390092104673386\n",
      "epoch: 1 step: 1232, loss is 0.23292888700962067\n",
      "epoch: 1 step: 1233, loss is 0.1507757604122162\n",
      "epoch: 1 step: 1234, loss is 0.05814530700445175\n",
      "epoch: 1 step: 1235, loss is 0.006663001142442226\n",
      "epoch: 1 step: 1236, loss is 0.16131795942783356\n",
      "epoch: 1 step: 1237, loss is 0.006923364941030741\n",
      "epoch: 1 step: 1238, loss is 0.04401512071490288\n",
      "epoch: 1 step: 1239, loss is 0.12288796156644821\n",
      "epoch: 1 step: 1240, loss is 0.06220976263284683\n",
      "epoch: 1 step: 1241, loss is 0.007553951349109411\n",
      "epoch: 1 step: 1242, loss is 0.03218606114387512\n",
      "epoch: 1 step: 1243, loss is 0.03156673535704613\n",
      "epoch: 1 step: 1244, loss is 0.0009142237831838429\n",
      "epoch: 1 step: 1245, loss is 0.04919186979532242\n",
      "epoch: 1 step: 1246, loss is 0.14596053957939148\n",
      "epoch: 1 step: 1247, loss is 0.02268650010228157\n",
      "epoch: 1 step: 1248, loss is 0.1304791271686554\n",
      "epoch: 1 step: 1249, loss is 0.055529940873384476\n",
      "epoch: 1 step: 1250, loss is 0.16770407557487488\n",
      "epoch: 1 step: 1251, loss is 0.20910343527793884\n",
      "epoch: 1 step: 1252, loss is 0.0036373764742165804\n",
      "epoch: 1 step: 1253, loss is 0.3248986303806305\n",
      "epoch: 1 step: 1254, loss is 0.14183367788791656\n",
      "epoch: 1 step: 1255, loss is 0.18944263458251953\n",
      "epoch: 1 step: 1256, loss is 0.1257006973028183\n",
      "epoch: 1 step: 1257, loss is 0.19581344723701477\n",
      "epoch: 1 step: 1258, loss is 0.0837898775935173\n",
      "epoch: 1 step: 1259, loss is 0.060113802552223206\n",
      "epoch: 1 step: 1260, loss is 0.13479302823543549\n",
      "epoch: 1 step: 1261, loss is 0.039736557751894\n",
      "epoch: 1 step: 1262, loss is 0.19703182578086853\n",
      "epoch: 1 step: 1263, loss is 0.008736366406083107\n",
      "epoch: 1 step: 1264, loss is 0.09602963179349899\n",
      "epoch: 1 step: 1265, loss is 0.42893311381340027\n",
      "epoch: 1 step: 1266, loss is 0.08975169062614441\n",
      "epoch: 1 step: 1267, loss is 0.04488268122076988\n",
      "epoch: 1 step: 1268, loss is 0.1128130853176117\n",
      "epoch: 1 step: 1269, loss is 0.08074945211410522\n",
      "epoch: 1 step: 1270, loss is 0.3621870279312134\n",
      "epoch: 1 step: 1271, loss is 0.21091033518314362\n",
      "epoch: 1 step: 1272, loss is 0.014100473374128342\n",
      "epoch: 1 step: 1273, loss is 0.08008124679327011\n",
      "epoch: 1 step: 1274, loss is 0.04186157509684563\n",
      "epoch: 1 step: 1275, loss is 0.19231103360652924\n",
      "epoch: 1 step: 1276, loss is 0.17252127826213837\n",
      "epoch: 1 step: 1277, loss is 0.09194901585578918\n",
      "epoch: 1 step: 1278, loss is 0.05868906155228615\n",
      "epoch: 1 step: 1279, loss is 0.04922507703304291\n",
      "epoch: 1 step: 1280, loss is 0.011684795841574669\n",
      "epoch: 1 step: 1281, loss is 0.23868714272975922\n",
      "epoch: 1 step: 1282, loss is 0.03007030487060547\n",
      "epoch: 1 step: 1283, loss is 0.009399453178048134\n",
      "epoch: 1 step: 1284, loss is 0.04737623408436775\n",
      "epoch: 1 step: 1285, loss is 0.022880706936120987\n",
      "epoch: 1 step: 1286, loss is 0.3646436035633087\n",
      "epoch: 1 step: 1287, loss is 0.061723027378320694\n",
      "epoch: 1 step: 1288, loss is 0.013359912671148777\n",
      "epoch: 1 step: 1289, loss is 0.07997753471136093\n",
      "epoch: 1 step: 1290, loss is 0.042111095041036606\n",
      "epoch: 1 step: 1291, loss is 0.027129147201776505\n",
      "epoch: 1 step: 1292, loss is 0.023493263870477676\n",
      "epoch: 1 step: 1293, loss is 0.011886043474078178\n",
      "epoch: 1 step: 1294, loss is 0.05973194167017937\n",
      "epoch: 1 step: 1295, loss is 0.06412463635206223\n",
      "epoch: 1 step: 1296, loss is 0.0063776373863220215\n",
      "epoch: 1 step: 1297, loss is 0.1381932497024536\n",
      "epoch: 1 step: 1298, loss is 0.007537118159234524\n",
      "epoch: 1 step: 1299, loss is 0.0022876153234392405\n",
      "epoch: 1 step: 1300, loss is 0.03598719835281372\n",
      "epoch: 1 step: 1301, loss is 0.03989424556493759\n",
      "epoch: 1 step: 1302, loss is 0.17425046861171722\n",
      "epoch: 1 step: 1303, loss is 0.002728742780163884\n",
      "epoch: 1 step: 1304, loss is 0.20227710902690887\n",
      "epoch: 1 step: 1305, loss is 0.01992659643292427\n",
      "epoch: 1 step: 1306, loss is 0.052643489092588425\n",
      "epoch: 1 step: 1307, loss is 0.25210410356521606\n",
      "epoch: 1 step: 1308, loss is 0.0066890413872897625\n",
      "epoch: 1 step: 1309, loss is 0.0030754092149436474\n",
      "epoch: 1 step: 1310, loss is 0.02292829006910324\n",
      "epoch: 1 step: 1311, loss is 0.019920039921998978\n",
      "epoch: 1 step: 1312, loss is 0.16365310549736023\n",
      "epoch: 1 step: 1313, loss is 0.05793464928865433\n",
      "epoch: 1 step: 1314, loss is 0.22914911806583405\n",
      "epoch: 1 step: 1315, loss is 0.07593409717082977\n",
      "epoch: 1 step: 1316, loss is 0.19510433077812195\n",
      "epoch: 1 step: 1317, loss is 0.27856171131134033\n",
      "epoch: 1 step: 1318, loss is 0.3130381405353546\n",
      "epoch: 1 step: 1319, loss is 0.03839772939682007\n",
      "epoch: 1 step: 1320, loss is 0.053638216108083725\n",
      "epoch: 1 step: 1321, loss is 0.043847307562828064\n",
      "epoch: 1 step: 1322, loss is 0.019525861367583275\n",
      "epoch: 1 step: 1323, loss is 0.11506085842847824\n",
      "epoch: 1 step: 1324, loss is 0.1283908635377884\n",
      "epoch: 1 step: 1325, loss is 0.05045301467180252\n",
      "epoch: 1 step: 1326, loss is 0.228581503033638\n",
      "epoch: 1 step: 1327, loss is 0.2737002372741699\n",
      "epoch: 1 step: 1328, loss is 0.025841863825917244\n",
      "epoch: 1 step: 1329, loss is 0.058157481253147125\n",
      "epoch: 1 step: 1330, loss is 0.09244714677333832\n",
      "epoch: 1 step: 1331, loss is 0.01319052278995514\n",
      "epoch: 1 step: 1332, loss is 0.09564775973558426\n",
      "epoch: 1 step: 1333, loss is 0.07135159522294998\n",
      "epoch: 1 step: 1334, loss is 0.29583436250686646\n",
      "epoch: 1 step: 1335, loss is 0.15125785768032074\n",
      "epoch: 1 step: 1336, loss is 0.02533290907740593\n",
      "epoch: 1 step: 1337, loss is 0.13942207396030426\n",
      "epoch: 1 step: 1338, loss is 0.09481754899024963\n",
      "epoch: 1 step: 1339, loss is 0.021663960069417953\n",
      "epoch: 1 step: 1340, loss is 0.02064729854464531\n",
      "epoch: 1 step: 1341, loss is 0.07882127165794373\n",
      "epoch: 1 step: 1342, loss is 0.010035100392997265\n",
      "epoch: 1 step: 1343, loss is 0.04756181314587593\n",
      "epoch: 1 step: 1344, loss is 0.13049277663230896\n",
      "epoch: 1 step: 1345, loss is 0.16662068665027618\n",
      "epoch: 1 step: 1346, loss is 0.03115634247660637\n",
      "epoch: 1 step: 1347, loss is 0.03406814858317375\n",
      "epoch: 1 step: 1348, loss is 0.2507525384426117\n",
      "epoch: 1 step: 1349, loss is 0.05660415068268776\n",
      "epoch: 1 step: 1350, loss is 0.14951899647712708\n",
      "epoch: 1 step: 1351, loss is 0.08931082487106323\n",
      "epoch: 1 step: 1352, loss is 0.013173287734389305\n",
      "epoch: 1 step: 1353, loss is 0.1619199514389038\n",
      "epoch: 1 step: 1354, loss is 0.09433393180370331\n",
      "epoch: 1 step: 1355, loss is 0.2013644278049469\n",
      "epoch: 1 step: 1356, loss is 0.009869802743196487\n",
      "epoch: 1 step: 1357, loss is 0.023182323202490807\n",
      "epoch: 1 step: 1358, loss is 0.1855810284614563\n",
      "epoch: 1 step: 1359, loss is 0.016703937202692032\n",
      "epoch: 1 step: 1360, loss is 0.06495829671621323\n",
      "epoch: 1 step: 1361, loss is 0.0039220452308654785\n",
      "epoch: 1 step: 1362, loss is 0.03822040930390358\n",
      "epoch: 1 step: 1363, loss is 0.006696712225675583\n",
      "epoch: 1 step: 1364, loss is 0.028598958626389503\n",
      "epoch: 1 step: 1365, loss is 0.007511497009545565\n",
      "epoch: 1 step: 1366, loss is 0.05551043152809143\n",
      "epoch: 1 step: 1367, loss is 0.15456266701221466\n",
      "epoch: 1 step: 1368, loss is 0.04322594404220581\n",
      "epoch: 1 step: 1369, loss is 0.014536061324179173\n",
      "epoch: 1 step: 1370, loss is 0.02950350195169449\n",
      "epoch: 1 step: 1371, loss is 0.055458225309848785\n",
      "epoch: 1 step: 1372, loss is 0.031175769865512848\n",
      "epoch: 1 step: 1373, loss is 0.0235135555267334\n",
      "epoch: 1 step: 1374, loss is 0.030741402879357338\n",
      "epoch: 1 step: 1375, loss is 0.14592213928699493\n",
      "epoch: 1 step: 1376, loss is 0.09064774960279465\n",
      "epoch: 1 step: 1377, loss is 0.04844847694039345\n",
      "epoch: 1 step: 1378, loss is 0.02301919274032116\n",
      "epoch: 1 step: 1379, loss is 0.06133827939629555\n",
      "epoch: 1 step: 1380, loss is 0.02127760648727417\n",
      "epoch: 1 step: 1381, loss is 0.01025551650673151\n",
      "epoch: 1 step: 1382, loss is 0.020910348743200302\n",
      "epoch: 1 step: 1383, loss is 0.17176105082035065\n",
      "epoch: 1 step: 1384, loss is 0.0274973027408123\n",
      "epoch: 1 step: 1385, loss is 0.050130799412727356\n",
      "epoch: 1 step: 1386, loss is 0.04500821605324745\n",
      "epoch: 1 step: 1387, loss is 0.13415321707725525\n",
      "epoch: 1 step: 1388, loss is 0.036818020045757294\n",
      "epoch: 1 step: 1389, loss is 0.022671088576316833\n",
      "epoch: 1 step: 1390, loss is 0.17686200141906738\n",
      "epoch: 1 step: 1391, loss is 0.00877360813319683\n",
      "epoch: 1 step: 1392, loss is 0.14007502794265747\n",
      "epoch: 1 step: 1393, loss is 0.04937423765659332\n",
      "epoch: 1 step: 1394, loss is 0.08976083248853683\n",
      "epoch: 1 step: 1395, loss is 0.035262782126665115\n",
      "epoch: 1 step: 1396, loss is 0.12037702649831772\n",
      "epoch: 1 step: 1397, loss is 0.2660771906375885\n",
      "epoch: 1 step: 1398, loss is 0.17604891955852509\n",
      "epoch: 1 step: 1399, loss is 0.020707448944449425\n",
      "epoch: 1 step: 1400, loss is 0.050726957619190216\n",
      "epoch: 1 step: 1401, loss is 0.011823337525129318\n",
      "epoch: 1 step: 1402, loss is 0.0440603606402874\n",
      "epoch: 1 step: 1403, loss is 0.07779082655906677\n",
      "epoch: 1 step: 1404, loss is 0.0867556780576706\n",
      "epoch: 1 step: 1405, loss is 0.009914722293615341\n",
      "epoch: 1 step: 1406, loss is 0.11362086236476898\n",
      "epoch: 1 step: 1407, loss is 0.2314150035381317\n",
      "epoch: 1 step: 1408, loss is 0.3129551410675049\n",
      "epoch: 1 step: 1409, loss is 0.05227800831198692\n",
      "epoch: 1 step: 1410, loss is 0.04380152374505997\n",
      "epoch: 1 step: 1411, loss is 0.4326930046081543\n",
      "epoch: 1 step: 1412, loss is 0.05604485794901848\n",
      "epoch: 1 step: 1413, loss is 0.16374613344669342\n",
      "epoch: 1 step: 1414, loss is 0.04191446304321289\n",
      "epoch: 1 step: 1415, loss is 0.16858617961406708\n",
      "epoch: 1 step: 1416, loss is 0.0645286813378334\n",
      "epoch: 1 step: 1417, loss is 0.18719108402729034\n",
      "epoch: 1 step: 1418, loss is 0.09751122444868088\n",
      "epoch: 1 step: 1419, loss is 0.029512928798794746\n",
      "epoch: 1 step: 1420, loss is 0.09974651783704758\n",
      "epoch: 1 step: 1421, loss is 0.10917945951223373\n",
      "epoch: 1 step: 1422, loss is 0.05705422908067703\n",
      "epoch: 1 step: 1423, loss is 0.15755237638950348\n",
      "epoch: 1 step: 1424, loss is 0.0415123850107193\n",
      "epoch: 1 step: 1425, loss is 0.0229325070977211\n",
      "epoch: 1 step: 1426, loss is 0.07427968084812164\n",
      "epoch: 1 step: 1427, loss is 0.017265917733311653\n",
      "epoch: 1 step: 1428, loss is 0.0058188592083752155\n",
      "epoch: 1 step: 1429, loss is 0.2486097365617752\n",
      "epoch: 1 step: 1430, loss is 0.111382856965065\n",
      "epoch: 1 step: 1431, loss is 0.2233291119337082\n",
      "epoch: 1 step: 1432, loss is 0.03889050334692001\n",
      "epoch: 1 step: 1433, loss is 0.050463855266571045\n",
      "epoch: 1 step: 1434, loss is 0.09214392304420471\n",
      "epoch: 1 step: 1435, loss is 0.15088167786598206\n",
      "epoch: 1 step: 1436, loss is 0.01791028305888176\n",
      "epoch: 1 step: 1437, loss is 0.23070189356803894\n",
      "epoch: 1 step: 1438, loss is 0.040741462260484695\n",
      "epoch: 1 step: 1439, loss is 0.013612963259220123\n",
      "epoch: 1 step: 1440, loss is 0.06584179401397705\n",
      "epoch: 1 step: 1441, loss is 0.044728003442287445\n",
      "epoch: 1 step: 1442, loss is 0.2789648175239563\n",
      "epoch: 1 step: 1443, loss is 0.36417442560195923\n",
      "epoch: 1 step: 1444, loss is 0.03202202171087265\n",
      "epoch: 1 step: 1445, loss is 0.1667889505624771\n",
      "epoch: 1 step: 1446, loss is 0.01307778712362051\n",
      "epoch: 1 step: 1447, loss is 0.26806196570396423\n",
      "epoch: 1 step: 1448, loss is 0.036599136888980865\n",
      "epoch: 1 step: 1449, loss is 0.09469544142484665\n",
      "epoch: 1 step: 1450, loss is 0.018592720851302147\n",
      "epoch: 1 step: 1451, loss is 0.03568243235349655\n",
      "epoch: 1 step: 1452, loss is 0.027315370738506317\n",
      "epoch: 1 step: 1453, loss is 0.3382056951522827\n",
      "epoch: 1 step: 1454, loss is 0.16222870349884033\n",
      "epoch: 1 step: 1455, loss is 0.049807872623205185\n",
      "epoch: 1 step: 1456, loss is 0.06272265315055847\n",
      "epoch: 1 step: 1457, loss is 0.047167643904685974\n",
      "epoch: 1 step: 1458, loss is 0.027772560715675354\n",
      "epoch: 1 step: 1459, loss is 0.15398123860359192\n",
      "epoch: 1 step: 1460, loss is 0.04768078774213791\n",
      "epoch: 1 step: 1461, loss is 0.13840408623218536\n",
      "epoch: 1 step: 1462, loss is 0.05830201506614685\n",
      "epoch: 1 step: 1463, loss is 0.008925396017730236\n",
      "epoch: 1 step: 1464, loss is 0.030000347644090652\n",
      "epoch: 1 step: 1465, loss is 0.259921133518219\n",
      "epoch: 1 step: 1466, loss is 0.03946271166205406\n",
      "epoch: 1 step: 1467, loss is 0.03883451595902443\n",
      "epoch: 1 step: 1468, loss is 0.025086412206292152\n",
      "epoch: 1 step: 1469, loss is 0.0944526344537735\n",
      "epoch: 1 step: 1470, loss is 0.09826776385307312\n",
      "epoch: 1 step: 1471, loss is 0.03589016944169998\n",
      "epoch: 1 step: 1472, loss is 0.36337870359420776\n",
      "epoch: 1 step: 1473, loss is 0.029294578358530998\n",
      "epoch: 1 step: 1474, loss is 0.032269641757011414\n",
      "epoch: 1 step: 1475, loss is 0.004704197868704796\n",
      "epoch: 1 step: 1476, loss is 0.02591666579246521\n",
      "epoch: 1 step: 1477, loss is 0.06472852826118469\n",
      "epoch: 1 step: 1478, loss is 0.020941112190485\n",
      "epoch: 1 step: 1479, loss is 0.04092073440551758\n",
      "epoch: 1 step: 1480, loss is 0.22862355411052704\n",
      "epoch: 1 step: 1481, loss is 0.0937076285481453\n",
      "epoch: 1 step: 1482, loss is 0.017649175599217415\n",
      "epoch: 1 step: 1483, loss is 0.08702411502599716\n",
      "epoch: 1 step: 1484, loss is 0.1536678969860077\n",
      "epoch: 1 step: 1485, loss is 0.03310209512710571\n",
      "epoch: 1 step: 1486, loss is 0.04065738618373871\n",
      "epoch: 1 step: 1487, loss is 0.64146888256073\n",
      "epoch: 1 step: 1488, loss is 0.0581834502518177\n",
      "epoch: 1 step: 1489, loss is 0.01751115545630455\n",
      "epoch: 1 step: 1490, loss is 0.0016081429785117507\n",
      "epoch: 1 step: 1491, loss is 0.06600555032491684\n",
      "epoch: 1 step: 1492, loss is 0.01563255675137043\n",
      "epoch: 1 step: 1493, loss is 0.05556600168347359\n",
      "epoch: 1 step: 1494, loss is 0.1317410171031952\n",
      "epoch: 1 step: 1495, loss is 0.034600455313920975\n",
      "epoch: 1 step: 1496, loss is 0.006400716491043568\n",
      "epoch: 1 step: 1497, loss is 0.007079622708261013\n",
      "epoch: 1 step: 1498, loss is 0.041293900460004807\n",
      "epoch: 1 step: 1499, loss is 0.0068715945817530155\n",
      "epoch: 1 step: 1500, loss is 0.16469664871692657\n",
      "epoch: 1 step: 1501, loss is 0.022565310820937157\n",
      "epoch: 1 step: 1502, loss is 0.08821740001440048\n",
      "epoch: 1 step: 1503, loss is 0.014133559539914131\n",
      "epoch: 1 step: 1504, loss is 0.003956980537623167\n",
      "epoch: 1 step: 1505, loss is 0.042119596153497696\n",
      "epoch: 1 step: 1506, loss is 0.012561077252030373\n",
      "epoch: 1 step: 1507, loss is 0.06675564497709274\n",
      "epoch: 1 step: 1508, loss is 0.01708270236849785\n",
      "epoch: 1 step: 1509, loss is 0.09604499489068985\n",
      "epoch: 1 step: 1510, loss is 0.08491799980401993\n",
      "epoch: 1 step: 1511, loss is 0.011128445155918598\n",
      "epoch: 1 step: 1512, loss is 0.1092202216386795\n",
      "epoch: 1 step: 1513, loss is 0.07859562337398529\n",
      "epoch: 1 step: 1514, loss is 0.1407584398984909\n",
      "epoch: 1 step: 1515, loss is 0.020964598283171654\n",
      "epoch: 1 step: 1516, loss is 0.15403760969638824\n",
      "epoch: 1 step: 1517, loss is 0.028777342289686203\n",
      "epoch: 1 step: 1518, loss is 0.044590286910533905\n",
      "epoch: 1 step: 1519, loss is 0.4371703565120697\n",
      "epoch: 1 step: 1520, loss is 0.05799456313252449\n",
      "epoch: 1 step: 1521, loss is 0.231008380651474\n",
      "epoch: 1 step: 1522, loss is 0.01574619486927986\n",
      "epoch: 1 step: 1523, loss is 0.020674986764788628\n",
      "epoch: 1 step: 1524, loss is 0.18881066143512726\n",
      "epoch: 1 step: 1525, loss is 0.009324450977146626\n",
      "epoch: 1 step: 1526, loss is 0.051608581095933914\n",
      "epoch: 1 step: 1527, loss is 0.04741504043340683\n",
      "epoch: 1 step: 1528, loss is 0.27528393268585205\n",
      "epoch: 1 step: 1529, loss is 0.13861283659934998\n",
      "epoch: 1 step: 1530, loss is 0.07960730791091919\n",
      "epoch: 1 step: 1531, loss is 0.1281125694513321\n",
      "epoch: 1 step: 1532, loss is 0.019708340987563133\n",
      "epoch: 1 step: 1533, loss is 0.06556418538093567\n",
      "epoch: 1 step: 1534, loss is 0.11845938116312027\n",
      "epoch: 1 step: 1535, loss is 0.05969219654798508\n",
      "epoch: 1 step: 1536, loss is 0.1293918937444687\n",
      "epoch: 1 step: 1537, loss is 0.17587099969387054\n",
      "epoch: 1 step: 1538, loss is 0.023271065205335617\n",
      "epoch: 1 step: 1539, loss is 0.015897653996944427\n",
      "epoch: 1 step: 1540, loss is 0.07481614500284195\n",
      "epoch: 1 step: 1541, loss is 0.07420799136161804\n",
      "epoch: 1 step: 1542, loss is 0.1053372472524643\n",
      "epoch: 1 step: 1543, loss is 0.017763791605830193\n",
      "epoch: 1 step: 1544, loss is 0.040355175733566284\n",
      "epoch: 1 step: 1545, loss is 0.3209623694419861\n",
      "epoch: 1 step: 1546, loss is 0.1041250005364418\n",
      "epoch: 1 step: 1547, loss is 0.052578434348106384\n",
      "epoch: 1 step: 1548, loss is 0.09897216409444809\n",
      "epoch: 1 step: 1549, loss is 0.04268714040517807\n",
      "epoch: 1 step: 1550, loss is 0.07971738278865814\n",
      "epoch: 1 step: 1551, loss is 0.06118203327059746\n",
      "epoch: 1 step: 1552, loss is 0.1759185940027237\n",
      "epoch: 1 step: 1553, loss is 0.021796097978949547\n",
      "epoch: 1 step: 1554, loss is 0.02785760723054409\n",
      "epoch: 1 step: 1555, loss is 0.0866435244679451\n",
      "epoch: 1 step: 1556, loss is 0.0029490292072296143\n",
      "epoch: 1 step: 1557, loss is 0.08326337486505508\n",
      "epoch: 1 step: 1558, loss is 0.0415269136428833\n",
      "epoch: 1 step: 1559, loss is 0.2231215387582779\n",
      "epoch: 1 step: 1560, loss is 0.1575787216424942\n",
      "epoch: 1 step: 1561, loss is 0.12817294895648956\n",
      "epoch: 1 step: 1562, loss is 0.20559902489185333\n",
      "epoch: 1 step: 1563, loss is 0.02281620353460312\n",
      "epoch: 1 step: 1564, loss is 0.018422182649374008\n",
      "epoch: 1 step: 1565, loss is 0.0983750969171524\n",
      "epoch: 1 step: 1566, loss is 0.01721538044512272\n",
      "epoch: 1 step: 1567, loss is 0.19430282711982727\n",
      "epoch: 1 step: 1568, loss is 0.010262814350426197\n",
      "epoch: 1 step: 1569, loss is 0.045479483902454376\n",
      "epoch: 1 step: 1570, loss is 0.009253855794668198\n",
      "epoch: 1 step: 1571, loss is 0.0033951487857848406\n",
      "epoch: 1 step: 1572, loss is 0.029425377026200294\n",
      "epoch: 1 step: 1573, loss is 0.037211235612630844\n",
      "epoch: 1 step: 1574, loss is 0.08106805384159088\n",
      "epoch: 1 step: 1575, loss is 0.08507073670625687\n",
      "epoch: 1 step: 1576, loss is 0.10830769687891006\n",
      "epoch: 1 step: 1577, loss is 0.05432438477873802\n",
      "epoch: 1 step: 1578, loss is 0.14843110740184784\n",
      "epoch: 1 step: 1579, loss is 0.36384835839271545\n",
      "epoch: 1 step: 1580, loss is 0.20977024734020233\n",
      "epoch: 1 step: 1581, loss is 0.04042397812008858\n",
      "epoch: 1 step: 1582, loss is 0.02527119219303131\n",
      "epoch: 1 step: 1583, loss is 0.05372515693306923\n",
      "epoch: 1 step: 1584, loss is 0.13232004642486572\n",
      "epoch: 1 step: 1585, loss is 0.014342143200337887\n",
      "epoch: 1 step: 1586, loss is 0.21963924169540405\n",
      "epoch: 1 step: 1587, loss is 0.02861330658197403\n",
      "epoch: 1 step: 1588, loss is 0.06594625860452652\n",
      "epoch: 1 step: 1589, loss is 0.0986395999789238\n",
      "epoch: 1 step: 1590, loss is 0.19599364697933197\n",
      "epoch: 1 step: 1591, loss is 0.05261439457535744\n",
      "epoch: 1 step: 1592, loss is 0.030599743127822876\n",
      "epoch: 1 step: 1593, loss is 0.008994262665510178\n",
      "epoch: 1 step: 1594, loss is 0.059275705367326736\n",
      "epoch: 1 step: 1595, loss is 0.07017906755208969\n",
      "epoch: 1 step: 1596, loss is 0.2937281131744385\n",
      "epoch: 1 step: 1597, loss is 0.13075347244739532\n",
      "epoch: 1 step: 1598, loss is 0.04974282532930374\n",
      "epoch: 1 step: 1599, loss is 0.12890493869781494\n",
      "epoch: 1 step: 1600, loss is 0.2177530974149704\n",
      "epoch: 1 step: 1601, loss is 0.0558059960603714\n",
      "epoch: 1 step: 1602, loss is 0.023743057623505592\n",
      "epoch: 1 step: 1603, loss is 0.004712946247309446\n",
      "epoch: 1 step: 1604, loss is 0.019669445231556892\n",
      "epoch: 1 step: 1605, loss is 0.10523495823144913\n",
      "epoch: 1 step: 1606, loss is 0.09399478137493134\n",
      "epoch: 1 step: 1607, loss is 0.013768980279564857\n",
      "epoch: 1 step: 1608, loss is 0.004617799539119005\n",
      "epoch: 1 step: 1609, loss is 0.22386668622493744\n",
      "epoch: 1 step: 1610, loss is 0.01024535484611988\n",
      "epoch: 1 step: 1611, loss is 0.3240247666835785\n",
      "epoch: 1 step: 1612, loss is 0.004551353398710489\n",
      "epoch: 1 step: 1613, loss is 0.017415020614862442\n",
      "epoch: 1 step: 1614, loss is 0.00396675756201148\n",
      "epoch: 1 step: 1615, loss is 0.03823959827423096\n",
      "epoch: 1 step: 1616, loss is 0.34125715494155884\n",
      "epoch: 1 step: 1617, loss is 0.03732655569911003\n",
      "epoch: 1 step: 1618, loss is 0.11173969507217407\n",
      "epoch: 1 step: 1619, loss is 0.008939798921346664\n",
      "epoch: 1 step: 1620, loss is 0.017774587497115135\n",
      "epoch: 1 step: 1621, loss is 0.08909009397029877\n",
      "epoch: 1 step: 1622, loss is 0.050280872732400894\n",
      "epoch: 1 step: 1623, loss is 0.0719548910856247\n",
      "epoch: 1 step: 1624, loss is 0.10114361345767975\n",
      "epoch: 1 step: 1625, loss is 0.05521832033991814\n",
      "epoch: 1 step: 1626, loss is 0.024141816422343254\n",
      "epoch: 1 step: 1627, loss is 0.020730149000883102\n",
      "epoch: 1 step: 1628, loss is 0.12533658742904663\n",
      "epoch: 1 step: 1629, loss is 0.004365659784525633\n",
      "epoch: 1 step: 1630, loss is 0.021770836785435677\n",
      "epoch: 1 step: 1631, loss is 0.11259693652391434\n",
      "epoch: 1 step: 1632, loss is 0.017834894359111786\n",
      "epoch: 1 step: 1633, loss is 0.078805111348629\n",
      "epoch: 1 step: 1634, loss is 0.0661039799451828\n",
      "epoch: 1 step: 1635, loss is 0.05064189061522484\n",
      "epoch: 1 step: 1636, loss is 0.07218092679977417\n",
      "epoch: 1 step: 1637, loss is 0.02213330566883087\n",
      "epoch: 1 step: 1638, loss is 0.04092603549361229\n",
      "epoch: 1 step: 1639, loss is 0.25526705384254456\n",
      "epoch: 1 step: 1640, loss is 0.025980589911341667\n",
      "epoch: 1 step: 1641, loss is 0.03690750524401665\n",
      "epoch: 1 step: 1642, loss is 0.14701786637306213\n",
      "epoch: 1 step: 1643, loss is 0.15529297292232513\n",
      "epoch: 1 step: 1644, loss is 0.05499092489480972\n",
      "epoch: 1 step: 1645, loss is 0.014944696798920631\n",
      "epoch: 1 step: 1646, loss is 0.11551788449287415\n",
      "epoch: 1 step: 1647, loss is 0.011636427603662014\n",
      "epoch: 1 step: 1648, loss is 0.09998849034309387\n",
      "epoch: 1 step: 1649, loss is 0.007161465473473072\n",
      "epoch: 1 step: 1650, loss is 0.03807968646287918\n",
      "epoch: 1 step: 1651, loss is 0.018802443519234657\n",
      "epoch: 1 step: 1652, loss is 0.02973904088139534\n",
      "epoch: 1 step: 1653, loss is 0.02076217718422413\n",
      "epoch: 1 step: 1654, loss is 0.009558609686791897\n",
      "epoch: 1 step: 1655, loss is 0.013248313218355179\n",
      "epoch: 1 step: 1656, loss is 0.13658270239830017\n",
      "epoch: 1 step: 1657, loss is 0.014826476573944092\n",
      "epoch: 1 step: 1658, loss is 0.10040058940649033\n",
      "epoch: 1 step: 1659, loss is 0.06740071624517441\n",
      "epoch: 1 step: 1660, loss is 0.05405106395483017\n",
      "epoch: 1 step: 1661, loss is 0.009145179763436317\n",
      "epoch: 1 step: 1662, loss is 0.1081300899386406\n",
      "epoch: 1 step: 1663, loss is 0.004880854859948158\n",
      "epoch: 1 step: 1664, loss is 0.013081005774438381\n",
      "epoch: 1 step: 1665, loss is 0.018723510205745697\n",
      "epoch: 1 step: 1666, loss is 0.015172507613897324\n",
      "epoch: 1 step: 1667, loss is 0.19322392344474792\n",
      "epoch: 1 step: 1668, loss is 0.03653703257441521\n",
      "epoch: 1 step: 1669, loss is 0.023786867037415504\n",
      "epoch: 1 step: 1670, loss is 0.07952775806188583\n",
      "epoch: 1 step: 1671, loss is 0.047629497945308685\n",
      "epoch: 1 step: 1672, loss is 0.04215225949883461\n",
      "epoch: 1 step: 1673, loss is 0.005296482238918543\n",
      "epoch: 1 step: 1674, loss is 0.06217764690518379\n",
      "epoch: 1 step: 1675, loss is 0.06511324644088745\n",
      "epoch: 1 step: 1676, loss is 0.0033261068165302277\n",
      "epoch: 1 step: 1677, loss is 0.01771126128733158\n",
      "epoch: 1 step: 1678, loss is 0.04234874248504639\n",
      "epoch: 1 step: 1679, loss is 0.021749084815382957\n",
      "epoch: 1 step: 1680, loss is 0.002990707755088806\n",
      "epoch: 1 step: 1681, loss is 0.013690413907170296\n",
      "epoch: 1 step: 1682, loss is 0.20416764914989471\n",
      "epoch: 1 step: 1683, loss is 0.014921496622264385\n",
      "epoch: 1 step: 1684, loss is 0.01027025654911995\n",
      "epoch: 1 step: 1685, loss is 0.055371467024087906\n",
      "epoch: 1 step: 1686, loss is 0.05649431422352791\n",
      "epoch: 1 step: 1687, loss is 0.056997619569301605\n",
      "epoch: 1 step: 1688, loss is 0.0029441521037369967\n",
      "epoch: 1 step: 1689, loss is 0.10548539459705353\n",
      "epoch: 1 step: 1690, loss is 0.0007562937680631876\n",
      "epoch: 1 step: 1691, loss is 0.18352250754833221\n",
      "epoch: 1 step: 1692, loss is 0.13044264912605286\n",
      "epoch: 1 step: 1693, loss is 0.022281799465417862\n",
      "epoch: 1 step: 1694, loss is 0.09114069491624832\n",
      "epoch: 1 step: 1695, loss is 0.019847698509693146\n",
      "epoch: 1 step: 1696, loss is 0.014335273765027523\n",
      "epoch: 1 step: 1697, loss is 0.23363688588142395\n",
      "epoch: 1 step: 1698, loss is 0.06971479207277298\n",
      "epoch: 1 step: 1699, loss is 0.004418688360601664\n",
      "epoch: 1 step: 1700, loss is 0.058864470571279526\n",
      "epoch: 1 step: 1701, loss is 0.0034556961618363857\n",
      "epoch: 1 step: 1702, loss is 0.002177201444283128\n",
      "epoch: 1 step: 1703, loss is 0.14951398968696594\n",
      "epoch: 1 step: 1704, loss is 0.09743853658437729\n",
      "epoch: 1 step: 1705, loss is 0.005301923491060734\n",
      "epoch: 1 step: 1706, loss is 0.019785001873970032\n",
      "epoch: 1 step: 1707, loss is 0.12633775174617767\n",
      "epoch: 1 step: 1708, loss is 0.0026841252110898495\n",
      "epoch: 1 step: 1709, loss is 0.06441774219274521\n",
      "epoch: 1 step: 1710, loss is 0.027239374816417694\n",
      "epoch: 1 step: 1711, loss is 0.21585682034492493\n",
      "epoch: 1 step: 1712, loss is 0.024871382862329483\n",
      "epoch: 1 step: 1713, loss is 0.12797273695468903\n",
      "epoch: 1 step: 1714, loss is 0.11484411358833313\n",
      "epoch: 1 step: 1715, loss is 0.1960209459066391\n",
      "epoch: 1 step: 1716, loss is 0.005583383608609438\n",
      "epoch: 1 step: 1717, loss is 0.2338865101337433\n",
      "epoch: 1 step: 1718, loss is 0.2017402946949005\n",
      "epoch: 1 step: 1719, loss is 0.10217852145433426\n",
      "epoch: 1 step: 1720, loss is 0.03419392555952072\n",
      "epoch: 1 step: 1721, loss is 0.008710836991667747\n",
      "epoch: 1 step: 1722, loss is 0.042960718274116516\n",
      "epoch: 1 step: 1723, loss is 0.006032941397279501\n",
      "epoch: 1 step: 1724, loss is 0.14001861214637756\n",
      "epoch: 1 step: 1725, loss is 0.012184621766209602\n",
      "epoch: 1 step: 1726, loss is 0.025664538145065308\n",
      "epoch: 1 step: 1727, loss is 0.16156817972660065\n",
      "epoch: 1 step: 1728, loss is 0.24128183722496033\n",
      "epoch: 1 step: 1729, loss is 0.07834716141223907\n",
      "epoch: 1 step: 1730, loss is 0.027528373524546623\n",
      "epoch: 1 step: 1731, loss is 0.14976854622364044\n",
      "epoch: 1 step: 1732, loss is 0.04927479103207588\n",
      "epoch: 1 step: 1733, loss is 0.009300722740590572\n",
      "epoch: 1 step: 1734, loss is 0.12192352861166\n",
      "epoch: 1 step: 1735, loss is 0.13993458449840546\n",
      "epoch: 1 step: 1736, loss is 0.20443935692310333\n",
      "epoch: 1 step: 1737, loss is 0.00539139611646533\n",
      "epoch: 1 step: 1738, loss is 0.3774811327457428\n",
      "epoch: 1 step: 1739, loss is 0.01497400552034378\n",
      "epoch: 1 step: 1740, loss is 0.027715150266885757\n",
      "epoch: 1 step: 1741, loss is 0.007467363961040974\n",
      "epoch: 1 step: 1742, loss is 0.038556668907403946\n",
      "epoch: 1 step: 1743, loss is 0.09788678586483002\n",
      "epoch: 1 step: 1744, loss is 0.12216240912675858\n",
      "epoch: 1 step: 1745, loss is 0.06777890771627426\n",
      "epoch: 1 step: 1746, loss is 0.12193606048822403\n",
      "epoch: 1 step: 1747, loss is 0.004464267287403345\n",
      "epoch: 1 step: 1748, loss is 0.26896870136260986\n",
      "epoch: 1 step: 1749, loss is 0.005815827287733555\n",
      "epoch: 1 step: 1750, loss is 0.03851318359375\n",
      "epoch: 1 step: 1751, loss is 0.2100442796945572\n",
      "epoch: 1 step: 1752, loss is 0.018194546923041344\n",
      "epoch: 1 step: 1753, loss is 0.4869711399078369\n",
      "epoch: 1 step: 1754, loss is 0.04351500794291496\n",
      "epoch: 1 step: 1755, loss is 0.18412147462368011\n",
      "epoch: 1 step: 1756, loss is 0.009066528640687466\n",
      "epoch: 1 step: 1757, loss is 0.07521868497133255\n",
      "epoch: 1 step: 1758, loss is 0.024811554700136185\n",
      "epoch: 1 step: 1759, loss is 0.014560244046151638\n",
      "epoch: 1 step: 1760, loss is 0.16557428240776062\n",
      "epoch: 1 step: 1761, loss is 0.07664436846971512\n",
      "epoch: 1 step: 1762, loss is 0.10292261838912964\n",
      "epoch: 1 step: 1763, loss is 0.2012389898300171\n",
      "epoch: 1 step: 1764, loss is 0.041241355240345\n",
      "epoch: 1 step: 1765, loss is 0.01029493473470211\n",
      "epoch: 1 step: 1766, loss is 0.01704874448478222\n",
      "epoch: 1 step: 1767, loss is 0.023795323446393013\n",
      "epoch: 1 step: 1768, loss is 0.08101806044578552\n",
      "epoch: 1 step: 1769, loss is 0.12456905841827393\n",
      "epoch: 1 step: 1770, loss is 0.15740612149238586\n",
      "epoch: 1 step: 1771, loss is 0.002411898225545883\n",
      "epoch: 1 step: 1772, loss is 0.0315270870923996\n",
      "epoch: 1 step: 1773, loss is 0.005332266911864281\n",
      "epoch: 1 step: 1774, loss is 0.15263698995113373\n",
      "epoch: 1 step: 1775, loss is 0.07334128022193909\n",
      "epoch: 1 step: 1776, loss is 0.04112178459763527\n",
      "epoch: 1 step: 1777, loss is 0.14982767403125763\n",
      "epoch: 1 step: 1778, loss is 0.02143080346286297\n",
      "epoch: 1 step: 1779, loss is 0.08894618600606918\n",
      "epoch: 1 step: 1780, loss is 0.12097292393445969\n",
      "epoch: 1 step: 1781, loss is 0.009887339547276497\n",
      "epoch: 1 step: 1782, loss is 0.011143505573272705\n",
      "epoch: 1 step: 1783, loss is 0.02990144118666649\n",
      "epoch: 1 step: 1784, loss is 0.04260561615228653\n",
      "epoch: 1 step: 1785, loss is 0.26384326815605164\n",
      "epoch: 1 step: 1786, loss is 0.01892794668674469\n",
      "epoch: 1 step: 1787, loss is 0.16345760226249695\n",
      "epoch: 1 step: 1788, loss is 0.04754066467285156\n",
      "epoch: 1 step: 1789, loss is 0.052540410310029984\n",
      "epoch: 1 step: 1790, loss is 0.20990422368049622\n",
      "epoch: 1 step: 1791, loss is 0.02551170252263546\n",
      "epoch: 1 step: 1792, loss is 0.07203655689954758\n",
      "epoch: 1 step: 1793, loss is 0.005519696976989508\n",
      "epoch: 1 step: 1794, loss is 0.012955042533576488\n",
      "epoch: 1 step: 1795, loss is 0.03268817439675331\n",
      "epoch: 1 step: 1796, loss is 0.037462033331394196\n",
      "epoch: 1 step: 1797, loss is 0.0015701742377132177\n",
      "epoch: 1 step: 1798, loss is 0.019985171034932137\n",
      "epoch: 1 step: 1799, loss is 0.02669382654130459\n",
      "epoch: 1 step: 1800, loss is 0.003786609973758459\n",
      "epoch: 1 step: 1801, loss is 0.0907898098230362\n",
      "epoch: 1 step: 1802, loss is 0.0016933426959440112\n",
      "epoch: 1 step: 1803, loss is 0.048029277473688126\n",
      "epoch: 1 step: 1804, loss is 0.06324639171361923\n",
      "epoch: 1 step: 1805, loss is 0.008666865527629852\n",
      "epoch: 1 step: 1806, loss is 0.39231741428375244\n",
      "epoch: 1 step: 1807, loss is 0.02461293525993824\n",
      "epoch: 1 step: 1808, loss is 0.20264321565628052\n",
      "epoch: 1 step: 1809, loss is 0.018769120797514915\n",
      "epoch: 1 step: 1810, loss is 0.09997748583555222\n",
      "epoch: 1 step: 1811, loss is 0.00313243898563087\n",
      "epoch: 1 step: 1812, loss is 0.16644354164600372\n",
      "epoch: 1 step: 1813, loss is 0.10550031810998917\n",
      "epoch: 1 step: 1814, loss is 0.06944794952869415\n",
      "epoch: 1 step: 1815, loss is 0.02298644185066223\n",
      "epoch: 1 step: 1816, loss is 0.06656001508235931\n",
      "epoch: 1 step: 1817, loss is 0.09518454968929291\n",
      "epoch: 1 step: 1818, loss is 0.009492346085608006\n",
      "epoch: 1 step: 1819, loss is 0.07736804336309433\n",
      "epoch: 1 step: 1820, loss is 0.012960555031895638\n",
      "epoch: 1 step: 1821, loss is 0.04421093687415123\n",
      "epoch: 1 step: 1822, loss is 0.06267981231212616\n",
      "epoch: 1 step: 1823, loss is 0.05367725342512131\n",
      "epoch: 1 step: 1824, loss is 0.043066758662462234\n",
      "epoch: 1 step: 1825, loss is 0.1669432520866394\n",
      "epoch: 1 step: 1826, loss is 0.13456688821315765\n",
      "epoch: 1 step: 1827, loss is 0.017517488449811935\n",
      "epoch: 1 step: 1828, loss is 0.1175953596830368\n",
      "epoch: 1 step: 1829, loss is 0.07596335560083389\n",
      "epoch: 1 step: 1830, loss is 0.006089644506573677\n",
      "epoch: 1 step: 1831, loss is 0.14038588106632233\n",
      "epoch: 1 step: 1832, loss is 0.03221995010972023\n",
      "epoch: 1 step: 1833, loss is 0.01396031491458416\n",
      "epoch: 1 step: 1834, loss is 0.02080046944320202\n",
      "epoch: 1 step: 1835, loss is 0.002532561542466283\n",
      "epoch: 1 step: 1836, loss is 0.052803393453359604\n",
      "epoch: 1 step: 1837, loss is 0.009152894839644432\n",
      "epoch: 1 step: 1838, loss is 0.12481246888637543\n",
      "epoch: 1 step: 1839, loss is 0.011655670590698719\n",
      "epoch: 1 step: 1840, loss is 0.6526724100112915\n",
      "epoch: 1 step: 1841, loss is 0.036798011511564255\n",
      "epoch: 1 step: 1842, loss is 0.07883521169424057\n",
      "epoch: 1 step: 1843, loss is 0.01152760349214077\n",
      "epoch: 1 step: 1844, loss is 0.0425197072327137\n",
      "epoch: 1 step: 1845, loss is 0.01885591819882393\n",
      "epoch: 1 step: 1846, loss is 0.19521503150463104\n",
      "epoch: 1 step: 1847, loss is 0.004348915535956621\n",
      "epoch: 1 step: 1848, loss is 0.061068687587976456\n",
      "epoch: 1 step: 1849, loss is 0.00833131279796362\n",
      "epoch: 1 step: 1850, loss is 0.17497901618480682\n",
      "epoch: 1 step: 1851, loss is 0.0321589820086956\n",
      "epoch: 1 step: 1852, loss is 0.029949788004159927\n",
      "epoch: 1 step: 1853, loss is 0.03551534563302994\n",
      "epoch: 1 step: 1854, loss is 0.06326372176408768\n",
      "epoch: 1 step: 1855, loss is 0.1859796941280365\n",
      "epoch: 1 step: 1856, loss is 0.003289520973339677\n",
      "epoch: 1 step: 1857, loss is 0.03693873807787895\n",
      "epoch: 1 step: 1858, loss is 0.05306524038314819\n",
      "epoch: 1 step: 1859, loss is 0.030746201053261757\n",
      "epoch: 1 step: 1860, loss is 0.012339195236563683\n",
      "epoch: 1 step: 1861, loss is 0.08620665222406387\n",
      "epoch: 1 step: 1862, loss is 0.009646860882639885\n",
      "epoch: 1 step: 1863, loss is 0.006382615305483341\n",
      "epoch: 1 step: 1864, loss is 0.022768745198845863\n",
      "epoch: 1 step: 1865, loss is 0.013484722934663296\n",
      "epoch: 1 step: 1866, loss is 0.006786624435335398\n",
      "epoch: 1 step: 1867, loss is 0.18628571927547455\n",
      "epoch: 1 step: 1868, loss is 0.033903300762176514\n",
      "epoch: 1 step: 1869, loss is 0.0606095977127552\n",
      "epoch: 1 step: 1870, loss is 0.01316596195101738\n",
      "epoch: 1 step: 1871, loss is 0.04795052483677864\n",
      "epoch: 1 step: 1872, loss is 0.11931038647890091\n",
      "epoch: 1 step: 1873, loss is 0.0163910910487175\n",
      "epoch: 1 step: 1874, loss is 0.14874006807804108\n",
      "epoch: 1 step: 1875, loss is 0.0141636086627841\n",
      "epoch: 2 step: 1, loss is 0.0904792994260788\n",
      "epoch: 2 step: 2, loss is 0.06836201995611191\n",
      "epoch: 2 step: 3, loss is 0.010116071440279484\n",
      "epoch: 2 step: 4, loss is 0.09566348046064377\n",
      "epoch: 2 step: 5, loss is 0.010765003971755505\n",
      "epoch: 2 step: 6, loss is 0.016658734530210495\n",
      "epoch: 2 step: 7, loss is 0.03049612231552601\n",
      "epoch: 2 step: 8, loss is 0.1123708188533783\n",
      "epoch: 2 step: 9, loss is 0.0038134988863021135\n",
      "epoch: 2 step: 10, loss is 0.011685253120958805\n",
      "epoch: 2 step: 11, loss is 0.033746082335710526\n",
      "epoch: 2 step: 12, loss is 0.02140556462109089\n",
      "epoch: 2 step: 13, loss is 0.158142551779747\n",
      "epoch: 2 step: 14, loss is 0.021739989519119263\n",
      "epoch: 2 step: 15, loss is 0.014772449620068073\n",
      "epoch: 2 step: 16, loss is 0.2599872052669525\n",
      "epoch: 2 step: 17, loss is 0.01376310084015131\n",
      "epoch: 2 step: 18, loss is 0.000697695417329669\n",
      "epoch: 2 step: 19, loss is 0.03501776233315468\n",
      "epoch: 2 step: 20, loss is 0.0017208068165928125\n",
      "epoch: 2 step: 21, loss is 0.07117125391960144\n",
      "epoch: 2 step: 22, loss is 0.03429902717471123\n",
      "epoch: 2 step: 23, loss is 0.45229294896125793\n",
      "epoch: 2 step: 24, loss is 0.04366388916969299\n",
      "epoch: 2 step: 25, loss is 0.16405759751796722\n",
      "epoch: 2 step: 26, loss is 0.0020282671321183443\n",
      "epoch: 2 step: 27, loss is 0.02802114561200142\n",
      "epoch: 2 step: 28, loss is 0.12454646825790405\n",
      "epoch: 2 step: 29, loss is 0.015380955301225185\n",
      "epoch: 2 step: 30, loss is 0.061567552387714386\n",
      "epoch: 2 step: 31, loss is 0.0940113365650177\n",
      "epoch: 2 step: 32, loss is 0.008511797524988651\n",
      "epoch: 2 step: 33, loss is 0.019065795466303825\n",
      "epoch: 2 step: 34, loss is 0.03547466918826103\n",
      "epoch: 2 step: 35, loss is 0.09226618707180023\n",
      "epoch: 2 step: 36, loss is 0.05392392724752426\n",
      "epoch: 2 step: 37, loss is 0.002820401219651103\n",
      "epoch: 2 step: 38, loss is 0.014126298017799854\n",
      "epoch: 2 step: 39, loss is 0.036419522017240524\n",
      "epoch: 2 step: 40, loss is 0.059294745326042175\n",
      "epoch: 2 step: 41, loss is 0.01056753471493721\n",
      "epoch: 2 step: 42, loss is 0.023965008556842804\n",
      "epoch: 2 step: 43, loss is 0.015676045790314674\n",
      "epoch: 2 step: 44, loss is 0.004315777216106653\n",
      "epoch: 2 step: 45, loss is 0.05644723400473595\n",
      "epoch: 2 step: 46, loss is 0.016989298164844513\n",
      "epoch: 2 step: 47, loss is 0.008068688213825226\n",
      "epoch: 2 step: 48, loss is 0.004340871702879667\n",
      "epoch: 2 step: 49, loss is 0.0014225001214072108\n",
      "epoch: 2 step: 50, loss is 0.034342389553785324\n",
      "epoch: 2 step: 51, loss is 0.02573103830218315\n",
      "epoch: 2 step: 52, loss is 0.09881563484668732\n",
      "epoch: 2 step: 53, loss is 0.06342767179012299\n",
      "epoch: 2 step: 54, loss is 0.025574058294296265\n",
      "epoch: 2 step: 55, loss is 0.0016930358251556754\n",
      "epoch: 2 step: 56, loss is 0.05630441755056381\n",
      "epoch: 2 step: 57, loss is 0.024914871901273727\n",
      "epoch: 2 step: 58, loss is 0.16736657917499542\n",
      "epoch: 2 step: 59, loss is 0.0029542569536715746\n",
      "epoch: 2 step: 60, loss is 0.02652067132294178\n",
      "epoch: 2 step: 61, loss is 0.0033446860034018755\n",
      "epoch: 2 step: 62, loss is 0.005326485726982355\n",
      "epoch: 2 step: 63, loss is 0.18916088342666626\n",
      "epoch: 2 step: 64, loss is 0.0010456994641572237\n",
      "epoch: 2 step: 65, loss is 0.08938612043857574\n",
      "epoch: 2 step: 66, loss is 0.03469047322869301\n",
      "epoch: 2 step: 67, loss is 0.1914546936750412\n",
      "epoch: 2 step: 68, loss is 0.3397495448589325\n",
      "epoch: 2 step: 69, loss is 0.030627772212028503\n",
      "epoch: 2 step: 70, loss is 0.0038862803485244513\n",
      "epoch: 2 step: 71, loss is 0.02327537164092064\n",
      "epoch: 2 step: 72, loss is 0.04584876447916031\n",
      "epoch: 2 step: 73, loss is 0.028918197378516197\n",
      "epoch: 2 step: 74, loss is 0.383165568113327\n",
      "epoch: 2 step: 75, loss is 0.10748852044343948\n",
      "epoch: 2 step: 76, loss is 0.056206971406936646\n",
      "epoch: 2 step: 77, loss is 0.04956596717238426\n",
      "epoch: 2 step: 78, loss is 0.054199375212192535\n",
      "epoch: 2 step: 79, loss is 0.007804105523973703\n",
      "epoch: 2 step: 80, loss is 0.09269965440034866\n",
      "epoch: 2 step: 81, loss is 0.05890800431370735\n",
      "epoch: 2 step: 82, loss is 0.004105542786419392\n",
      "epoch: 2 step: 83, loss is 0.08716633170843124\n",
      "epoch: 2 step: 84, loss is 0.07713743299245834\n",
      "epoch: 2 step: 85, loss is 0.16449235379695892\n",
      "epoch: 2 step: 86, loss is 0.15260164439678192\n",
      "epoch: 2 step: 87, loss is 0.006375223398208618\n",
      "epoch: 2 step: 88, loss is 0.09620384126901627\n",
      "epoch: 2 step: 89, loss is 0.016108417883515358\n",
      "epoch: 2 step: 90, loss is 0.002315906109288335\n",
      "epoch: 2 step: 91, loss is 0.39955881237983704\n",
      "epoch: 2 step: 92, loss is 0.0054063620045781136\n",
      "epoch: 2 step: 93, loss is 0.010893385857343674\n",
      "epoch: 2 step: 94, loss is 0.006792726460844278\n",
      "epoch: 2 step: 95, loss is 0.014589506201446056\n",
      "epoch: 2 step: 96, loss is 0.06325395405292511\n",
      "epoch: 2 step: 97, loss is 0.017620939761400223\n",
      "epoch: 2 step: 98, loss is 0.02698121964931488\n",
      "epoch: 2 step: 99, loss is 0.012801669538021088\n",
      "epoch: 2 step: 100, loss is 0.024974709376692772\n",
      "epoch: 2 step: 101, loss is 0.1015840619802475\n",
      "epoch: 2 step: 102, loss is 0.0023221331648528576\n",
      "epoch: 2 step: 103, loss is 0.11369580775499344\n",
      "epoch: 2 step: 104, loss is 0.16181036829948425\n",
      "epoch: 2 step: 105, loss is 0.02825864776968956\n",
      "epoch: 2 step: 106, loss is 0.02131989412009716\n",
      "epoch: 2 step: 107, loss is 0.0892827957868576\n",
      "epoch: 2 step: 108, loss is 0.012984734028577805\n",
      "epoch: 2 step: 109, loss is 0.13280518352985382\n",
      "epoch: 2 step: 110, loss is 0.011079770512878895\n",
      "epoch: 2 step: 111, loss is 0.10255731642246246\n",
      "epoch: 2 step: 112, loss is 0.09513205289840698\n",
      "epoch: 2 step: 113, loss is 0.008078468032181263\n",
      "epoch: 2 step: 114, loss is 0.08897203207015991\n",
      "epoch: 2 step: 115, loss is 0.12718652188777924\n",
      "epoch: 2 step: 116, loss is 0.05586747080087662\n",
      "epoch: 2 step: 117, loss is 0.07840777188539505\n",
      "epoch: 2 step: 118, loss is 0.18458139896392822\n",
      "epoch: 2 step: 119, loss is 0.32070451974868774\n",
      "epoch: 2 step: 120, loss is 0.024460572749376297\n",
      "epoch: 2 step: 121, loss is 0.17542558908462524\n",
      "epoch: 2 step: 122, loss is 0.11730614304542542\n",
      "epoch: 2 step: 123, loss is 0.15012824535369873\n",
      "epoch: 2 step: 124, loss is 0.009514408186078072\n",
      "epoch: 2 step: 125, loss is 0.15270677208900452\n",
      "epoch: 2 step: 126, loss is 0.009679696522653103\n",
      "epoch: 2 step: 127, loss is 0.06409259885549545\n",
      "epoch: 2 step: 128, loss is 0.004766827914863825\n",
      "epoch: 2 step: 129, loss is 0.013875509612262249\n",
      "epoch: 2 step: 130, loss is 0.004831543657928705\n",
      "epoch: 2 step: 131, loss is 0.16055266559123993\n",
      "epoch: 2 step: 132, loss is 0.0481751449406147\n",
      "epoch: 2 step: 133, loss is 0.06323140114545822\n",
      "epoch: 2 step: 134, loss is 0.10838553309440613\n",
      "epoch: 2 step: 135, loss is 0.045184094458818436\n",
      "epoch: 2 step: 136, loss is 0.015802251175045967\n",
      "epoch: 2 step: 137, loss is 0.019979650154709816\n",
      "epoch: 2 step: 138, loss is 0.2966347932815552\n",
      "epoch: 2 step: 139, loss is 0.007885766215622425\n",
      "epoch: 2 step: 140, loss is 0.09444982558488846\n",
      "epoch: 2 step: 141, loss is 0.05322742089629173\n",
      "epoch: 2 step: 142, loss is 0.23103581368923187\n",
      "epoch: 2 step: 143, loss is 0.015574456192553043\n",
      "epoch: 2 step: 144, loss is 0.024059206247329712\n",
      "epoch: 2 step: 145, loss is 0.004973351024091244\n",
      "epoch: 2 step: 146, loss is 0.025685161352157593\n",
      "epoch: 2 step: 147, loss is 0.004675362724810839\n",
      "epoch: 2 step: 148, loss is 0.08043073117733002\n",
      "epoch: 2 step: 149, loss is 0.28604385256767273\n",
      "epoch: 2 step: 150, loss is 0.07241440564393997\n",
      "epoch: 2 step: 151, loss is 0.01589307375252247\n",
      "epoch: 2 step: 152, loss is 0.007165583316236734\n",
      "epoch: 2 step: 153, loss is 0.06876424700021744\n",
      "epoch: 2 step: 154, loss is 0.12034707516431808\n",
      "epoch: 2 step: 155, loss is 0.016511570662260056\n",
      "epoch: 2 step: 156, loss is 0.05936238169670105\n",
      "epoch: 2 step: 157, loss is 0.04969768971204758\n",
      "epoch: 2 step: 158, loss is 0.05503826588392258\n",
      "epoch: 2 step: 159, loss is 0.05078132450580597\n",
      "epoch: 2 step: 160, loss is 0.10336866229772568\n",
      "epoch: 2 step: 161, loss is 0.29790523648262024\n",
      "epoch: 2 step: 162, loss is 0.0040221926756203175\n",
      "epoch: 2 step: 163, loss is 0.019433025270700455\n",
      "epoch: 2 step: 164, loss is 0.014064713381230831\n",
      "epoch: 2 step: 165, loss is 0.07424865663051605\n",
      "epoch: 2 step: 166, loss is 0.011978963389992714\n",
      "epoch: 2 step: 167, loss is 0.00925915315747261\n",
      "epoch: 2 step: 168, loss is 0.03290779888629913\n",
      "epoch: 2 step: 169, loss is 0.0024026683531701565\n",
      "epoch: 2 step: 170, loss is 0.021824674680829048\n",
      "epoch: 2 step: 171, loss is 0.1373077780008316\n",
      "epoch: 2 step: 172, loss is 0.08015529811382294\n",
      "epoch: 2 step: 173, loss is 0.02409890480339527\n",
      "epoch: 2 step: 174, loss is 0.04751412570476532\n",
      "epoch: 2 step: 175, loss is 0.002060504164546728\n",
      "epoch: 2 step: 176, loss is 0.3002164959907532\n",
      "epoch: 2 step: 177, loss is 0.1765296310186386\n",
      "epoch: 2 step: 178, loss is 0.14882579445838928\n",
      "epoch: 2 step: 179, loss is 0.2596418857574463\n",
      "epoch: 2 step: 180, loss is 0.05923978611826897\n",
      "epoch: 2 step: 181, loss is 0.0018016016110777855\n",
      "epoch: 2 step: 182, loss is 0.06544081121683121\n",
      "epoch: 2 step: 183, loss is 0.07726865261793137\n",
      "epoch: 2 step: 184, loss is 0.006559303496032953\n",
      "epoch: 2 step: 185, loss is 0.011915730312466621\n",
      "epoch: 2 step: 186, loss is 0.003196515841409564\n",
      "epoch: 2 step: 187, loss is 0.04360532760620117\n",
      "epoch: 2 step: 188, loss is 0.02331988327205181\n",
      "epoch: 2 step: 189, loss is 0.009017552249133587\n",
      "epoch: 2 step: 190, loss is 0.19487595558166504\n",
      "epoch: 2 step: 191, loss is 0.127559095621109\n",
      "epoch: 2 step: 192, loss is 0.07363998144865036\n",
      "epoch: 2 step: 193, loss is 0.011054267175495625\n",
      "epoch: 2 step: 194, loss is 0.08360178023576736\n",
      "epoch: 2 step: 195, loss is 0.3086822032928467\n",
      "epoch: 2 step: 196, loss is 0.030195508152246475\n",
      "epoch: 2 step: 197, loss is 0.016153281554579735\n",
      "epoch: 2 step: 198, loss is 0.033359408378601074\n",
      "epoch: 2 step: 199, loss is 0.002705621998757124\n",
      "epoch: 2 step: 200, loss is 0.06761641800403595\n",
      "epoch: 2 step: 201, loss is 0.003908153623342514\n",
      "epoch: 2 step: 202, loss is 0.06914252042770386\n",
      "epoch: 2 step: 203, loss is 0.011938642710447311\n",
      "epoch: 2 step: 204, loss is 0.10936617106199265\n",
      "epoch: 2 step: 205, loss is 0.04430244117975235\n",
      "epoch: 2 step: 206, loss is 0.0170182716101408\n",
      "epoch: 2 step: 207, loss is 0.05023209750652313\n",
      "epoch: 2 step: 208, loss is 0.01281832903623581\n",
      "epoch: 2 step: 209, loss is 0.017478032037615776\n",
      "epoch: 2 step: 210, loss is 0.040387317538261414\n",
      "epoch: 2 step: 211, loss is 0.002577355829998851\n",
      "epoch: 2 step: 212, loss is 0.11721530556678772\n",
      "epoch: 2 step: 213, loss is 0.06973619759082794\n",
      "epoch: 2 step: 214, loss is 0.0827644020318985\n",
      "epoch: 2 step: 215, loss is 0.07883468270301819\n",
      "epoch: 2 step: 216, loss is 0.04816839471459389\n",
      "epoch: 2 step: 217, loss is 0.009877286851406097\n",
      "epoch: 2 step: 218, loss is 0.1385064721107483\n",
      "epoch: 2 step: 219, loss is 0.05103223770856857\n",
      "epoch: 2 step: 220, loss is 0.013216789811849594\n",
      "epoch: 2 step: 221, loss is 0.012479045428335667\n",
      "epoch: 2 step: 222, loss is 0.08898071944713593\n",
      "epoch: 2 step: 223, loss is 0.08501101285219193\n",
      "epoch: 2 step: 224, loss is 0.010119457729160786\n",
      "epoch: 2 step: 225, loss is 0.07554232329130173\n",
      "epoch: 2 step: 226, loss is 0.07757019996643066\n",
      "epoch: 2 step: 227, loss is 0.007799712009727955\n",
      "epoch: 2 step: 228, loss is 0.08187493681907654\n",
      "epoch: 2 step: 229, loss is 0.005046843085438013\n",
      "epoch: 2 step: 230, loss is 0.013408315367996693\n",
      "epoch: 2 step: 231, loss is 0.018177296966314316\n",
      "epoch: 2 step: 232, loss is 0.03230850398540497\n",
      "epoch: 2 step: 233, loss is 0.0033693306613713503\n",
      "epoch: 2 step: 234, loss is 0.0026974212378263474\n",
      "epoch: 2 step: 235, loss is 0.009198488667607307\n",
      "epoch: 2 step: 236, loss is 0.00747778220102191\n",
      "epoch: 2 step: 237, loss is 0.011030704714357853\n",
      "epoch: 2 step: 238, loss is 0.026364939287304878\n",
      "epoch: 2 step: 239, loss is 0.017542755231261253\n",
      "epoch: 2 step: 240, loss is 0.012229189276695251\n",
      "epoch: 2 step: 241, loss is 0.05360044538974762\n",
      "epoch: 2 step: 242, loss is 0.019577141851186752\n",
      "epoch: 2 step: 243, loss is 0.1547153741121292\n",
      "epoch: 2 step: 244, loss is 0.024106379598379135\n",
      "epoch: 2 step: 245, loss is 0.001080502406693995\n",
      "epoch: 2 step: 246, loss is 0.019260162487626076\n",
      "epoch: 2 step: 247, loss is 0.11048899590969086\n",
      "epoch: 2 step: 248, loss is 0.04680844023823738\n",
      "epoch: 2 step: 249, loss is 0.09164663404226303\n",
      "epoch: 2 step: 250, loss is 0.0057326070964336395\n",
      "epoch: 2 step: 251, loss is 0.040549494326114655\n",
      "epoch: 2 step: 252, loss is 0.23579438030719757\n",
      "epoch: 2 step: 253, loss is 0.001088633551262319\n",
      "epoch: 2 step: 254, loss is 0.1434279978275299\n",
      "epoch: 2 step: 255, loss is 0.18778502941131592\n",
      "epoch: 2 step: 256, loss is 0.00706349266692996\n",
      "epoch: 2 step: 257, loss is 0.009277078323066235\n",
      "epoch: 2 step: 258, loss is 0.002921934239566326\n",
      "epoch: 2 step: 259, loss is 0.0028226773720234632\n",
      "epoch: 2 step: 260, loss is 0.10961387306451797\n",
      "epoch: 2 step: 261, loss is 0.051885269582271576\n",
      "epoch: 2 step: 262, loss is 0.14886054396629333\n",
      "epoch: 2 step: 263, loss is 0.00895160436630249\n",
      "epoch: 2 step: 264, loss is 0.05832583084702492\n",
      "epoch: 2 step: 265, loss is 0.11612232029438019\n",
      "epoch: 2 step: 266, loss is 0.008896607905626297\n",
      "epoch: 2 step: 267, loss is 0.17535808682441711\n",
      "epoch: 2 step: 268, loss is 0.010345651768147945\n",
      "epoch: 2 step: 269, loss is 0.010463782586157322\n",
      "epoch: 2 step: 270, loss is 0.03357238695025444\n",
      "epoch: 2 step: 271, loss is 0.08460074663162231\n",
      "epoch: 2 step: 272, loss is 0.10627870261669159\n",
      "epoch: 2 step: 273, loss is 0.03347738832235336\n",
      "epoch: 2 step: 274, loss is 0.07205580919981003\n",
      "epoch: 2 step: 275, loss is 0.12942790985107422\n",
      "epoch: 2 step: 276, loss is 0.08690720796585083\n",
      "epoch: 2 step: 277, loss is 0.009687150828540325\n",
      "epoch: 2 step: 278, loss is 0.0020799085032194853\n",
      "epoch: 2 step: 279, loss is 0.016185831278562546\n",
      "epoch: 2 step: 280, loss is 0.015047567896544933\n",
      "epoch: 2 step: 281, loss is 0.06992609798908234\n",
      "epoch: 2 step: 282, loss is 0.00034761487040668726\n",
      "epoch: 2 step: 283, loss is 0.12287640571594238\n",
      "epoch: 2 step: 284, loss is 0.017504876479506493\n",
      "epoch: 2 step: 285, loss is 0.012694341130554676\n",
      "epoch: 2 step: 286, loss is 0.0015381050761789083\n",
      "epoch: 2 step: 287, loss is 0.06167088449001312\n",
      "epoch: 2 step: 288, loss is 0.009470142424106598\n",
      "epoch: 2 step: 289, loss is 0.08952033519744873\n",
      "epoch: 2 step: 290, loss is 0.026204116642475128\n",
      "epoch: 2 step: 291, loss is 0.00437861867249012\n",
      "epoch: 2 step: 292, loss is 0.06845135241746902\n",
      "epoch: 2 step: 293, loss is 0.116800457239151\n",
      "epoch: 2 step: 294, loss is 0.11500327289104462\n",
      "epoch: 2 step: 295, loss is 0.01067732647061348\n",
      "epoch: 2 step: 296, loss is 0.0178250502794981\n",
      "epoch: 2 step: 297, loss is 0.26555147767066956\n",
      "epoch: 2 step: 298, loss is 0.011582673527300358\n",
      "epoch: 2 step: 299, loss is 0.03715403750538826\n",
      "epoch: 2 step: 300, loss is 0.004650422837585211\n",
      "epoch: 2 step: 301, loss is 0.02519337087869644\n",
      "epoch: 2 step: 302, loss is 0.006386229302734137\n",
      "epoch: 2 step: 303, loss is 0.15295812487602234\n",
      "epoch: 2 step: 304, loss is 0.028926266357302666\n",
      "epoch: 2 step: 305, loss is 0.044634073972702026\n",
      "epoch: 2 step: 306, loss is 0.0009765332215465605\n",
      "epoch: 2 step: 307, loss is 0.02555767074227333\n",
      "epoch: 2 step: 308, loss is 0.04630006104707718\n",
      "epoch: 2 step: 309, loss is 0.0013781996676698327\n",
      "epoch: 2 step: 310, loss is 0.0014388718409463763\n",
      "epoch: 2 step: 311, loss is 0.24088576436042786\n",
      "epoch: 2 step: 312, loss is 0.06965409219264984\n",
      "epoch: 2 step: 313, loss is 0.007271130569279194\n",
      "epoch: 2 step: 314, loss is 0.0073625934310257435\n",
      "epoch: 2 step: 315, loss is 0.11216270923614502\n",
      "epoch: 2 step: 316, loss is 0.002249429700896144\n",
      "epoch: 2 step: 317, loss is 0.03613589331507683\n",
      "epoch: 2 step: 318, loss is 0.0072489166632294655\n",
      "epoch: 2 step: 319, loss is 0.024846557527780533\n",
      "epoch: 2 step: 320, loss is 0.10134007036685944\n",
      "epoch: 2 step: 321, loss is 0.22306443750858307\n",
      "epoch: 2 step: 322, loss is 0.0014758219476789236\n",
      "epoch: 2 step: 323, loss is 0.18927507102489471\n",
      "epoch: 2 step: 324, loss is 0.13644355535507202\n",
      "epoch: 2 step: 325, loss is 0.2284785807132721\n",
      "epoch: 2 step: 326, loss is 0.15895740687847137\n",
      "epoch: 2 step: 327, loss is 0.08308230340480804\n",
      "epoch: 2 step: 328, loss is 0.00317360064946115\n",
      "epoch: 2 step: 329, loss is 0.045274753123521805\n",
      "epoch: 2 step: 330, loss is 0.025184139609336853\n",
      "epoch: 2 step: 331, loss is 0.05072414502501488\n",
      "epoch: 2 step: 332, loss is 0.07797282934188843\n",
      "epoch: 2 step: 333, loss is 0.15950334072113037\n",
      "epoch: 2 step: 334, loss is 0.002698412863537669\n",
      "epoch: 2 step: 335, loss is 0.01564720831811428\n",
      "epoch: 2 step: 336, loss is 0.015954528003931046\n",
      "epoch: 2 step: 337, loss is 0.1819005012512207\n",
      "epoch: 2 step: 338, loss is 0.03718149662017822\n",
      "epoch: 2 step: 339, loss is 0.13970190286636353\n",
      "epoch: 2 step: 340, loss is 0.06503833830356598\n",
      "epoch: 2 step: 341, loss is 0.013454781845211983\n",
      "epoch: 2 step: 342, loss is 0.11571702361106873\n",
      "epoch: 2 step: 343, loss is 0.05972955375909805\n",
      "epoch: 2 step: 344, loss is 0.18238714337348938\n",
      "epoch: 2 step: 345, loss is 0.09834074974060059\n",
      "epoch: 2 step: 346, loss is 0.03139624744653702\n",
      "epoch: 2 step: 347, loss is 0.023122025653719902\n",
      "epoch: 2 step: 348, loss is 0.02208702825009823\n",
      "epoch: 2 step: 349, loss is 0.16798308491706848\n",
      "epoch: 2 step: 350, loss is 0.013947652652859688\n",
      "epoch: 2 step: 351, loss is 0.012363308109343052\n",
      "epoch: 2 step: 352, loss is 0.08257535845041275\n",
      "epoch: 2 step: 353, loss is 0.22461485862731934\n",
      "epoch: 2 step: 354, loss is 0.003452262142673135\n",
      "epoch: 2 step: 355, loss is 0.05290570855140686\n",
      "epoch: 2 step: 356, loss is 0.0035531881731003523\n",
      "epoch: 2 step: 357, loss is 0.03894586116075516\n",
      "epoch: 2 step: 358, loss is 0.06260456889867783\n",
      "epoch: 2 step: 359, loss is 0.032150544226169586\n",
      "epoch: 2 step: 360, loss is 0.060550034046173096\n",
      "epoch: 2 step: 361, loss is 0.10247813165187836\n",
      "epoch: 2 step: 362, loss is 0.0077020106837153435\n",
      "epoch: 2 step: 363, loss is 0.0516679622232914\n",
      "epoch: 2 step: 364, loss is 0.013219976797699928\n",
      "epoch: 2 step: 365, loss is 0.022938348352909088\n",
      "epoch: 2 step: 366, loss is 0.07752158492803574\n",
      "epoch: 2 step: 367, loss is 0.002009723801165819\n",
      "epoch: 2 step: 368, loss is 0.010540556162595749\n",
      "epoch: 2 step: 369, loss is 0.11714556068181992\n",
      "epoch: 2 step: 370, loss is 0.005214603617787361\n",
      "epoch: 2 step: 371, loss is 0.11137466877698898\n",
      "epoch: 2 step: 372, loss is 0.025653740391135216\n",
      "epoch: 2 step: 373, loss is 0.1216479241847992\n",
      "epoch: 2 step: 374, loss is 0.2601611316204071\n",
      "epoch: 2 step: 375, loss is 0.004514344967901707\n",
      "epoch: 2 step: 376, loss is 0.09867330640554428\n",
      "epoch: 2 step: 377, loss is 0.057947225868701935\n",
      "epoch: 2 step: 378, loss is 0.013288339599967003\n",
      "epoch: 2 step: 379, loss is 0.09929102659225464\n",
      "epoch: 2 step: 380, loss is 0.0023311064578592777\n",
      "epoch: 2 step: 381, loss is 0.03515135124325752\n",
      "epoch: 2 step: 382, loss is 0.12645797431468964\n",
      "epoch: 2 step: 383, loss is 0.030637307092547417\n",
      "epoch: 2 step: 384, loss is 0.024682356044650078\n",
      "epoch: 2 step: 385, loss is 0.1925283819437027\n",
      "epoch: 2 step: 386, loss is 0.004985422361642122\n",
      "epoch: 2 step: 387, loss is 0.0859120786190033\n",
      "epoch: 2 step: 388, loss is 0.04369862377643585\n",
      "epoch: 2 step: 389, loss is 0.010594023391604424\n",
      "epoch: 2 step: 390, loss is 0.11309818178415298\n",
      "epoch: 2 step: 391, loss is 0.00997332762926817\n",
      "epoch: 2 step: 392, loss is 0.02139565348625183\n",
      "epoch: 2 step: 393, loss is 0.35844582319259644\n",
      "epoch: 2 step: 394, loss is 0.12871821224689484\n",
      "epoch: 2 step: 395, loss is 0.038535214960575104\n",
      "epoch: 2 step: 396, loss is 0.0897534191608429\n",
      "epoch: 2 step: 397, loss is 0.08379051834344864\n",
      "epoch: 2 step: 398, loss is 0.05823424458503723\n",
      "epoch: 2 step: 399, loss is 0.02500845678150654\n",
      "epoch: 2 step: 400, loss is 0.008341224864125252\n",
      "epoch: 2 step: 401, loss is 0.1147230863571167\n",
      "epoch: 2 step: 402, loss is 0.015372328460216522\n",
      "epoch: 2 step: 403, loss is 0.12388155609369278\n",
      "epoch: 2 step: 404, loss is 0.0028753115329891443\n",
      "epoch: 2 step: 405, loss is 0.003684940515086055\n",
      "epoch: 2 step: 406, loss is 0.22580963373184204\n",
      "epoch: 2 step: 407, loss is 0.2678256332874298\n",
      "epoch: 2 step: 408, loss is 0.05522938445210457\n",
      "epoch: 2 step: 409, loss is 0.06568270921707153\n",
      "epoch: 2 step: 410, loss is 0.018453938886523247\n",
      "epoch: 2 step: 411, loss is 0.018075210973620415\n",
      "epoch: 2 step: 412, loss is 0.014876647852361202\n",
      "epoch: 2 step: 413, loss is 0.25397828221321106\n",
      "epoch: 2 step: 414, loss is 0.17774079740047455\n",
      "epoch: 2 step: 415, loss is 0.2491355687379837\n",
      "epoch: 2 step: 416, loss is 0.3019814193248749\n",
      "epoch: 2 step: 417, loss is 0.11804147809743881\n",
      "epoch: 2 step: 418, loss is 0.04620416462421417\n",
      "epoch: 2 step: 419, loss is 0.060902029275894165\n",
      "epoch: 2 step: 420, loss is 0.11310930550098419\n",
      "epoch: 2 step: 421, loss is 0.019849520176649094\n",
      "epoch: 2 step: 422, loss is 0.07021849602460861\n",
      "epoch: 2 step: 423, loss is 0.1174042671918869\n",
      "epoch: 2 step: 424, loss is 0.26550841331481934\n",
      "epoch: 2 step: 425, loss is 0.1604849249124527\n",
      "epoch: 2 step: 426, loss is 0.10190080851316452\n",
      "epoch: 2 step: 427, loss is 0.026318401098251343\n",
      "epoch: 2 step: 428, loss is 0.04603084549307823\n",
      "epoch: 2 step: 429, loss is 0.019536036998033524\n",
      "epoch: 2 step: 430, loss is 0.10140129923820496\n",
      "epoch: 2 step: 431, loss is 0.06528544425964355\n",
      "epoch: 2 step: 432, loss is 0.12943974137306213\n",
      "epoch: 2 step: 433, loss is 0.060052916407585144\n",
      "epoch: 2 step: 434, loss is 0.01155758835375309\n",
      "epoch: 2 step: 435, loss is 0.013898065313696861\n",
      "epoch: 2 step: 436, loss is 0.01752350479364395\n",
      "epoch: 2 step: 437, loss is 0.0374918058514595\n",
      "epoch: 2 step: 438, loss is 0.00890103168785572\n",
      "epoch: 2 step: 439, loss is 0.013717377558350563\n",
      "epoch: 2 step: 440, loss is 0.024680515751242638\n",
      "epoch: 2 step: 441, loss is 0.1059187799692154\n",
      "epoch: 2 step: 442, loss is 0.024564778432250023\n",
      "epoch: 2 step: 443, loss is 0.15399357676506042\n",
      "epoch: 2 step: 444, loss is 0.0323207788169384\n",
      "epoch: 2 step: 445, loss is 0.017138514667749405\n",
      "epoch: 2 step: 446, loss is 0.048260647803545\n",
      "epoch: 2 step: 447, loss is 0.6335518956184387\n",
      "epoch: 2 step: 448, loss is 0.0845889002084732\n",
      "epoch: 2 step: 449, loss is 0.010385694913566113\n",
      "epoch: 2 step: 450, loss is 0.004562967922538519\n",
      "epoch: 2 step: 451, loss is 0.09746110439300537\n",
      "epoch: 2 step: 452, loss is 0.09581974893808365\n",
      "epoch: 2 step: 453, loss is 0.10086766630411148\n",
      "epoch: 2 step: 454, loss is 0.15846937894821167\n",
      "epoch: 2 step: 455, loss is 0.0036178668960928917\n",
      "epoch: 2 step: 456, loss is 0.08508262038230896\n",
      "epoch: 2 step: 457, loss is 0.0508774034678936\n",
      "epoch: 2 step: 458, loss is 0.042589131742715836\n",
      "epoch: 2 step: 459, loss is 0.012175696901977062\n",
      "epoch: 2 step: 460, loss is 0.024437282234430313\n",
      "epoch: 2 step: 461, loss is 0.07404787838459015\n",
      "epoch: 2 step: 462, loss is 0.056628450751304626\n",
      "epoch: 2 step: 463, loss is 0.03593843802809715\n",
      "epoch: 2 step: 464, loss is 0.01911522075533867\n",
      "epoch: 2 step: 465, loss is 0.005178774707019329\n",
      "epoch: 2 step: 466, loss is 0.05301932245492935\n",
      "epoch: 2 step: 467, loss is 0.00808507390320301\n",
      "epoch: 2 step: 468, loss is 0.04000038281083107\n",
      "epoch: 2 step: 469, loss is 0.049814749509096146\n",
      "epoch: 2 step: 470, loss is 0.0024278287310153246\n",
      "epoch: 2 step: 471, loss is 0.19330565631389618\n",
      "epoch: 2 step: 472, loss is 0.015707572922110558\n",
      "epoch: 2 step: 473, loss is 0.025218479335308075\n",
      "epoch: 2 step: 474, loss is 0.006981248036026955\n",
      "epoch: 2 step: 475, loss is 0.006251251325011253\n",
      "epoch: 2 step: 476, loss is 0.031058816239237785\n",
      "epoch: 2 step: 477, loss is 0.18478544056415558\n",
      "epoch: 2 step: 478, loss is 0.05814555287361145\n",
      "epoch: 2 step: 479, loss is 0.005255636293441057\n",
      "epoch: 2 step: 480, loss is 0.00859083142131567\n",
      "epoch: 2 step: 481, loss is 0.004341666121035814\n",
      "epoch: 2 step: 482, loss is 0.03897187113761902\n",
      "epoch: 2 step: 483, loss is 0.18537947535514832\n",
      "epoch: 2 step: 484, loss is 0.07958538085222244\n",
      "epoch: 2 step: 485, loss is 0.14528733491897583\n",
      "epoch: 2 step: 486, loss is 0.0024650967679917812\n",
      "epoch: 2 step: 487, loss is 0.025651993229985237\n",
      "epoch: 2 step: 488, loss is 0.23031771183013916\n",
      "epoch: 2 step: 489, loss is 0.030102286487817764\n",
      "epoch: 2 step: 490, loss is 0.055186815559864044\n",
      "epoch: 2 step: 491, loss is 0.029691621661186218\n",
      "epoch: 2 step: 492, loss is 0.18440048396587372\n",
      "epoch: 2 step: 493, loss is 0.03569024056196213\n",
      "epoch: 2 step: 494, loss is 0.09159928560256958\n",
      "epoch: 2 step: 495, loss is 0.09867751598358154\n",
      "epoch: 2 step: 496, loss is 0.0778426080942154\n",
      "epoch: 2 step: 497, loss is 0.17219378054141998\n",
      "epoch: 2 step: 498, loss is 0.05518336594104767\n",
      "epoch: 2 step: 499, loss is 0.06141416355967522\n",
      "epoch: 2 step: 500, loss is 0.01720537804067135\n",
      "epoch: 2 step: 501, loss is 0.017948461696505547\n",
      "epoch: 2 step: 502, loss is 0.21354852616786957\n",
      "epoch: 2 step: 503, loss is 0.0009007127955555916\n",
      "epoch: 2 step: 504, loss is 0.0956348180770874\n",
      "epoch: 2 step: 505, loss is 0.03695495426654816\n",
      "epoch: 2 step: 506, loss is 0.02142724022269249\n",
      "epoch: 2 step: 507, loss is 0.008037556894123554\n",
      "epoch: 2 step: 508, loss is 0.05955620855093002\n",
      "epoch: 2 step: 509, loss is 0.028549732640385628\n",
      "epoch: 2 step: 510, loss is 0.04300030693411827\n",
      "epoch: 2 step: 511, loss is 0.08151400834321976\n",
      "epoch: 2 step: 512, loss is 0.004615388810634613\n",
      "epoch: 2 step: 513, loss is 0.06854371726512909\n",
      "epoch: 2 step: 514, loss is 0.01689290814101696\n",
      "epoch: 2 step: 515, loss is 0.2636212408542633\n",
      "epoch: 2 step: 516, loss is 0.023212913423776627\n",
      "epoch: 2 step: 517, loss is 0.0032919645309448242\n",
      "epoch: 2 step: 518, loss is 0.04824557155370712\n",
      "epoch: 2 step: 519, loss is 0.16309474408626556\n",
      "epoch: 2 step: 520, loss is 0.006696089170873165\n",
      "epoch: 2 step: 521, loss is 0.0690397247672081\n",
      "epoch: 2 step: 522, loss is 0.051160458475351334\n",
      "epoch: 2 step: 523, loss is 0.0977427139878273\n",
      "epoch: 2 step: 524, loss is 0.08929114788770676\n",
      "epoch: 2 step: 525, loss is 0.13009324669837952\n",
      "epoch: 2 step: 526, loss is 0.03551387041807175\n",
      "epoch: 2 step: 527, loss is 0.7854085564613342\n",
      "epoch: 2 step: 528, loss is 0.020678525790572166\n",
      "epoch: 2 step: 529, loss is 0.050189707428216934\n",
      "epoch: 2 step: 530, loss is 0.0055365487933158875\n",
      "epoch: 2 step: 531, loss is 0.0328805036842823\n",
      "epoch: 2 step: 532, loss is 0.10862542688846588\n",
      "epoch: 2 step: 533, loss is 0.1578468382358551\n",
      "epoch: 2 step: 534, loss is 0.009552029892802238\n",
      "epoch: 2 step: 535, loss is 0.02676314301788807\n",
      "epoch: 2 step: 536, loss is 0.07160978019237518\n",
      "epoch: 2 step: 537, loss is 0.07828330248594284\n",
      "epoch: 2 step: 538, loss is 0.022956090047955513\n",
      "epoch: 2 step: 539, loss is 0.0024515893310308456\n",
      "epoch: 2 step: 540, loss is 0.00577176408842206\n",
      "epoch: 2 step: 541, loss is 0.025731893256306648\n",
      "epoch: 2 step: 542, loss is 0.021154897287487984\n",
      "epoch: 2 step: 543, loss is 0.05348379537463188\n",
      "epoch: 2 step: 544, loss is 0.024534890428185463\n",
      "epoch: 2 step: 545, loss is 0.035977959632873535\n",
      "epoch: 2 step: 546, loss is 0.03974578157067299\n",
      "epoch: 2 step: 547, loss is 0.3643699586391449\n",
      "epoch: 2 step: 548, loss is 0.1366046667098999\n",
      "epoch: 2 step: 549, loss is 0.07218272984027863\n",
      "epoch: 2 step: 550, loss is 0.07780805975198746\n",
      "epoch: 2 step: 551, loss is 0.01920589990913868\n",
      "epoch: 2 step: 552, loss is 0.03234873712062836\n",
      "epoch: 2 step: 553, loss is 0.02905442751944065\n",
      "epoch: 2 step: 554, loss is 0.017025554552674294\n",
      "epoch: 2 step: 555, loss is 0.0882076770067215\n",
      "epoch: 2 step: 556, loss is 0.014667701907455921\n",
      "epoch: 2 step: 557, loss is 0.05406251922249794\n",
      "epoch: 2 step: 558, loss is 0.012005951255559921\n",
      "epoch: 2 step: 559, loss is 0.05776083096861839\n",
      "epoch: 2 step: 560, loss is 0.13432912528514862\n",
      "epoch: 2 step: 561, loss is 0.0384451299905777\n",
      "epoch: 2 step: 562, loss is 0.0056518311612308025\n",
      "epoch: 2 step: 563, loss is 0.02952694334089756\n",
      "epoch: 2 step: 564, loss is 0.1347886472940445\n",
      "epoch: 2 step: 565, loss is 0.15134702622890472\n",
      "epoch: 2 step: 566, loss is 0.015304610133171082\n",
      "epoch: 2 step: 567, loss is 0.1397612988948822\n",
      "epoch: 2 step: 568, loss is 0.051804810762405396\n",
      "epoch: 2 step: 569, loss is 0.05034025013446808\n",
      "epoch: 2 step: 570, loss is 0.06140485033392906\n",
      "epoch: 2 step: 571, loss is 0.12819933891296387\n",
      "epoch: 2 step: 572, loss is 0.01181294210255146\n",
      "epoch: 2 step: 573, loss is 0.1534871757030487\n",
      "epoch: 2 step: 574, loss is 0.01644361950457096\n",
      "epoch: 2 step: 575, loss is 0.018834827467799187\n",
      "epoch: 2 step: 576, loss is 0.10138571262359619\n",
      "epoch: 2 step: 577, loss is 0.03041117824614048\n",
      "epoch: 2 step: 578, loss is 0.010512697510421276\n",
      "epoch: 2 step: 579, loss is 0.09292346984148026\n",
      "epoch: 2 step: 580, loss is 0.26736921072006226\n",
      "epoch: 2 step: 581, loss is 0.0030677439644932747\n",
      "epoch: 2 step: 582, loss is 0.005098635330796242\n",
      "epoch: 2 step: 583, loss is 0.07173590362071991\n",
      "epoch: 2 step: 584, loss is 0.028303708881139755\n",
      "epoch: 2 step: 585, loss is 0.40977931022644043\n",
      "epoch: 2 step: 586, loss is 0.013072283007204533\n",
      "epoch: 2 step: 587, loss is 0.0041460199281573296\n",
      "epoch: 2 step: 588, loss is 0.03548939898610115\n",
      "epoch: 2 step: 589, loss is 0.010244794189929962\n",
      "epoch: 2 step: 590, loss is 0.0029815612360835075\n",
      "epoch: 2 step: 591, loss is 0.041943881660699844\n",
      "epoch: 2 step: 592, loss is 0.013941069133579731\n",
      "epoch: 2 step: 593, loss is 0.019073735922574997\n",
      "epoch: 2 step: 594, loss is 0.14952504634857178\n",
      "epoch: 2 step: 595, loss is 0.11288528144359589\n",
      "epoch: 2 step: 596, loss is 0.26594704389572144\n",
      "epoch: 2 step: 597, loss is 0.01029907539486885\n",
      "epoch: 2 step: 598, loss is 0.12939469516277313\n",
      "epoch: 2 step: 599, loss is 0.010823966935276985\n",
      "epoch: 2 step: 600, loss is 0.06721619516611099\n",
      "epoch: 2 step: 601, loss is 0.007006581407040358\n",
      "epoch: 2 step: 602, loss is 0.019610457122325897\n",
      "epoch: 2 step: 603, loss is 0.13850460946559906\n",
      "epoch: 2 step: 604, loss is 0.006215048488229513\n",
      "epoch: 2 step: 605, loss is 0.06078227609395981\n",
      "epoch: 2 step: 606, loss is 0.18706011772155762\n",
      "epoch: 2 step: 607, loss is 0.045447010546922684\n",
      "epoch: 2 step: 608, loss is 0.004714662674814463\n",
      "epoch: 2 step: 609, loss is 0.0028116991743445396\n",
      "epoch: 2 step: 610, loss is 0.04319325461983681\n",
      "epoch: 2 step: 611, loss is 0.010783858597278595\n",
      "epoch: 2 step: 612, loss is 0.12409704178571701\n",
      "epoch: 2 step: 613, loss is 0.08259262144565582\n",
      "epoch: 2 step: 614, loss is 0.03166573867201805\n",
      "epoch: 2 step: 615, loss is 0.05648766830563545\n",
      "epoch: 2 step: 616, loss is 0.1666204184293747\n",
      "epoch: 2 step: 617, loss is 0.06922005116939545\n",
      "epoch: 2 step: 618, loss is 0.018144236877560616\n",
      "epoch: 2 step: 619, loss is 0.123241126537323\n",
      "epoch: 2 step: 620, loss is 0.06177688017487526\n",
      "epoch: 2 step: 621, loss is 0.003515478689223528\n",
      "epoch: 2 step: 622, loss is 0.17206355929374695\n",
      "epoch: 2 step: 623, loss is 0.07354690879583359\n",
      "epoch: 2 step: 624, loss is 0.03233713656663895\n",
      "epoch: 2 step: 625, loss is 0.14720278978347778\n",
      "epoch: 2 step: 626, loss is 0.07693491131067276\n",
      "epoch: 2 step: 627, loss is 0.000842036388348788\n",
      "epoch: 2 step: 628, loss is 0.30812782049179077\n",
      "epoch: 2 step: 629, loss is 0.18675971031188965\n",
      "epoch: 2 step: 630, loss is 0.04546032100915909\n",
      "epoch: 2 step: 631, loss is 0.07455570995807648\n",
      "epoch: 2 step: 632, loss is 0.11261224001646042\n",
      "epoch: 2 step: 633, loss is 0.044077664613723755\n",
      "epoch: 2 step: 634, loss is 0.010487392544746399\n",
      "epoch: 2 step: 635, loss is 0.035495538264513016\n",
      "epoch: 2 step: 636, loss is 0.01398191973567009\n",
      "epoch: 2 step: 637, loss is 0.018406536430120468\n",
      "epoch: 2 step: 638, loss is 0.15497925877571106\n",
      "epoch: 2 step: 639, loss is 0.002468973631039262\n",
      "epoch: 2 step: 640, loss is 0.010751626454293728\n",
      "epoch: 2 step: 641, loss is 0.0055650826543569565\n",
      "epoch: 2 step: 642, loss is 0.1222696304321289\n",
      "epoch: 2 step: 643, loss is 0.014806526713073254\n",
      "epoch: 2 step: 644, loss is 0.08412251621484756\n",
      "epoch: 2 step: 645, loss is 0.0071991216391325\n",
      "epoch: 2 step: 646, loss is 0.0269150547683239\n",
      "epoch: 2 step: 647, loss is 0.03783915936946869\n",
      "epoch: 2 step: 648, loss is 0.008827991783618927\n",
      "epoch: 2 step: 649, loss is 0.21253277361392975\n",
      "epoch: 2 step: 650, loss is 0.013074634596705437\n",
      "epoch: 2 step: 651, loss is 0.1542271226644516\n",
      "epoch: 2 step: 652, loss is 0.032567620277404785\n",
      "epoch: 2 step: 653, loss is 0.019379161298274994\n",
      "epoch: 2 step: 654, loss is 0.03801082819700241\n",
      "epoch: 2 step: 655, loss is 0.06147577613592148\n",
      "epoch: 2 step: 656, loss is 0.006895097903907299\n",
      "epoch: 2 step: 657, loss is 0.027703430503606796\n",
      "epoch: 2 step: 658, loss is 0.014305522665381432\n",
      "epoch: 2 step: 659, loss is 0.1157212108373642\n",
      "epoch: 2 step: 660, loss is 0.025443976745009422\n",
      "epoch: 2 step: 661, loss is 0.06039542332291603\n",
      "epoch: 2 step: 662, loss is 0.0020802426151931286\n",
      "epoch: 2 step: 663, loss is 0.029815373942255974\n",
      "epoch: 2 step: 664, loss is 0.020746849477291107\n",
      "epoch: 2 step: 665, loss is 0.047865621745586395\n",
      "epoch: 2 step: 666, loss is 0.002744336612522602\n",
      "epoch: 2 step: 667, loss is 0.0007148445583879948\n",
      "epoch: 2 step: 668, loss is 0.01867763139307499\n",
      "epoch: 2 step: 669, loss is 0.06082000955939293\n",
      "epoch: 2 step: 670, loss is 0.002235104562714696\n",
      "epoch: 2 step: 671, loss is 0.004024514462798834\n",
      "epoch: 2 step: 672, loss is 0.10872524231672287\n",
      "epoch: 2 step: 673, loss is 0.07261646538972855\n",
      "epoch: 2 step: 674, loss is 0.0017964609432965517\n",
      "epoch: 2 step: 675, loss is 0.0985954999923706\n",
      "epoch: 2 step: 676, loss is 0.14596012234687805\n",
      "epoch: 2 step: 677, loss is 0.0072208186611533165\n",
      "epoch: 2 step: 678, loss is 0.25478917360305786\n",
      "epoch: 2 step: 679, loss is 0.00641186349093914\n",
      "epoch: 2 step: 680, loss is 0.0049278330989181995\n",
      "epoch: 2 step: 681, loss is 0.20021826028823853\n",
      "epoch: 2 step: 682, loss is 0.10736329853534698\n",
      "epoch: 2 step: 683, loss is 0.1174040138721466\n",
      "epoch: 2 step: 684, loss is 0.047822315245866776\n",
      "epoch: 2 step: 685, loss is 0.027393503114581108\n",
      "epoch: 2 step: 686, loss is 0.1595786064863205\n",
      "epoch: 2 step: 687, loss is 0.006935910787433386\n",
      "epoch: 2 step: 688, loss is 0.03596808761358261\n",
      "epoch: 2 step: 689, loss is 0.00290110195055604\n",
      "epoch: 2 step: 690, loss is 0.010967181995511055\n",
      "epoch: 2 step: 691, loss is 0.00991620309650898\n",
      "epoch: 2 step: 692, loss is 0.14085905253887177\n",
      "epoch: 2 step: 693, loss is 0.1921362280845642\n",
      "epoch: 2 step: 694, loss is 0.1815182864665985\n",
      "epoch: 2 step: 695, loss is 0.03810946270823479\n",
      "epoch: 2 step: 696, loss is 0.01909678988158703\n",
      "epoch: 2 step: 697, loss is 0.1308719366788864\n",
      "epoch: 2 step: 698, loss is 0.01450298447161913\n",
      "epoch: 2 step: 699, loss is 0.1624305099248886\n",
      "epoch: 2 step: 700, loss is 0.13697050511837006\n",
      "epoch: 2 step: 701, loss is 0.11913798004388809\n",
      "epoch: 2 step: 702, loss is 0.13958394527435303\n",
      "epoch: 2 step: 703, loss is 0.10381707549095154\n",
      "epoch: 2 step: 704, loss is 0.02643965743482113\n",
      "epoch: 2 step: 705, loss is 0.013889012858271599\n",
      "epoch: 2 step: 706, loss is 0.06181243434548378\n",
      "epoch: 2 step: 707, loss is 0.020006580278277397\n",
      "epoch: 2 step: 708, loss is 0.010379998944699764\n",
      "epoch: 2 step: 709, loss is 0.08968984335660934\n",
      "epoch: 2 step: 710, loss is 0.025967001914978027\n",
      "epoch: 2 step: 711, loss is 0.08377324044704437\n",
      "epoch: 2 step: 712, loss is 0.03439042717218399\n",
      "epoch: 2 step: 713, loss is 0.1844537854194641\n",
      "epoch: 2 step: 714, loss is 0.022129066288471222\n",
      "epoch: 2 step: 715, loss is 0.13043072819709778\n",
      "epoch: 2 step: 716, loss is 0.017123116180300713\n",
      "epoch: 2 step: 717, loss is 0.00313773425295949\n",
      "epoch: 2 step: 718, loss is 0.008751907385885715\n",
      "epoch: 2 step: 719, loss is 0.25271207094192505\n",
      "epoch: 2 step: 720, loss is 0.08503970503807068\n",
      "epoch: 2 step: 721, loss is 0.02453656494617462\n",
      "epoch: 2 step: 722, loss is 0.06376511603593826\n",
      "epoch: 2 step: 723, loss is 0.21209144592285156\n",
      "epoch: 2 step: 724, loss is 0.05846492573618889\n",
      "epoch: 2 step: 725, loss is 0.16420692205429077\n",
      "epoch: 2 step: 726, loss is 0.0139820147305727\n",
      "epoch: 2 step: 727, loss is 0.07332707941532135\n",
      "epoch: 2 step: 728, loss is 0.1523800939321518\n",
      "epoch: 2 step: 729, loss is 0.03227231279015541\n",
      "epoch: 2 step: 730, loss is 0.02584451623260975\n",
      "epoch: 2 step: 731, loss is 0.05132985860109329\n",
      "epoch: 2 step: 732, loss is 0.0161883644759655\n",
      "epoch: 2 step: 733, loss is 0.09132940322160721\n",
      "epoch: 2 step: 734, loss is 0.010666722431778908\n",
      "epoch: 2 step: 735, loss is 0.1729910671710968\n",
      "epoch: 2 step: 736, loss is 0.013299592770636082\n",
      "epoch: 2 step: 737, loss is 0.13595333695411682\n",
      "epoch: 2 step: 738, loss is 0.009952203370630741\n",
      "epoch: 2 step: 739, loss is 0.012618081644177437\n",
      "epoch: 2 step: 740, loss is 0.09381989389657974\n",
      "epoch: 2 step: 741, loss is 0.020439669489860535\n",
      "epoch: 2 step: 742, loss is 0.10323890298604965\n",
      "epoch: 2 step: 743, loss is 0.24707290530204773\n",
      "epoch: 2 step: 744, loss is 0.013964265584945679\n",
      "epoch: 2 step: 745, loss is 0.010290390811860561\n",
      "epoch: 2 step: 746, loss is 0.21048100292682648\n",
      "epoch: 2 step: 747, loss is 0.008294655941426754\n",
      "epoch: 2 step: 748, loss is 0.11433690786361694\n",
      "epoch: 2 step: 749, loss is 0.007365328259766102\n",
      "epoch: 2 step: 750, loss is 0.0660235583782196\n",
      "epoch: 2 step: 751, loss is 0.10641861706972122\n",
      "epoch: 2 step: 752, loss is 0.172439306974411\n",
      "epoch: 2 step: 753, loss is 0.1309853345155716\n",
      "epoch: 2 step: 754, loss is 0.12774842977523804\n",
      "epoch: 2 step: 755, loss is 0.11959832906723022\n",
      "epoch: 2 step: 756, loss is 0.007688052486628294\n",
      "epoch: 2 step: 757, loss is 0.07880807667970657\n",
      "epoch: 2 step: 758, loss is 0.04935771971940994\n",
      "epoch: 2 step: 759, loss is 0.05470229685306549\n",
      "epoch: 2 step: 760, loss is 0.001761767896823585\n",
      "epoch: 2 step: 761, loss is 0.17345188558101654\n",
      "epoch: 2 step: 762, loss is 0.04378960281610489\n",
      "epoch: 2 step: 763, loss is 0.03867984563112259\n",
      "epoch: 2 step: 764, loss is 0.05231301486492157\n",
      "epoch: 2 step: 765, loss is 0.2732796370983124\n",
      "epoch: 2 step: 766, loss is 0.04821443185210228\n",
      "epoch: 2 step: 767, loss is 0.014079718850553036\n",
      "epoch: 2 step: 768, loss is 0.086025170981884\n",
      "epoch: 2 step: 769, loss is 0.11977484822273254\n",
      "epoch: 2 step: 770, loss is 0.013431115075945854\n",
      "epoch: 2 step: 771, loss is 0.05770767480134964\n",
      "epoch: 2 step: 772, loss is 0.0027633123099803925\n",
      "epoch: 2 step: 773, loss is 0.10191469639539719\n",
      "epoch: 2 step: 774, loss is 0.14207550883293152\n",
      "epoch: 2 step: 775, loss is 0.20881099998950958\n",
      "epoch: 2 step: 776, loss is 0.16693441569805145\n",
      "epoch: 2 step: 777, loss is 0.10125087946653366\n",
      "epoch: 2 step: 778, loss is 0.017980756238102913\n",
      "epoch: 2 step: 779, loss is 0.11449781060218811\n",
      "epoch: 2 step: 780, loss is 0.00481589836999774\n",
      "epoch: 2 step: 781, loss is 0.015280025079846382\n",
      "epoch: 2 step: 782, loss is 0.05897999182343483\n",
      "epoch: 2 step: 783, loss is 0.005551219452172518\n",
      "epoch: 2 step: 784, loss is 0.10796178132295609\n",
      "epoch: 2 step: 785, loss is 0.0032104647252708673\n",
      "epoch: 2 step: 786, loss is 0.085401251912117\n",
      "epoch: 2 step: 787, loss is 0.017359765246510506\n",
      "epoch: 2 step: 788, loss is 0.057697925716638565\n",
      "epoch: 2 step: 789, loss is 0.05928603187203407\n",
      "epoch: 2 step: 790, loss is 0.009852522984147072\n",
      "epoch: 2 step: 791, loss is 0.17013420164585114\n",
      "epoch: 2 step: 792, loss is 0.1134471595287323\n",
      "epoch: 2 step: 793, loss is 0.014224760234355927\n",
      "epoch: 2 step: 794, loss is 0.011117816902697086\n",
      "epoch: 2 step: 795, loss is 0.059253524988889694\n",
      "epoch: 2 step: 796, loss is 0.02354559674859047\n",
      "epoch: 2 step: 797, loss is 0.016983216628432274\n",
      "epoch: 2 step: 798, loss is 0.03633096441626549\n",
      "epoch: 2 step: 799, loss is 0.04430829733610153\n",
      "epoch: 2 step: 800, loss is 0.029028283432126045\n",
      "epoch: 2 step: 801, loss is 0.018316209316253662\n",
      "epoch: 2 step: 802, loss is 0.010239985771477222\n",
      "epoch: 2 step: 803, loss is 0.016676079481840134\n",
      "epoch: 2 step: 804, loss is 0.017439546063542366\n",
      "epoch: 2 step: 805, loss is 0.3994370102882385\n",
      "epoch: 2 step: 806, loss is 0.029427925124764442\n",
      "epoch: 2 step: 807, loss is 0.14670813083648682\n",
      "epoch: 2 step: 808, loss is 0.10772102326154709\n",
      "epoch: 2 step: 809, loss is 0.005504675209522247\n",
      "epoch: 2 step: 810, loss is 0.10431428253650665\n",
      "epoch: 2 step: 811, loss is 0.02722073905169964\n",
      "epoch: 2 step: 812, loss is 0.24536697566509247\n",
      "epoch: 2 step: 813, loss is 0.035952985286712646\n",
      "epoch: 2 step: 814, loss is 0.06434918940067291\n",
      "epoch: 2 step: 815, loss is 0.2212350219488144\n",
      "epoch: 2 step: 816, loss is 0.05503112077713013\n",
      "epoch: 2 step: 817, loss is 0.03423168137669563\n",
      "epoch: 2 step: 818, loss is 0.010526805184781551\n",
      "epoch: 2 step: 819, loss is 0.020976971834897995\n",
      "epoch: 2 step: 820, loss is 0.01754257269203663\n",
      "epoch: 2 step: 821, loss is 0.009054158814251423\n",
      "epoch: 2 step: 822, loss is 0.029552945867180824\n",
      "epoch: 2 step: 823, loss is 0.12972292304039001\n",
      "epoch: 2 step: 824, loss is 0.033082637935876846\n",
      "epoch: 2 step: 825, loss is 0.09080052375793457\n",
      "epoch: 2 step: 826, loss is 0.001620342954993248\n",
      "epoch: 2 step: 827, loss is 0.018130453303456306\n",
      "epoch: 2 step: 828, loss is 0.0028368490748107433\n",
      "epoch: 2 step: 829, loss is 0.14425979554653168\n",
      "epoch: 2 step: 830, loss is 0.0215995404869318\n",
      "epoch: 2 step: 831, loss is 0.005811695940792561\n",
      "epoch: 2 step: 832, loss is 0.005420632194727659\n",
      "epoch: 2 step: 833, loss is 0.09420865774154663\n",
      "epoch: 2 step: 834, loss is 0.0012889159843325615\n",
      "epoch: 2 step: 835, loss is 0.021050991490483284\n",
      "epoch: 2 step: 836, loss is 0.1328725665807724\n",
      "epoch: 2 step: 837, loss is 0.05282507464289665\n",
      "epoch: 2 step: 838, loss is 0.005121475551277399\n",
      "epoch: 2 step: 839, loss is 0.005220699589699507\n",
      "epoch: 2 step: 840, loss is 0.003061181167140603\n",
      "epoch: 2 step: 841, loss is 0.005657058209180832\n",
      "epoch: 2 step: 842, loss is 0.15851877629756927\n",
      "epoch: 2 step: 843, loss is 0.03757520765066147\n",
      "epoch: 2 step: 844, loss is 0.12158771604299545\n",
      "epoch: 2 step: 845, loss is 0.06510818749666214\n",
      "epoch: 2 step: 846, loss is 0.054177671670913696\n",
      "epoch: 2 step: 847, loss is 0.003409583820030093\n",
      "epoch: 2 step: 848, loss is 0.007670902647078037\n",
      "epoch: 2 step: 849, loss is 0.08030030131340027\n",
      "epoch: 2 step: 850, loss is 0.024922387674450874\n",
      "epoch: 2 step: 851, loss is 0.03342943638563156\n",
      "epoch: 2 step: 852, loss is 0.2892341911792755\n",
      "epoch: 2 step: 853, loss is 0.08704731613397598\n",
      "epoch: 2 step: 854, loss is 0.0041731265373528\n",
      "epoch: 2 step: 855, loss is 0.030638139694929123\n",
      "epoch: 2 step: 856, loss is 0.06527391076087952\n",
      "epoch: 2 step: 857, loss is 0.012781630270183086\n",
      "epoch: 2 step: 858, loss is 0.041830189526081085\n",
      "epoch: 2 step: 859, loss is 0.005454807076603174\n",
      "epoch: 2 step: 860, loss is 0.49706730246543884\n",
      "epoch: 2 step: 861, loss is 0.13145463168621063\n",
      "epoch: 2 step: 862, loss is 0.011394388973712921\n",
      "epoch: 2 step: 863, loss is 0.09847374260425568\n",
      "epoch: 2 step: 864, loss is 0.12417487800121307\n",
      "epoch: 2 step: 865, loss is 0.015885457396507263\n",
      "epoch: 2 step: 866, loss is 0.002406095387414098\n",
      "epoch: 2 step: 867, loss is 0.18644104897975922\n",
      "epoch: 2 step: 868, loss is 0.21956996619701385\n",
      "epoch: 2 step: 869, loss is 0.014877250418066978\n",
      "epoch: 2 step: 870, loss is 0.0672621876001358\n",
      "epoch: 2 step: 871, loss is 0.23649364709854126\n",
      "epoch: 2 step: 872, loss is 0.10528289526700974\n",
      "epoch: 2 step: 873, loss is 0.08775293081998825\n",
      "epoch: 2 step: 874, loss is 0.057018667459487915\n",
      "epoch: 2 step: 875, loss is 0.17060458660125732\n",
      "epoch: 2 step: 876, loss is 0.11546127498149872\n",
      "epoch: 2 step: 877, loss is 0.30226123332977295\n",
      "epoch: 2 step: 878, loss is 0.08290062099695206\n",
      "epoch: 2 step: 879, loss is 0.10594116151332855\n",
      "epoch: 2 step: 880, loss is 0.21549516916275024\n",
      "epoch: 2 step: 881, loss is 0.0375983752310276\n",
      "epoch: 2 step: 882, loss is 0.05890102684497833\n",
      "epoch: 2 step: 883, loss is 0.1281776875257492\n",
      "epoch: 2 step: 884, loss is 0.08046313375234604\n",
      "epoch: 2 step: 885, loss is 0.0836491733789444\n",
      "epoch: 2 step: 886, loss is 0.31480318307876587\n",
      "epoch: 2 step: 887, loss is 0.06721477210521698\n",
      "epoch: 2 step: 888, loss is 0.209027498960495\n",
      "epoch: 2 step: 889, loss is 0.08079897612333298\n",
      "epoch: 2 step: 890, loss is 0.16073107719421387\n",
      "epoch: 2 step: 891, loss is 0.18750101327896118\n",
      "epoch: 2 step: 892, loss is 0.03485848382115364\n",
      "epoch: 2 step: 893, loss is 0.025428876280784607\n",
      "epoch: 2 step: 894, loss is 0.0511697381734848\n",
      "epoch: 2 step: 895, loss is 0.002664051251485944\n",
      "epoch: 2 step: 896, loss is 0.012679622508585453\n",
      "epoch: 2 step: 897, loss is 0.0833631306886673\n",
      "epoch: 2 step: 898, loss is 0.1533670723438263\n",
      "epoch: 2 step: 899, loss is 0.1633307784795761\n",
      "epoch: 2 step: 900, loss is 0.1079598143696785\n",
      "epoch: 2 step: 901, loss is 0.027446400374174118\n",
      "epoch: 2 step: 902, loss is 0.005529711488634348\n",
      "epoch: 2 step: 903, loss is 0.039117008447647095\n",
      "epoch: 2 step: 904, loss is 0.024064604192972183\n",
      "epoch: 2 step: 905, loss is 0.009667935781180859\n",
      "epoch: 2 step: 906, loss is 0.09282480180263519\n",
      "epoch: 2 step: 907, loss is 0.11214783042669296\n",
      "epoch: 2 step: 908, loss is 0.03482377156615257\n",
      "epoch: 2 step: 909, loss is 0.013754607178270817\n",
      "epoch: 2 step: 910, loss is 0.1521124690771103\n",
      "epoch: 2 step: 911, loss is 0.017566587775945663\n",
      "epoch: 2 step: 912, loss is 0.011678831651806831\n",
      "epoch: 2 step: 913, loss is 0.12213768064975739\n",
      "epoch: 2 step: 914, loss is 0.008011776022613049\n",
      "epoch: 2 step: 915, loss is 0.011861056089401245\n",
      "epoch: 2 step: 916, loss is 0.03475162014365196\n",
      "epoch: 2 step: 917, loss is 0.03957132622599602\n",
      "epoch: 2 step: 918, loss is 0.0032172210048884153\n",
      "epoch: 2 step: 919, loss is 0.007019686512649059\n",
      "epoch: 2 step: 920, loss is 0.3907439410686493\n",
      "epoch: 2 step: 921, loss is 0.04179700091481209\n",
      "epoch: 2 step: 922, loss is 0.12897241115570068\n",
      "epoch: 2 step: 923, loss is 0.004681298974901438\n",
      "epoch: 2 step: 924, loss is 0.002815540647134185\n",
      "epoch: 2 step: 925, loss is 0.13676415383815765\n",
      "epoch: 2 step: 926, loss is 0.023494364693760872\n",
      "epoch: 2 step: 927, loss is 0.01810750551521778\n",
      "epoch: 2 step: 928, loss is 0.22810201346874237\n",
      "epoch: 2 step: 929, loss is 0.07700714468955994\n",
      "epoch: 2 step: 930, loss is 0.0098654143512249\n",
      "epoch: 2 step: 931, loss is 0.117642842233181\n",
      "epoch: 2 step: 932, loss is 0.01629887893795967\n",
      "epoch: 2 step: 933, loss is 0.08105052262544632\n",
      "epoch: 2 step: 934, loss is 0.1656116247177124\n",
      "epoch: 2 step: 935, loss is 0.05741514638066292\n",
      "epoch: 2 step: 936, loss is 0.03881123289465904\n",
      "epoch: 2 step: 937, loss is 0.01691921427845955\n",
      "epoch: 2 step: 938, loss is 0.0059087541885674\n",
      "epoch: 2 step: 939, loss is 0.19618360698223114\n",
      "epoch: 2 step: 940, loss is 0.19872741401195526\n",
      "epoch: 2 step: 941, loss is 0.11579727381467819\n",
      "epoch: 2 step: 942, loss is 0.31660112738609314\n",
      "epoch: 2 step: 943, loss is 0.11369259655475616\n",
      "epoch: 2 step: 944, loss is 0.1271236687898636\n",
      "epoch: 2 step: 945, loss is 0.05860258266329765\n",
      "epoch: 2 step: 946, loss is 0.30953580141067505\n",
      "epoch: 2 step: 947, loss is 0.005229901522397995\n",
      "epoch: 2 step: 948, loss is 0.007992641068994999\n",
      "epoch: 2 step: 949, loss is 0.14613711833953857\n",
      "epoch: 2 step: 950, loss is 0.12958328425884247\n",
      "epoch: 2 step: 951, loss is 0.012346640229225159\n",
      "epoch: 2 step: 952, loss is 0.004537725355476141\n",
      "epoch: 2 step: 953, loss is 0.009000010788440704\n",
      "epoch: 2 step: 954, loss is 0.036722052842378616\n",
      "epoch: 2 step: 955, loss is 0.036503203213214874\n",
      "epoch: 2 step: 956, loss is 0.020301060751080513\n",
      "epoch: 2 step: 957, loss is 0.09644048660993576\n",
      "epoch: 2 step: 958, loss is 0.2525973916053772\n",
      "epoch: 2 step: 959, loss is 0.010748257860541344\n",
      "epoch: 2 step: 960, loss is 0.00973252858966589\n",
      "epoch: 2 step: 961, loss is 0.11880705505609512\n",
      "epoch: 2 step: 962, loss is 0.06365225464105606\n",
      "epoch: 2 step: 963, loss is 0.21968616545200348\n",
      "epoch: 2 step: 964, loss is 0.0861305370926857\n",
      "epoch: 2 step: 965, loss is 0.1299462765455246\n",
      "epoch: 2 step: 966, loss is 0.012534760870039463\n",
      "epoch: 2 step: 967, loss is 0.022990332916378975\n",
      "epoch: 2 step: 968, loss is 0.00895906612277031\n",
      "epoch: 2 step: 969, loss is 0.016564013436436653\n",
      "epoch: 2 step: 970, loss is 0.17659364640712738\n",
      "epoch: 2 step: 971, loss is 0.029298026114702225\n",
      "epoch: 2 step: 972, loss is 0.08707252889871597\n",
      "epoch: 2 step: 973, loss is 0.07470749318599701\n",
      "epoch: 2 step: 974, loss is 0.025386203080415726\n",
      "epoch: 2 step: 975, loss is 0.021907513961195946\n",
      "epoch: 2 step: 976, loss is 0.012829054147005081\n",
      "epoch: 2 step: 977, loss is 0.1295139044523239\n",
      "epoch: 2 step: 978, loss is 0.07182713598012924\n",
      "epoch: 2 step: 979, loss is 0.13130655884742737\n",
      "epoch: 2 step: 980, loss is 0.051378704607486725\n",
      "epoch: 2 step: 981, loss is 0.006846296135336161\n",
      "epoch: 2 step: 982, loss is 0.19247111678123474\n",
      "epoch: 2 step: 983, loss is 0.0059483652003109455\n",
      "epoch: 2 step: 984, loss is 0.14128521084785461\n",
      "epoch: 2 step: 985, loss is 0.008711147122085094\n",
      "epoch: 2 step: 986, loss is 0.005141771864145994\n",
      "epoch: 2 step: 987, loss is 0.19959771633148193\n",
      "epoch: 2 step: 988, loss is 0.038873091340065\n",
      "epoch: 2 step: 989, loss is 0.08343597501516342\n",
      "epoch: 2 step: 990, loss is 0.029669029638171196\n",
      "epoch: 2 step: 991, loss is 0.14984390139579773\n",
      "epoch: 2 step: 992, loss is 0.004587232135236263\n",
      "epoch: 2 step: 993, loss is 0.03760990872979164\n",
      "epoch: 2 step: 994, loss is 0.20282484591007233\n",
      "epoch: 2 step: 995, loss is 0.0707511454820633\n",
      "epoch: 2 step: 996, loss is 0.01577656902372837\n",
      "epoch: 2 step: 997, loss is 0.020862357690930367\n",
      "epoch: 2 step: 998, loss is 0.015364476479589939\n",
      "epoch: 2 step: 999, loss is 0.08489207178354263\n",
      "epoch: 2 step: 1000, loss is 0.02003716118633747\n",
      "epoch: 2 step: 1001, loss is 0.04688706994056702\n",
      "epoch: 2 step: 1002, loss is 0.23473787307739258\n",
      "epoch: 2 step: 1003, loss is 0.05628056079149246\n",
      "epoch: 2 step: 1004, loss is 0.010947084054350853\n",
      "epoch: 2 step: 1005, loss is 0.024508032947778702\n",
      "epoch: 2 step: 1006, loss is 0.11301366239786148\n",
      "epoch: 2 step: 1007, loss is 0.03299792483448982\n",
      "epoch: 2 step: 1008, loss is 0.29261747002601624\n",
      "epoch: 2 step: 1009, loss is 0.0038713624235242605\n",
      "epoch: 2 step: 1010, loss is 0.14693762362003326\n",
      "epoch: 2 step: 1011, loss is 0.011763840913772583\n",
      "epoch: 2 step: 1012, loss is 0.12311289459466934\n",
      "epoch: 2 step: 1013, loss is 0.010434459894895554\n",
      "epoch: 2 step: 1014, loss is 0.18841615319252014\n",
      "epoch: 2 step: 1015, loss is 0.08388855308294296\n",
      "epoch: 2 step: 1016, loss is 0.03230610489845276\n",
      "epoch: 2 step: 1017, loss is 0.07194080948829651\n",
      "epoch: 2 step: 1018, loss is 0.0017605966422706842\n",
      "epoch: 2 step: 1019, loss is 0.4947366714477539\n",
      "epoch: 2 step: 1020, loss is 0.018505729734897614\n",
      "epoch: 2 step: 1021, loss is 0.007254708092659712\n",
      "epoch: 2 step: 1022, loss is 0.003407949348911643\n",
      "epoch: 2 step: 1023, loss is 0.1718970686197281\n",
      "epoch: 2 step: 1024, loss is 0.012873194180428982\n",
      "epoch: 2 step: 1025, loss is 0.03553326427936554\n",
      "epoch: 2 step: 1026, loss is 0.036198198795318604\n",
      "epoch: 2 step: 1027, loss is 0.01567961648106575\n",
      "epoch: 2 step: 1028, loss is 0.027913598343729973\n",
      "epoch: 2 step: 1029, loss is 0.17923365533351898\n",
      "epoch: 2 step: 1030, loss is 0.017600493505597115\n",
      "epoch: 2 step: 1031, loss is 0.048895906656980515\n",
      "epoch: 2 step: 1032, loss is 0.10496093332767487\n",
      "epoch: 2 step: 1033, loss is 0.004641612991690636\n",
      "epoch: 2 step: 1034, loss is 0.17960894107818604\n",
      "epoch: 2 step: 1035, loss is 0.014142079278826714\n",
      "epoch: 2 step: 1036, loss is 0.16766372323036194\n",
      "epoch: 2 step: 1037, loss is 0.02323012612760067\n",
      "epoch: 2 step: 1038, loss is 0.1157611832022667\n",
      "epoch: 2 step: 1039, loss is 0.42256423830986023\n",
      "epoch: 2 step: 1040, loss is 0.019032420590519905\n",
      "epoch: 2 step: 1041, loss is 0.0022789379581809044\n",
      "epoch: 2 step: 1042, loss is 0.028659427538514137\n",
      "epoch: 2 step: 1043, loss is 0.02557349018752575\n",
      "epoch: 2 step: 1044, loss is 0.042131491005420685\n",
      "epoch: 2 step: 1045, loss is 0.07022882252931595\n",
      "epoch: 2 step: 1046, loss is 0.09596572071313858\n",
      "epoch: 2 step: 1047, loss is 0.025886742398142815\n",
      "epoch: 2 step: 1048, loss is 0.022895056754350662\n",
      "epoch: 2 step: 1049, loss is 0.023577457293868065\n",
      "epoch: 2 step: 1050, loss is 0.051582008600234985\n",
      "epoch: 2 step: 1051, loss is 0.02061779424548149\n",
      "epoch: 2 step: 1052, loss is 0.091999351978302\n",
      "epoch: 2 step: 1053, loss is 0.05702470242977142\n",
      "epoch: 2 step: 1054, loss is 0.19169141352176666\n",
      "epoch: 2 step: 1055, loss is 0.004279986023902893\n",
      "epoch: 2 step: 1056, loss is 0.001917448709718883\n",
      "epoch: 2 step: 1057, loss is 0.018222626298666\n",
      "epoch: 2 step: 1058, loss is 0.07350032776594162\n",
      "epoch: 2 step: 1059, loss is 0.02465183101594448\n",
      "epoch: 2 step: 1060, loss is 0.04724886640906334\n",
      "epoch: 2 step: 1061, loss is 0.1826828420162201\n",
      "epoch: 2 step: 1062, loss is 0.003946213982999325\n",
      "epoch: 2 step: 1063, loss is 0.004473971202969551\n",
      "epoch: 2 step: 1064, loss is 0.04834653064608574\n",
      "epoch: 2 step: 1065, loss is 0.004653169773519039\n",
      "epoch: 2 step: 1066, loss is 0.034290120005607605\n",
      "epoch: 2 step: 1067, loss is 0.22963137924671173\n",
      "epoch: 2 step: 1068, loss is 0.1360473334789276\n",
      "epoch: 2 step: 1069, loss is 0.03816591575741768\n",
      "epoch: 2 step: 1070, loss is 0.002337515586987138\n",
      "epoch: 2 step: 1071, loss is 0.007588502950966358\n",
      "epoch: 2 step: 1072, loss is 0.004688818473368883\n",
      "epoch: 2 step: 1073, loss is 0.006344634108245373\n",
      "epoch: 2 step: 1074, loss is 0.011651812121272087\n",
      "epoch: 2 step: 1075, loss is 0.01893002912402153\n",
      "epoch: 2 step: 1076, loss is 0.1378641426563263\n",
      "epoch: 2 step: 1077, loss is 0.06412307918071747\n",
      "epoch: 2 step: 1078, loss is 0.12150532007217407\n",
      "epoch: 2 step: 1079, loss is 0.029344771057367325\n",
      "epoch: 2 step: 1080, loss is 0.04406966269016266\n",
      "epoch: 2 step: 1081, loss is 0.06578687578439713\n",
      "epoch: 2 step: 1082, loss is 0.18243268132209778\n",
      "epoch: 2 step: 1083, loss is 0.17986509203910828\n",
      "epoch: 2 step: 1084, loss is 0.004198085051029921\n",
      "epoch: 2 step: 1085, loss is 0.1114870086312294\n",
      "epoch: 2 step: 1086, loss is 0.10208268463611603\n",
      "epoch: 2 step: 1087, loss is 0.024884261190891266\n",
      "epoch: 2 step: 1088, loss is 0.10381431877613068\n",
      "epoch: 2 step: 1089, loss is 0.015417931601405144\n",
      "epoch: 2 step: 1090, loss is 0.02136167138814926\n",
      "epoch: 2 step: 1091, loss is 0.04388697072863579\n",
      "epoch: 2 step: 1092, loss is 0.010998384095728397\n",
      "epoch: 2 step: 1093, loss is 0.15821143984794617\n",
      "epoch: 2 step: 1094, loss is 0.011287237517535686\n",
      "epoch: 2 step: 1095, loss is 0.12556886672973633\n",
      "epoch: 2 step: 1096, loss is 0.006598610896617174\n",
      "epoch: 2 step: 1097, loss is 0.012698073871433735\n",
      "epoch: 2 step: 1098, loss is 0.016381418332457542\n",
      "epoch: 2 step: 1099, loss is 0.1304043084383011\n",
      "epoch: 2 step: 1100, loss is 0.00854856614023447\n",
      "epoch: 2 step: 1101, loss is 0.007936852052807808\n",
      "epoch: 2 step: 1102, loss is 0.0026237056590616703\n",
      "epoch: 2 step: 1103, loss is 0.033502280712127686\n",
      "epoch: 2 step: 1104, loss is 0.0376938171684742\n",
      "epoch: 2 step: 1105, loss is 0.015608163550496101\n",
      "epoch: 2 step: 1106, loss is 0.06175162270665169\n",
      "epoch: 2 step: 1107, loss is 0.0036423394922167063\n",
      "epoch: 2 step: 1108, loss is 0.05891070142388344\n",
      "epoch: 2 step: 1109, loss is 0.03751225396990776\n",
      "epoch: 2 step: 1110, loss is 0.22148536145687103\n",
      "epoch: 2 step: 1111, loss is 0.017862940207123756\n",
      "epoch: 2 step: 1112, loss is 0.2505590319633484\n",
      "epoch: 2 step: 1113, loss is 0.036369387060403824\n",
      "epoch: 2 step: 1114, loss is 0.028605492785573006\n",
      "epoch: 2 step: 1115, loss is 0.005713572725653648\n",
      "epoch: 2 step: 1116, loss is 0.09543809294700623\n",
      "epoch: 2 step: 1117, loss is 0.00348509568721056\n",
      "epoch: 2 step: 1118, loss is 0.07988186180591583\n",
      "epoch: 2 step: 1119, loss is 0.013226663693785667\n",
      "epoch: 2 step: 1120, loss is 0.23078255355358124\n",
      "epoch: 2 step: 1121, loss is 0.00681394524872303\n",
      "epoch: 2 step: 1122, loss is 0.3659113943576813\n",
      "epoch: 2 step: 1123, loss is 0.05848987028002739\n",
      "epoch: 2 step: 1124, loss is 0.013012593612074852\n",
      "epoch: 2 step: 1125, loss is 0.0027047377079725266\n",
      "epoch: 2 step: 1126, loss is 0.03966183215379715\n",
      "epoch: 2 step: 1127, loss is 0.058549437671899796\n",
      "epoch: 2 step: 1128, loss is 0.016996752470731735\n",
      "epoch: 2 step: 1129, loss is 0.0027095128316432238\n",
      "epoch: 2 step: 1130, loss is 0.441209614276886\n",
      "epoch: 2 step: 1131, loss is 0.11481072008609772\n",
      "epoch: 2 step: 1132, loss is 0.00559047469869256\n",
      "epoch: 2 step: 1133, loss is 0.04592389985918999\n",
      "epoch: 2 step: 1134, loss is 0.0030868686735630035\n",
      "epoch: 2 step: 1135, loss is 0.09371533989906311\n",
      "epoch: 2 step: 1136, loss is 0.003996976651251316\n",
      "epoch: 2 step: 1137, loss is 0.09888919442892075\n",
      "epoch: 2 step: 1138, loss is 0.01027744635939598\n",
      "epoch: 2 step: 1139, loss is 0.01849536783993244\n",
      "epoch: 2 step: 1140, loss is 0.018957747146487236\n",
      "epoch: 2 step: 1141, loss is 0.0019494927255436778\n",
      "epoch: 2 step: 1142, loss is 0.0025731197092682123\n",
      "epoch: 2 step: 1143, loss is 0.24810948967933655\n",
      "epoch: 2 step: 1144, loss is 0.001573752029798925\n",
      "epoch: 2 step: 1145, loss is 0.06561336666345596\n",
      "epoch: 2 step: 1146, loss is 0.007233179174363613\n",
      "epoch: 2 step: 1147, loss is 0.010086338967084885\n",
      "epoch: 2 step: 1148, loss is 0.13043025135993958\n",
      "epoch: 2 step: 1149, loss is 0.023243894800543785\n",
      "epoch: 2 step: 1150, loss is 0.005789995193481445\n",
      "epoch: 2 step: 1151, loss is 0.005755619145929813\n",
      "epoch: 2 step: 1152, loss is 0.015115481801331043\n",
      "epoch: 2 step: 1153, loss is 0.20840734243392944\n",
      "epoch: 2 step: 1154, loss is 0.22183343768119812\n",
      "epoch: 2 step: 1155, loss is 0.07561634480953217\n",
      "epoch: 2 step: 1156, loss is 0.010351723060011864\n",
      "epoch: 2 step: 1157, loss is 0.12272805720567703\n",
      "epoch: 2 step: 1158, loss is 0.05540608987212181\n",
      "epoch: 2 step: 1159, loss is 0.011589557863771915\n",
      "epoch: 2 step: 1160, loss is 0.04869112744927406\n",
      "epoch: 2 step: 1161, loss is 0.03716347739100456\n",
      "epoch: 2 step: 1162, loss is 0.0024125336203724146\n",
      "epoch: 2 step: 1163, loss is 0.05023960396647453\n",
      "epoch: 2 step: 1164, loss is 0.26804471015930176\n",
      "epoch: 2 step: 1165, loss is 0.07532509416341782\n",
      "epoch: 2 step: 1166, loss is 0.005826180335134268\n",
      "epoch: 2 step: 1167, loss is 0.005264395382255316\n",
      "epoch: 2 step: 1168, loss is 0.07502525299787521\n",
      "epoch: 2 step: 1169, loss is 0.041952285915613174\n",
      "epoch: 2 step: 1170, loss is 0.004291442688554525\n",
      "epoch: 2 step: 1171, loss is 0.010403158143162727\n",
      "epoch: 2 step: 1172, loss is 0.01250117551535368\n",
      "epoch: 2 step: 1173, loss is 0.012674113735556602\n",
      "epoch: 2 step: 1174, loss is 0.023068897426128387\n",
      "epoch: 2 step: 1175, loss is 0.1221046969294548\n",
      "epoch: 2 step: 1176, loss is 0.013953660614788532\n",
      "epoch: 2 step: 1177, loss is 0.19907476007938385\n",
      "epoch: 2 step: 1178, loss is 0.005373714491724968\n",
      "epoch: 2 step: 1179, loss is 0.008010191842913628\n",
      "epoch: 2 step: 1180, loss is 0.006370478309690952\n",
      "epoch: 2 step: 1181, loss is 0.0049045505002141\n",
      "epoch: 2 step: 1182, loss is 0.01191922277212143\n",
      "epoch: 2 step: 1183, loss is 0.009515173733234406\n",
      "epoch: 2 step: 1184, loss is 0.01894504390656948\n",
      "epoch: 2 step: 1185, loss is 0.018539398908615112\n",
      "epoch: 2 step: 1186, loss is 0.24123722314834595\n",
      "epoch: 2 step: 1187, loss is 0.008363709785044193\n",
      "epoch: 2 step: 1188, loss is 0.14530152082443237\n",
      "epoch: 2 step: 1189, loss is 0.006700202357023954\n",
      "epoch: 2 step: 1190, loss is 0.033340923488140106\n",
      "epoch: 2 step: 1191, loss is 0.016019873321056366\n",
      "epoch: 2 step: 1192, loss is 0.025175848975777626\n",
      "epoch: 2 step: 1193, loss is 0.004299468826502562\n",
      "epoch: 2 step: 1194, loss is 0.0017153436783701181\n",
      "epoch: 2 step: 1195, loss is 0.007378664333373308\n",
      "epoch: 2 step: 1196, loss is 0.010013343766331673\n",
      "epoch: 2 step: 1197, loss is 0.011420894414186478\n",
      "epoch: 2 step: 1198, loss is 0.14159463346004486\n",
      "epoch: 2 step: 1199, loss is 0.07990756630897522\n",
      "epoch: 2 step: 1200, loss is 0.03885667771100998\n",
      "epoch: 2 step: 1201, loss is 0.11004772037267685\n",
      "epoch: 2 step: 1202, loss is 0.04977203905582428\n",
      "epoch: 2 step: 1203, loss is 0.0032525849528610706\n",
      "epoch: 2 step: 1204, loss is 0.0033571594394743443\n",
      "epoch: 2 step: 1205, loss is 0.012795502319931984\n",
      "epoch: 2 step: 1206, loss is 0.010908634401857853\n",
      "epoch: 2 step: 1207, loss is 0.022474683821201324\n",
      "epoch: 2 step: 1208, loss is 0.1164417415857315\n",
      "epoch: 2 step: 1209, loss is 0.020605511963367462\n",
      "epoch: 2 step: 1210, loss is 0.010067779570817947\n",
      "epoch: 2 step: 1211, loss is 0.003950606100261211\n",
      "epoch: 2 step: 1212, loss is 0.12886033952236176\n",
      "epoch: 2 step: 1213, loss is 0.08489753305912018\n",
      "epoch: 2 step: 1214, loss is 0.06756123155355453\n",
      "epoch: 2 step: 1215, loss is 0.044889770448207855\n",
      "epoch: 2 step: 1216, loss is 0.08129405975341797\n",
      "epoch: 2 step: 1217, loss is 0.08711887151002884\n",
      "epoch: 2 step: 1218, loss is 0.19432465732097626\n",
      "epoch: 2 step: 1219, loss is 0.15225698053836823\n",
      "epoch: 2 step: 1220, loss is 0.045389991253614426\n",
      "epoch: 2 step: 1221, loss is 0.0019701861310750246\n",
      "epoch: 2 step: 1222, loss is 0.002736796857789159\n",
      "epoch: 2 step: 1223, loss is 0.09281671792268753\n",
      "epoch: 2 step: 1224, loss is 0.08482209593057632\n",
      "epoch: 2 step: 1225, loss is 0.014423588290810585\n",
      "epoch: 2 step: 1226, loss is 0.0008166921907104552\n",
      "epoch: 2 step: 1227, loss is 0.09627126157283783\n",
      "epoch: 2 step: 1228, loss is 0.018793068826198578\n",
      "epoch: 2 step: 1229, loss is 0.19840478897094727\n",
      "epoch: 2 step: 1230, loss is 0.010925914160907269\n",
      "epoch: 2 step: 1231, loss is 0.005852628964930773\n",
      "epoch: 2 step: 1232, loss is 0.0701344758272171\n",
      "epoch: 2 step: 1233, loss is 0.10474635660648346\n",
      "epoch: 2 step: 1234, loss is 0.028392484411597252\n",
      "epoch: 2 step: 1235, loss is 0.2203371226787567\n",
      "epoch: 2 step: 1236, loss is 0.010228726081550121\n",
      "epoch: 2 step: 1237, loss is 0.05345531925559044\n",
      "epoch: 2 step: 1238, loss is 0.038779038935899734\n",
      "epoch: 2 step: 1239, loss is 0.21810239553451538\n",
      "epoch: 2 step: 1240, loss is 0.025499572977423668\n",
      "epoch: 2 step: 1241, loss is 0.08675745874643326\n",
      "epoch: 2 step: 1242, loss is 0.0026017469353973866\n",
      "epoch: 2 step: 1243, loss is 0.0063036647625267506\n",
      "epoch: 2 step: 1244, loss is 0.0011025206185877323\n",
      "epoch: 2 step: 1245, loss is 0.05339512228965759\n",
      "epoch: 2 step: 1246, loss is 0.14272117614746094\n",
      "epoch: 2 step: 1247, loss is 0.07454320788383484\n",
      "epoch: 2 step: 1248, loss is 0.08288747817277908\n",
      "epoch: 2 step: 1249, loss is 0.013507852330803871\n",
      "epoch: 2 step: 1250, loss is 0.1786169707775116\n",
      "epoch: 2 step: 1251, loss is 0.03604534640908241\n",
      "epoch: 2 step: 1252, loss is 0.09596030414104462\n",
      "epoch: 2 step: 1253, loss is 0.009446311742067337\n",
      "epoch: 2 step: 1254, loss is 0.0008041826076805592\n",
      "epoch: 2 step: 1255, loss is 0.0025701578706502914\n",
      "epoch: 2 step: 1256, loss is 0.0036323145031929016\n",
      "epoch: 2 step: 1257, loss is 0.0020203141029924154\n",
      "epoch: 2 step: 1258, loss is 0.07989481836557388\n",
      "epoch: 2 step: 1259, loss is 0.003244113177061081\n",
      "epoch: 2 step: 1260, loss is 0.002233274281024933\n",
      "epoch: 2 step: 1261, loss is 0.007521221414208412\n",
      "epoch: 2 step: 1262, loss is 0.2783472537994385\n",
      "epoch: 2 step: 1263, loss is 0.007153018843382597\n",
      "epoch: 2 step: 1264, loss is 0.004906062968075275\n",
      "epoch: 2 step: 1265, loss is 0.0018338714726269245\n",
      "epoch: 2 step: 1266, loss is 0.08423817902803421\n",
      "epoch: 2 step: 1267, loss is 0.09530682116746902\n",
      "epoch: 2 step: 1268, loss is 0.003846831386908889\n",
      "epoch: 2 step: 1269, loss is 0.18060465157032013\n",
      "epoch: 2 step: 1270, loss is 0.012803272344172001\n",
      "epoch: 2 step: 1271, loss is 0.18482887744903564\n",
      "epoch: 2 step: 1272, loss is 0.0024185108486562967\n",
      "epoch: 2 step: 1273, loss is 0.013501379638910294\n",
      "epoch: 2 step: 1274, loss is 0.046848364174366\n",
      "epoch: 2 step: 1275, loss is 0.0018397343810647726\n",
      "epoch: 2 step: 1276, loss is 0.006625290960073471\n",
      "epoch: 2 step: 1277, loss is 0.18517054617404938\n",
      "epoch: 2 step: 1278, loss is 0.021216966211795807\n",
      "epoch: 2 step: 1279, loss is 0.04963844269514084\n",
      "epoch: 2 step: 1280, loss is 0.028933091089129448\n",
      "epoch: 2 step: 1281, loss is 0.0082729896530509\n",
      "epoch: 2 step: 1282, loss is 0.032092221081256866\n",
      "epoch: 2 step: 1283, loss is 0.005587427411228418\n",
      "epoch: 2 step: 1284, loss is 0.003563912585377693\n",
      "epoch: 2 step: 1285, loss is 0.08271897584199905\n",
      "epoch: 2 step: 1286, loss is 0.019804732874035835\n",
      "epoch: 2 step: 1287, loss is 0.011665168218314648\n",
      "epoch: 2 step: 1288, loss is 0.016818411648273468\n",
      "epoch: 2 step: 1289, loss is 0.0026895711198449135\n",
      "epoch: 2 step: 1290, loss is 0.0068414174020290375\n",
      "epoch: 2 step: 1291, loss is 0.030368825420737267\n",
      "epoch: 2 step: 1292, loss is 0.2854127585887909\n",
      "epoch: 2 step: 1293, loss is 0.0034268773160874844\n",
      "epoch: 2 step: 1294, loss is 0.001908984617330134\n",
      "epoch: 2 step: 1295, loss is 0.009401296265423298\n",
      "epoch: 2 step: 1296, loss is 0.025755904614925385\n",
      "epoch: 2 step: 1297, loss is 0.018947577103972435\n",
      "epoch: 2 step: 1298, loss is 0.032570384442806244\n",
      "epoch: 2 step: 1299, loss is 0.006839943118393421\n",
      "epoch: 2 step: 1300, loss is 0.0577123761177063\n",
      "epoch: 2 step: 1301, loss is 0.033127788454294205\n",
      "epoch: 2 step: 1302, loss is 0.05099596083164215\n",
      "epoch: 2 step: 1303, loss is 0.06765206158161163\n",
      "epoch: 2 step: 1304, loss is 0.006484702229499817\n",
      "epoch: 2 step: 1305, loss is 0.050915710628032684\n",
      "epoch: 2 step: 1306, loss is 0.06450841575860977\n",
      "epoch: 2 step: 1307, loss is 0.21245788037776947\n",
      "epoch: 2 step: 1308, loss is 0.023000922054052353\n",
      "epoch: 2 step: 1309, loss is 0.37854692339897156\n",
      "epoch: 2 step: 1310, loss is 0.030646581202745438\n",
      "epoch: 2 step: 1311, loss is 0.2289752960205078\n",
      "epoch: 2 step: 1312, loss is 0.0014467011205852032\n",
      "epoch: 2 step: 1313, loss is 0.04286148026585579\n",
      "epoch: 2 step: 1314, loss is 0.15466009080410004\n",
      "epoch: 2 step: 1315, loss is 0.08345025032758713\n",
      "epoch: 2 step: 1316, loss is 0.01697395369410515\n",
      "epoch: 2 step: 1317, loss is 0.010822691023349762\n",
      "epoch: 2 step: 1318, loss is 0.014937914907932281\n",
      "epoch: 2 step: 1319, loss is 0.1572069376707077\n",
      "epoch: 2 step: 1320, loss is 0.00243524182587862\n",
      "epoch: 2 step: 1321, loss is 0.0016438714228570461\n",
      "epoch: 2 step: 1322, loss is 0.003302649362012744\n",
      "epoch: 2 step: 1323, loss is 0.003814373165369034\n",
      "epoch: 2 step: 1324, loss is 0.04196060448884964\n",
      "epoch: 2 step: 1325, loss is 0.10682764649391174\n",
      "epoch: 2 step: 1326, loss is 0.18249192833900452\n",
      "epoch: 2 step: 1327, loss is 0.008971977047622204\n",
      "epoch: 2 step: 1328, loss is 0.0028064993675798178\n",
      "epoch: 2 step: 1329, loss is 0.08523324131965637\n",
      "epoch: 2 step: 1330, loss is 0.14988841116428375\n",
      "epoch: 2 step: 1331, loss is 0.10836392641067505\n",
      "epoch: 2 step: 1332, loss is 0.17252159118652344\n",
      "epoch: 2 step: 1333, loss is 0.043134286999702454\n",
      "epoch: 2 step: 1334, loss is 0.11549168080091476\n",
      "epoch: 2 step: 1335, loss is 0.09634613990783691\n",
      "epoch: 2 step: 1336, loss is 0.04388481006026268\n",
      "epoch: 2 step: 1337, loss is 0.01612669788300991\n",
      "epoch: 2 step: 1338, loss is 0.09298824518918991\n",
      "epoch: 2 step: 1339, loss is 0.007123409304767847\n",
      "epoch: 2 step: 1340, loss is 0.004909759387373924\n",
      "epoch: 2 step: 1341, loss is 0.014409615658223629\n",
      "epoch: 2 step: 1342, loss is 0.0024800633545964956\n",
      "epoch: 2 step: 1343, loss is 0.06849311292171478\n",
      "epoch: 2 step: 1344, loss is 0.023613855242729187\n",
      "epoch: 2 step: 1345, loss is 0.007895311340689659\n",
      "epoch: 2 step: 1346, loss is 0.05859261006116867\n",
      "epoch: 2 step: 1347, loss is 0.004834703169763088\n",
      "epoch: 2 step: 1348, loss is 0.0211098100990057\n",
      "epoch: 2 step: 1349, loss is 0.014836680144071579\n",
      "epoch: 2 step: 1350, loss is 0.12188853323459625\n",
      "epoch: 2 step: 1351, loss is 0.0035125515423715115\n",
      "epoch: 2 step: 1352, loss is 0.03085251897573471\n",
      "epoch: 2 step: 1353, loss is 0.1609826385974884\n",
      "epoch: 2 step: 1354, loss is 0.006487652659416199\n",
      "epoch: 2 step: 1355, loss is 0.14664630591869354\n",
      "epoch: 2 step: 1356, loss is 0.216586172580719\n",
      "epoch: 2 step: 1357, loss is 0.1710989624261856\n",
      "epoch: 2 step: 1358, loss is 0.015147363767027855\n",
      "epoch: 2 step: 1359, loss is 0.0035233821254223585\n",
      "epoch: 2 step: 1360, loss is 0.017050888389348984\n",
      "epoch: 2 step: 1361, loss is 0.05882471799850464\n",
      "epoch: 2 step: 1362, loss is 0.008780687116086483\n",
      "epoch: 2 step: 1363, loss is 0.004420234821736813\n",
      "epoch: 2 step: 1364, loss is 0.11255521327257156\n",
      "epoch: 2 step: 1365, loss is 0.12172335386276245\n",
      "epoch: 2 step: 1366, loss is 0.017086049541831017\n",
      "epoch: 2 step: 1367, loss is 0.020080452784895897\n",
      "epoch: 2 step: 1368, loss is 0.30373910069465637\n",
      "epoch: 2 step: 1369, loss is 0.04550028219819069\n",
      "epoch: 2 step: 1370, loss is 0.005241528153419495\n",
      "epoch: 2 step: 1371, loss is 0.005371092818677425\n",
      "epoch: 2 step: 1372, loss is 0.009810468181967735\n",
      "epoch: 2 step: 1373, loss is 0.02345476672053337\n",
      "epoch: 2 step: 1374, loss is 0.15619732439517975\n",
      "epoch: 2 step: 1375, loss is 0.0039273216389119625\n",
      "epoch: 2 step: 1376, loss is 0.020295346155762672\n",
      "epoch: 2 step: 1377, loss is 0.026459211483597755\n",
      "epoch: 2 step: 1378, loss is 0.028430674225091934\n",
      "epoch: 2 step: 1379, loss is 0.0259652491658926\n",
      "epoch: 2 step: 1380, loss is 0.011465909890830517\n",
      "epoch: 2 step: 1381, loss is 0.04059753194451332\n",
      "epoch: 2 step: 1382, loss is 0.030401818454265594\n",
      "epoch: 2 step: 1383, loss is 0.18633435666561127\n",
      "epoch: 2 step: 1384, loss is 0.0071907639503479\n",
      "epoch: 2 step: 1385, loss is 0.04455104097723961\n",
      "epoch: 2 step: 1386, loss is 0.01140381209552288\n",
      "epoch: 2 step: 1387, loss is 0.017204703763127327\n",
      "epoch: 2 step: 1388, loss is 0.003628955688327551\n",
      "epoch: 2 step: 1389, loss is 0.010776104405522346\n",
      "epoch: 2 step: 1390, loss is 0.002540410729125142\n",
      "epoch: 2 step: 1391, loss is 0.0034841997548937798\n",
      "epoch: 2 step: 1392, loss is 0.0053357030265033245\n",
      "epoch: 2 step: 1393, loss is 0.07800741493701935\n",
      "epoch: 2 step: 1394, loss is 0.06325585395097733\n",
      "epoch: 2 step: 1395, loss is 0.0027571849059313536\n",
      "epoch: 2 step: 1396, loss is 0.0549609437584877\n",
      "epoch: 2 step: 1397, loss is 0.062472909688949585\n",
      "epoch: 2 step: 1398, loss is 0.014163161627948284\n",
      "epoch: 2 step: 1399, loss is 0.002071758033707738\n",
      "epoch: 2 step: 1400, loss is 0.19757810235023499\n",
      "epoch: 2 step: 1401, loss is 0.008269676938652992\n",
      "epoch: 2 step: 1402, loss is 0.11843935400247574\n",
      "epoch: 2 step: 1403, loss is 0.0025479511823505163\n",
      "epoch: 2 step: 1404, loss is 0.06500134617090225\n",
      "epoch: 2 step: 1405, loss is 0.30465489625930786\n",
      "epoch: 2 step: 1406, loss is 0.2130132019519806\n",
      "epoch: 2 step: 1407, loss is 0.046807944774627686\n",
      "epoch: 2 step: 1408, loss is 0.012590973637998104\n",
      "epoch: 2 step: 1409, loss is 0.06651894003152847\n",
      "epoch: 2 step: 1410, loss is 0.08021193742752075\n",
      "epoch: 2 step: 1411, loss is 0.03448617830872536\n",
      "epoch: 2 step: 1412, loss is 0.004951751325279474\n",
      "epoch: 2 step: 1413, loss is 0.0328323058784008\n",
      "epoch: 2 step: 1414, loss is 0.004795696120709181\n",
      "epoch: 2 step: 1415, loss is 0.17330212891101837\n",
      "epoch: 2 step: 1416, loss is 0.1825452595949173\n",
      "epoch: 2 step: 1417, loss is 0.011257045902311802\n",
      "epoch: 2 step: 1418, loss is 0.009768446907401085\n",
      "epoch: 2 step: 1419, loss is 0.26516467332839966\n",
      "epoch: 2 step: 1420, loss is 0.0033166352659463882\n",
      "epoch: 2 step: 1421, loss is 0.07008422166109085\n",
      "epoch: 2 step: 1422, loss is 0.12820225954055786\n",
      "epoch: 2 step: 1423, loss is 0.008651603944599628\n",
      "epoch: 2 step: 1424, loss is 0.017787529155611992\n",
      "epoch: 2 step: 1425, loss is 0.06061588227748871\n",
      "epoch: 2 step: 1426, loss is 0.0028957901522517204\n",
      "epoch: 2 step: 1427, loss is 0.032938163727521896\n",
      "epoch: 2 step: 1428, loss is 0.0961519405245781\n",
      "epoch: 2 step: 1429, loss is 0.031078651547431946\n",
      "epoch: 2 step: 1430, loss is 0.052394382655620575\n",
      "epoch: 2 step: 1431, loss is 0.10845957696437836\n",
      "epoch: 2 step: 1432, loss is 0.013242817483842373\n",
      "epoch: 2 step: 1433, loss is 0.0793694332242012\n",
      "epoch: 2 step: 1434, loss is 0.002660494763404131\n",
      "epoch: 2 step: 1435, loss is 0.011681620962917805\n",
      "epoch: 2 step: 1436, loss is 0.01829010620713234\n",
      "epoch: 2 step: 1437, loss is 0.02003134973347187\n",
      "epoch: 2 step: 1438, loss is 0.07659344375133514\n",
      "epoch: 2 step: 1439, loss is 0.03060079738497734\n",
      "epoch: 2 step: 1440, loss is 0.0013817960862070322\n",
      "epoch: 2 step: 1441, loss is 0.1319768726825714\n",
      "epoch: 2 step: 1442, loss is 0.004001564811915159\n",
      "epoch: 2 step: 1443, loss is 0.03985622152686119\n",
      "epoch: 2 step: 1444, loss is 0.1663215607404709\n",
      "epoch: 2 step: 1445, loss is 0.011138233356177807\n",
      "epoch: 2 step: 1446, loss is 0.009656190872192383\n",
      "epoch: 2 step: 1447, loss is 0.154371440410614\n",
      "epoch: 2 step: 1448, loss is 0.0037449714727699757\n",
      "epoch: 2 step: 1449, loss is 0.037124983966350555\n",
      "epoch: 2 step: 1450, loss is 0.06431849300861359\n",
      "epoch: 2 step: 1451, loss is 0.01498500071465969\n",
      "epoch: 2 step: 1452, loss is 0.00712288822978735\n",
      "epoch: 2 step: 1453, loss is 0.0059629036113619804\n",
      "epoch: 2 step: 1454, loss is 0.007287824060767889\n",
      "epoch: 2 step: 1455, loss is 0.028925178572535515\n",
      "epoch: 2 step: 1456, loss is 0.06697016954421997\n",
      "epoch: 2 step: 1457, loss is 0.1985318511724472\n",
      "epoch: 2 step: 1458, loss is 0.005469440016895533\n",
      "epoch: 2 step: 1459, loss is 0.04348765313625336\n",
      "epoch: 2 step: 1460, loss is 0.031246367841959\n",
      "epoch: 2 step: 1461, loss is 0.0027364366687834263\n",
      "epoch: 2 step: 1462, loss is 0.040324509143829346\n",
      "epoch: 2 step: 1463, loss is 0.10851093381643295\n",
      "epoch: 2 step: 1464, loss is 0.04239391162991524\n",
      "epoch: 2 step: 1465, loss is 0.038359709084033966\n",
      "epoch: 2 step: 1466, loss is 0.036649152636528015\n",
      "epoch: 2 step: 1467, loss is 0.06870662420988083\n",
      "epoch: 2 step: 1468, loss is 0.001335433218628168\n",
      "epoch: 2 step: 1469, loss is 0.0021150363609194756\n",
      "epoch: 2 step: 1470, loss is 0.004168427083641291\n",
      "epoch: 2 step: 1471, loss is 0.011364085599780083\n",
      "epoch: 2 step: 1472, loss is 0.011689949780702591\n",
      "epoch: 2 step: 1473, loss is 0.19334834814071655\n",
      "epoch: 2 step: 1474, loss is 0.029033536091446877\n",
      "epoch: 2 step: 1475, loss is 0.055666882544755936\n",
      "epoch: 2 step: 1476, loss is 0.015250727534294128\n",
      "epoch: 2 step: 1477, loss is 0.07192644476890564\n",
      "epoch: 2 step: 1478, loss is 0.2373940497636795\n",
      "epoch: 2 step: 1479, loss is 0.010373897850513458\n",
      "epoch: 2 step: 1480, loss is 0.026758285239338875\n",
      "epoch: 2 step: 1481, loss is 0.3682723939418793\n",
      "epoch: 2 step: 1482, loss is 0.007727432064712048\n",
      "epoch: 2 step: 1483, loss is 0.03444015979766846\n",
      "epoch: 2 step: 1484, loss is 0.054630208760499954\n",
      "epoch: 2 step: 1485, loss is 0.006628974340856075\n",
      "epoch: 2 step: 1486, loss is 0.020880766212940216\n",
      "epoch: 2 step: 1487, loss is 0.14018601179122925\n",
      "epoch: 2 step: 1488, loss is 0.029302295297384262\n",
      "epoch: 2 step: 1489, loss is 0.04143866151571274\n",
      "epoch: 2 step: 1490, loss is 0.07996534556150436\n",
      "epoch: 2 step: 1491, loss is 0.2749252915382385\n",
      "epoch: 2 step: 1492, loss is 0.046746984124183655\n",
      "epoch: 2 step: 1493, loss is 0.06627250462770462\n",
      "epoch: 2 step: 1494, loss is 0.01670883223414421\n",
      "epoch: 2 step: 1495, loss is 0.14830978214740753\n",
      "epoch: 2 step: 1496, loss is 0.12690597772598267\n",
      "epoch: 2 step: 1497, loss is 0.006914710160344839\n",
      "epoch: 2 step: 1498, loss is 0.019308380782604218\n",
      "epoch: 2 step: 1499, loss is 0.004928604234009981\n",
      "epoch: 2 step: 1500, loss is 0.025786912068724632\n",
      "epoch: 2 step: 1501, loss is 0.045052457600831985\n",
      "epoch: 2 step: 1502, loss is 0.05052509158849716\n",
      "epoch: 2 step: 1503, loss is 0.07007165253162384\n",
      "epoch: 2 step: 1504, loss is 0.011011073365807533\n",
      "epoch: 2 step: 1505, loss is 0.16638928651809692\n",
      "epoch: 2 step: 1506, loss is 0.0012331861071288586\n",
      "epoch: 2 step: 1507, loss is 0.05686039477586746\n",
      "epoch: 2 step: 1508, loss is 0.001271946937777102\n",
      "epoch: 2 step: 1509, loss is 0.05499640479683876\n",
      "epoch: 2 step: 1510, loss is 0.026969825848937035\n",
      "epoch: 2 step: 1511, loss is 0.006327163428068161\n",
      "epoch: 2 step: 1512, loss is 0.009762141853570938\n",
      "epoch: 2 step: 1513, loss is 0.014398891478776932\n",
      "epoch: 2 step: 1514, loss is 0.009041309356689453\n",
      "epoch: 2 step: 1515, loss is 0.0016174949705600739\n",
      "epoch: 2 step: 1516, loss is 0.04724116250872612\n",
      "epoch: 2 step: 1517, loss is 0.0018461914733052254\n",
      "epoch: 2 step: 1518, loss is 0.07476018369197845\n",
      "epoch: 2 step: 1519, loss is 0.041081592440605164\n",
      "epoch: 2 step: 1520, loss is 0.2629968523979187\n",
      "epoch: 2 step: 1521, loss is 0.008901177905499935\n",
      "epoch: 2 step: 1522, loss is 0.013784061186015606\n",
      "epoch: 2 step: 1523, loss is 0.05416281521320343\n",
      "epoch: 2 step: 1524, loss is 0.25543883442878723\n",
      "epoch: 2 step: 1525, loss is 0.03418588638305664\n",
      "epoch: 2 step: 1526, loss is 0.01597970351576805\n",
      "epoch: 2 step: 1527, loss is 0.012109372764825821\n",
      "epoch: 2 step: 1528, loss is 0.017131106927990913\n",
      "epoch: 2 step: 1529, loss is 0.2441016286611557\n",
      "epoch: 2 step: 1530, loss is 0.005046168342232704\n",
      "epoch: 2 step: 1531, loss is 0.011840661987662315\n",
      "epoch: 2 step: 1532, loss is 0.03956439718604088\n",
      "epoch: 2 step: 1533, loss is 0.024464551359415054\n",
      "epoch: 2 step: 1534, loss is 0.16910956799983978\n",
      "epoch: 2 step: 1535, loss is 0.2845132052898407\n",
      "epoch: 2 step: 1536, loss is 0.004129122477024794\n",
      "epoch: 2 step: 1537, loss is 0.0029614234808832407\n",
      "epoch: 2 step: 1538, loss is 0.023042337968945503\n",
      "epoch: 2 step: 1539, loss is 0.0009331248002126813\n",
      "epoch: 2 step: 1540, loss is 0.06801789253950119\n",
      "epoch: 2 step: 1541, loss is 0.005264307837933302\n",
      "epoch: 2 step: 1542, loss is 0.009595174342393875\n",
      "epoch: 2 step: 1543, loss is 0.054011084139347076\n",
      "epoch: 2 step: 1544, loss is 0.045412253588438034\n",
      "epoch: 2 step: 1545, loss is 0.004387798253446817\n",
      "epoch: 2 step: 1546, loss is 0.1143650934100151\n",
      "epoch: 2 step: 1547, loss is 0.012800282798707485\n",
      "epoch: 2 step: 1548, loss is 0.23086823523044586\n",
      "epoch: 2 step: 1549, loss is 0.005017013754695654\n",
      "epoch: 2 step: 1550, loss is 0.0046740625984966755\n",
      "epoch: 2 step: 1551, loss is 0.00405303156003356\n",
      "epoch: 2 step: 1552, loss is 0.026242997497320175\n",
      "epoch: 2 step: 1553, loss is 0.008668970316648483\n",
      "epoch: 2 step: 1554, loss is 0.04535122215747833\n",
      "epoch: 2 step: 1555, loss is 0.022971948608756065\n",
      "epoch: 2 step: 1556, loss is 0.0064692748710513115\n",
      "epoch: 2 step: 1557, loss is 0.16073209047317505\n",
      "epoch: 2 step: 1558, loss is 0.019572660326957703\n",
      "epoch: 2 step: 1559, loss is 0.020242270082235336\n",
      "epoch: 2 step: 1560, loss is 0.0355115570127964\n",
      "epoch: 2 step: 1561, loss is 0.0073861931450665\n",
      "epoch: 2 step: 1562, loss is 0.11634017527103424\n",
      "epoch: 2 step: 1563, loss is 0.023285377770662308\n",
      "epoch: 2 step: 1564, loss is 0.006075321696698666\n",
      "epoch: 2 step: 1565, loss is 0.02343958616256714\n",
      "epoch: 2 step: 1566, loss is 0.027546098455786705\n",
      "epoch: 2 step: 1567, loss is 0.12619559466838837\n",
      "epoch: 2 step: 1568, loss is 0.18068575859069824\n",
      "epoch: 2 step: 1569, loss is 0.01377126108855009\n",
      "epoch: 2 step: 1570, loss is 0.04068639874458313\n",
      "epoch: 2 step: 1571, loss is 0.07546886801719666\n",
      "epoch: 2 step: 1572, loss is 0.0515955314040184\n",
      "epoch: 2 step: 1573, loss is 0.006663368549197912\n",
      "epoch: 2 step: 1574, loss is 0.021579191088676453\n",
      "epoch: 2 step: 1575, loss is 0.03792000934481621\n",
      "epoch: 2 step: 1576, loss is 0.009796926751732826\n",
      "epoch: 2 step: 1577, loss is 0.05802086368203163\n",
      "epoch: 2 step: 1578, loss is 0.00316102197393775\n",
      "epoch: 2 step: 1579, loss is 0.02888403832912445\n",
      "epoch: 2 step: 1580, loss is 0.0028564375825226307\n",
      "epoch: 2 step: 1581, loss is 0.0015018770936876535\n",
      "epoch: 2 step: 1582, loss is 0.07935980707406998\n",
      "epoch: 2 step: 1583, loss is 0.01146840862929821\n",
      "epoch: 2 step: 1584, loss is 0.10050452500581741\n",
      "epoch: 2 step: 1585, loss is 0.013724994845688343\n",
      "epoch: 2 step: 1586, loss is 0.008017295971512794\n",
      "epoch: 2 step: 1587, loss is 0.05382630228996277\n",
      "epoch: 2 step: 1588, loss is 0.0055025857873260975\n",
      "epoch: 2 step: 1589, loss is 0.05958346650004387\n",
      "epoch: 2 step: 1590, loss is 0.00533475074917078\n",
      "epoch: 2 step: 1591, loss is 0.3697642982006073\n",
      "epoch: 2 step: 1592, loss is 0.03228426352143288\n",
      "epoch: 2 step: 1593, loss is 0.21180236339569092\n",
      "epoch: 2 step: 1594, loss is 0.006849130615592003\n",
      "epoch: 2 step: 1595, loss is 0.005934818182140589\n",
      "epoch: 2 step: 1596, loss is 0.12843863666057587\n",
      "epoch: 2 step: 1597, loss is 0.00142333481926471\n",
      "epoch: 2 step: 1598, loss is 0.21042540669441223\n",
      "epoch: 2 step: 1599, loss is 0.13604459166526794\n",
      "epoch: 2 step: 1600, loss is 0.07550812512636185\n",
      "epoch: 2 step: 1601, loss is 0.005112383048981428\n",
      "epoch: 2 step: 1602, loss is 0.014759108424186707\n",
      "epoch: 2 step: 1603, loss is 0.016512703150510788\n",
      "epoch: 2 step: 1604, loss is 0.05616108328104019\n",
      "epoch: 2 step: 1605, loss is 0.0016722080763429403\n",
      "epoch: 2 step: 1606, loss is 0.0015469592763110995\n",
      "epoch: 2 step: 1607, loss is 0.05191298574209213\n",
      "epoch: 2 step: 1608, loss is 0.09566425532102585\n",
      "epoch: 2 step: 1609, loss is 0.0008248640224337578\n",
      "epoch: 2 step: 1610, loss is 0.010155542753636837\n",
      "epoch: 2 step: 1611, loss is 0.027817560359835625\n",
      "epoch: 2 step: 1612, loss is 0.013459795154631138\n",
      "epoch: 2 step: 1613, loss is 0.025918258354067802\n",
      "epoch: 2 step: 1614, loss is 0.024950124323368073\n",
      "epoch: 2 step: 1615, loss is 0.19403210282325745\n",
      "epoch: 2 step: 1616, loss is 0.023887041956186295\n",
      "epoch: 2 step: 1617, loss is 0.01694541797041893\n",
      "epoch: 2 step: 1618, loss is 0.014219283126294613\n",
      "epoch: 2 step: 1619, loss is 0.17046323418617249\n",
      "epoch: 2 step: 1620, loss is 0.07972575724124908\n",
      "epoch: 2 step: 1621, loss is 0.002403496764600277\n",
      "epoch: 2 step: 1622, loss is 0.07586723566055298\n",
      "epoch: 2 step: 1623, loss is 0.0021318707149475813\n",
      "epoch: 2 step: 1624, loss is 0.002742556156590581\n",
      "epoch: 2 step: 1625, loss is 0.005159393418580294\n",
      "epoch: 2 step: 1626, loss is 0.03421967476606369\n",
      "epoch: 2 step: 1627, loss is 0.012174231931567192\n",
      "epoch: 2 step: 1628, loss is 0.0035144705325365067\n",
      "epoch: 2 step: 1629, loss is 0.001400304608978331\n",
      "epoch: 2 step: 1630, loss is 0.01923898048698902\n",
      "epoch: 2 step: 1631, loss is 0.004923746921122074\n",
      "epoch: 2 step: 1632, loss is 0.20012718439102173\n",
      "epoch: 2 step: 1633, loss is 0.08659201115369797\n",
      "epoch: 2 step: 1634, loss is 0.15783849358558655\n",
      "epoch: 2 step: 1635, loss is 0.0037840870209038258\n",
      "epoch: 2 step: 1636, loss is 0.0018655501771718264\n",
      "epoch: 2 step: 1637, loss is 0.11106054484844208\n",
      "epoch: 2 step: 1638, loss is 0.1366359144449234\n",
      "epoch: 2 step: 1639, loss is 0.013248180970549583\n",
      "epoch: 2 step: 1640, loss is 0.013225718401372433\n",
      "epoch: 2 step: 1641, loss is 0.030387820675969124\n",
      "epoch: 2 step: 1642, loss is 0.009394221007823944\n",
      "epoch: 2 step: 1643, loss is 0.05950155109167099\n",
      "epoch: 2 step: 1644, loss is 0.0017730072140693665\n",
      "epoch: 2 step: 1645, loss is 0.19409386813640594\n",
      "epoch: 2 step: 1646, loss is 0.005040808115154505\n",
      "epoch: 2 step: 1647, loss is 0.0012000909773632884\n",
      "epoch: 2 step: 1648, loss is 0.008515608496963978\n",
      "epoch: 2 step: 1649, loss is 0.0009552078554406762\n",
      "epoch: 2 step: 1650, loss is 0.001028576516546309\n",
      "epoch: 2 step: 1651, loss is 0.0341230109333992\n",
      "epoch: 2 step: 1652, loss is 0.0013895458541810513\n",
      "epoch: 2 step: 1653, loss is 0.2766020596027374\n",
      "epoch: 2 step: 1654, loss is 0.07151699811220169\n",
      "epoch: 2 step: 1655, loss is 0.0023296568542718887\n",
      "epoch: 2 step: 1656, loss is 0.0054482510313391685\n",
      "epoch: 2 step: 1657, loss is 0.08832895010709763\n",
      "epoch: 2 step: 1658, loss is 0.002284377347677946\n",
      "epoch: 2 step: 1659, loss is 0.002824500435963273\n",
      "epoch: 2 step: 1660, loss is 0.022655054926872253\n",
      "epoch: 2 step: 1661, loss is 0.003120842156931758\n",
      "epoch: 2 step: 1662, loss is 0.017414448782801628\n",
      "epoch: 2 step: 1663, loss is 0.01262857485562563\n",
      "epoch: 2 step: 1664, loss is 0.10074161738157272\n",
      "epoch: 2 step: 1665, loss is 0.002991962479427457\n",
      "epoch: 2 step: 1666, loss is 0.022374074906110764\n",
      "epoch: 2 step: 1667, loss is 0.027831964194774628\n",
      "epoch: 2 step: 1668, loss is 0.11884766072034836\n",
      "epoch: 2 step: 1669, loss is 0.016271380707621574\n",
      "epoch: 2 step: 1670, loss is 0.03500102460384369\n",
      "epoch: 2 step: 1671, loss is 0.03177384287118912\n",
      "epoch: 2 step: 1672, loss is 0.012558733113110065\n",
      "epoch: 2 step: 1673, loss is 0.21481509506702423\n",
      "epoch: 2 step: 1674, loss is 0.0012006678152829409\n",
      "epoch: 2 step: 1675, loss is 0.03984773904085159\n",
      "epoch: 2 step: 1676, loss is 0.0010000187903642654\n",
      "epoch: 2 step: 1677, loss is 0.013769128359854221\n",
      "epoch: 2 step: 1678, loss is 0.05973447859287262\n",
      "epoch: 2 step: 1679, loss is 0.02359791286289692\n",
      "epoch: 2 step: 1680, loss is 0.0030686804093420506\n",
      "epoch: 2 step: 1681, loss is 0.32026758790016174\n",
      "epoch: 2 step: 1682, loss is 0.009945961646735668\n",
      "epoch: 2 step: 1683, loss is 0.08018963038921356\n",
      "epoch: 2 step: 1684, loss is 0.0007497482583858073\n",
      "epoch: 2 step: 1685, loss is 0.06826969236135483\n",
      "epoch: 2 step: 1686, loss is 0.023421498015522957\n",
      "epoch: 2 step: 1687, loss is 0.05484583228826523\n",
      "epoch: 2 step: 1688, loss is 0.013587204739451408\n",
      "epoch: 2 step: 1689, loss is 0.16960816085338593\n",
      "epoch: 2 step: 1690, loss is 0.017199192196130753\n",
      "epoch: 2 step: 1691, loss is 0.03289845213294029\n",
      "epoch: 2 step: 1692, loss is 0.023643769323825836\n",
      "epoch: 2 step: 1693, loss is 0.15012051165103912\n",
      "epoch: 2 step: 1694, loss is 0.00863330252468586\n",
      "epoch: 2 step: 1695, loss is 0.014600437134504318\n",
      "epoch: 2 step: 1696, loss is 0.00043134501902386546\n",
      "epoch: 2 step: 1697, loss is 0.38217175006866455\n",
      "epoch: 2 step: 1698, loss is 0.025276007130742073\n",
      "epoch: 2 step: 1699, loss is 0.012707434594631195\n",
      "epoch: 2 step: 1700, loss is 0.003491800045594573\n",
      "epoch: 2 step: 1701, loss is 0.0032311519607901573\n",
      "epoch: 2 step: 1702, loss is 0.14687000215053558\n",
      "epoch: 2 step: 1703, loss is 0.26447945833206177\n",
      "epoch: 2 step: 1704, loss is 0.06375380605459213\n",
      "epoch: 2 step: 1705, loss is 0.11025930196046829\n",
      "epoch: 2 step: 1706, loss is 0.006688485853374004\n",
      "epoch: 2 step: 1707, loss is 0.002356678480282426\n",
      "epoch: 2 step: 1708, loss is 0.0017762293573468924\n",
      "epoch: 2 step: 1709, loss is 0.003233435330912471\n",
      "epoch: 2 step: 1710, loss is 0.005201653111726046\n",
      "epoch: 2 step: 1711, loss is 0.12326245754957199\n",
      "epoch: 2 step: 1712, loss is 0.037965327501297\n",
      "epoch: 2 step: 1713, loss is 0.0090292077511549\n",
      "epoch: 2 step: 1714, loss is 0.006986198481172323\n",
      "epoch: 2 step: 1715, loss is 0.027848463505506516\n",
      "epoch: 2 step: 1716, loss is 0.009559453465044498\n",
      "epoch: 2 step: 1717, loss is 0.014632383361458778\n",
      "epoch: 2 step: 1718, loss is 0.0033646365627646446\n",
      "epoch: 2 step: 1719, loss is 0.1499793380498886\n",
      "epoch: 2 step: 1720, loss is 0.017415037378668785\n",
      "epoch: 2 step: 1721, loss is 0.09688597172498703\n",
      "epoch: 2 step: 1722, loss is 0.007957561872899532\n",
      "epoch: 2 step: 1723, loss is 0.10029750317335129\n",
      "epoch: 2 step: 1724, loss is 0.03262745589017868\n",
      "epoch: 2 step: 1725, loss is 0.005095860455185175\n",
      "epoch: 2 step: 1726, loss is 0.061121392995119095\n",
      "epoch: 2 step: 1727, loss is 0.04241849482059479\n",
      "epoch: 2 step: 1728, loss is 0.14425522089004517\n",
      "epoch: 2 step: 1729, loss is 0.007798417937010527\n",
      "epoch: 2 step: 1730, loss is 0.07881350815296173\n",
      "epoch: 2 step: 1731, loss is 0.0045058648101985455\n",
      "epoch: 2 step: 1732, loss is 0.05676732584834099\n",
      "epoch: 2 step: 1733, loss is 0.08423361927270889\n",
      "epoch: 2 step: 1734, loss is 0.008721303194761276\n",
      "epoch: 2 step: 1735, loss is 0.0030656899325549603\n",
      "epoch: 2 step: 1736, loss is 0.05342952162027359\n",
      "epoch: 2 step: 1737, loss is 0.024844158440828323\n",
      "epoch: 2 step: 1738, loss is 0.019378433004021645\n",
      "epoch: 2 step: 1739, loss is 0.011764181777834892\n",
      "epoch: 2 step: 1740, loss is 0.004943142179399729\n",
      "epoch: 2 step: 1741, loss is 0.06527607142925262\n",
      "epoch: 2 step: 1742, loss is 0.040201254189014435\n",
      "epoch: 2 step: 1743, loss is 0.0047787330113351345\n",
      "epoch: 2 step: 1744, loss is 0.01186845451593399\n",
      "epoch: 2 step: 1745, loss is 0.015358532778918743\n",
      "epoch: 2 step: 1746, loss is 0.01818108558654785\n",
      "epoch: 2 step: 1747, loss is 0.023736784234642982\n",
      "epoch: 2 step: 1748, loss is 0.003953629173338413\n",
      "epoch: 2 step: 1749, loss is 0.0017236191779375076\n",
      "epoch: 2 step: 1750, loss is 0.034226611256599426\n",
      "epoch: 2 step: 1751, loss is 0.030927423387765884\n",
      "epoch: 2 step: 1752, loss is 0.004059928003698587\n",
      "epoch: 2 step: 1753, loss is 0.16872674226760864\n",
      "epoch: 2 step: 1754, loss is 0.12462262809276581\n",
      "epoch: 2 step: 1755, loss is 0.005769488401710987\n",
      "epoch: 2 step: 1756, loss is 0.06346910446882248\n",
      "epoch: 2 step: 1757, loss is 0.0019383589969947934\n",
      "epoch: 2 step: 1758, loss is 0.002228028140962124\n",
      "epoch: 2 step: 1759, loss is 0.021205268800258636\n",
      "epoch: 2 step: 1760, loss is 0.017385143786668777\n",
      "epoch: 2 step: 1761, loss is 0.0010692152427509427\n",
      "epoch: 2 step: 1762, loss is 0.07539688050746918\n",
      "epoch: 2 step: 1763, loss is 0.07925670593976974\n",
      "epoch: 2 step: 1764, loss is 0.11282810568809509\n",
      "epoch: 2 step: 1765, loss is 0.011274728924036026\n",
      "epoch: 2 step: 1766, loss is 0.0014521953416988254\n",
      "epoch: 2 step: 1767, loss is 0.0026669842191040516\n",
      "epoch: 2 step: 1768, loss is 0.006285731215029955\n",
      "epoch: 2 step: 1769, loss is 0.0078768040984869\n",
      "epoch: 2 step: 1770, loss is 0.2928989827632904\n",
      "epoch: 2 step: 1771, loss is 0.03403375297784805\n",
      "epoch: 2 step: 1772, loss is 0.007741077803075314\n",
      "epoch: 2 step: 1773, loss is 0.014513576403260231\n",
      "epoch: 2 step: 1774, loss is 0.015371494926512241\n",
      "epoch: 2 step: 1775, loss is 0.004029774572700262\n",
      "epoch: 2 step: 1776, loss is 0.05252770334482193\n",
      "epoch: 2 step: 1777, loss is 0.009664204902946949\n",
      "epoch: 2 step: 1778, loss is 0.03192521631717682\n",
      "epoch: 2 step: 1779, loss is 0.031781941652297974\n",
      "epoch: 2 step: 1780, loss is 0.3928505778312683\n",
      "epoch: 2 step: 1781, loss is 0.012979098595678806\n",
      "epoch: 2 step: 1782, loss is 0.0011751472484320402\n",
      "epoch: 2 step: 1783, loss is 0.008356907404959202\n",
      "epoch: 2 step: 1784, loss is 0.0037071134429425\n",
      "epoch: 2 step: 1785, loss is 0.02826065383851528\n",
      "epoch: 2 step: 1786, loss is 0.018217964097857475\n",
      "epoch: 2 step: 1787, loss is 0.0024607821833342314\n",
      "epoch: 2 step: 1788, loss is 0.004323359578847885\n",
      "epoch: 2 step: 1789, loss is 0.0152964498847723\n",
      "epoch: 2 step: 1790, loss is 0.233760803937912\n",
      "epoch: 2 step: 1791, loss is 0.010156447999179363\n",
      "epoch: 2 step: 1792, loss is 0.054049890488386154\n",
      "epoch: 2 step: 1793, loss is 0.0076461900025606155\n",
      "epoch: 2 step: 1794, loss is 0.004690765403211117\n",
      "epoch: 2 step: 1795, loss is 0.006544496398419142\n",
      "epoch: 2 step: 1796, loss is 0.05363370105624199\n",
      "epoch: 2 step: 1797, loss is 0.004697050433605909\n",
      "epoch: 2 step: 1798, loss is 0.0036261696368455887\n",
      "epoch: 2 step: 1799, loss is 0.0036748021375387907\n",
      "epoch: 2 step: 1800, loss is 0.015841832384467125\n",
      "epoch: 2 step: 1801, loss is 0.0020851450972259045\n",
      "epoch: 2 step: 1802, loss is 0.015278180129826069\n",
      "epoch: 2 step: 1803, loss is 0.007807573303580284\n",
      "epoch: 2 step: 1804, loss is 0.00797682348638773\n",
      "epoch: 2 step: 1805, loss is 0.04408370703458786\n",
      "epoch: 2 step: 1806, loss is 0.09545300900936127\n",
      "epoch: 2 step: 1807, loss is 0.062143415212631226\n",
      "epoch: 2 step: 1808, loss is 0.09602341800928116\n",
      "epoch: 2 step: 1809, loss is 0.3942730724811554\n",
      "epoch: 2 step: 1810, loss is 0.018609922379255295\n",
      "epoch: 2 step: 1811, loss is 0.010461315512657166\n",
      "epoch: 2 step: 1812, loss is 0.025819426402449608\n",
      "epoch: 2 step: 1813, loss is 0.22647114098072052\n",
      "epoch: 2 step: 1814, loss is 0.3113704323768616\n",
      "epoch: 2 step: 1815, loss is 0.041160501539707184\n",
      "epoch: 2 step: 1816, loss is 0.27455249428749084\n",
      "epoch: 2 step: 1817, loss is 0.11271215975284576\n",
      "epoch: 2 step: 1818, loss is 0.0177259910851717\n",
      "epoch: 2 step: 1819, loss is 0.002231225371360779\n",
      "epoch: 2 step: 1820, loss is 0.006538683548569679\n",
      "epoch: 2 step: 1821, loss is 0.07696916908025742\n",
      "epoch: 2 step: 1822, loss is 0.011674895882606506\n",
      "epoch: 2 step: 1823, loss is 0.005471079610288143\n",
      "epoch: 2 step: 1824, loss is 0.011067298240959644\n",
      "epoch: 2 step: 1825, loss is 0.02649202197790146\n",
      "epoch: 2 step: 1826, loss is 0.013568178750574589\n",
      "epoch: 2 step: 1827, loss is 0.08576551079750061\n",
      "epoch: 2 step: 1828, loss is 0.012785801663994789\n",
      "epoch: 2 step: 1829, loss is 0.06718435138463974\n",
      "epoch: 2 step: 1830, loss is 0.03239106014370918\n",
      "epoch: 2 step: 1831, loss is 0.019774489104747772\n",
      "epoch: 2 step: 1832, loss is 0.006543297786265612\n",
      "epoch: 2 step: 1833, loss is 0.015681149438023567\n",
      "epoch: 2 step: 1834, loss is 0.12546643614768982\n",
      "epoch: 2 step: 1835, loss is 0.019839217886328697\n",
      "epoch: 2 step: 1836, loss is 0.028529077768325806\n",
      "epoch: 2 step: 1837, loss is 0.06769172847270966\n",
      "epoch: 2 step: 1838, loss is 0.0018973249243572354\n",
      "epoch: 2 step: 1839, loss is 0.004215282388031483\n",
      "epoch: 2 step: 1840, loss is 0.021665070205926895\n",
      "epoch: 2 step: 1841, loss is 0.0882408395409584\n",
      "epoch: 2 step: 1842, loss is 0.00030084257014095783\n",
      "epoch: 2 step: 1843, loss is 0.09917378425598145\n",
      "epoch: 2 step: 1844, loss is 0.00677482271566987\n",
      "epoch: 2 step: 1845, loss is 0.0009354823268949986\n",
      "epoch: 2 step: 1846, loss is 0.10234813392162323\n",
      "epoch: 2 step: 1847, loss is 0.0014762368518859148\n",
      "epoch: 2 step: 1848, loss is 0.039110228419303894\n",
      "epoch: 2 step: 1849, loss is 0.0035257432609796524\n",
      "epoch: 2 step: 1850, loss is 0.0015128031373023987\n",
      "epoch: 2 step: 1851, loss is 0.010306421667337418\n",
      "epoch: 2 step: 1852, loss is 0.013313579373061657\n",
      "epoch: 2 step: 1853, loss is 0.013320179656147957\n",
      "epoch: 2 step: 1854, loss is 0.0023654589895159006\n",
      "epoch: 2 step: 1855, loss is 0.007233894895762205\n",
      "epoch: 2 step: 1856, loss is 0.0127372732385993\n",
      "epoch: 2 step: 1857, loss is 0.02173004485666752\n",
      "epoch: 2 step: 1858, loss is 0.14158600568771362\n",
      "epoch: 2 step: 1859, loss is 0.03497779741883278\n",
      "epoch: 2 step: 1860, loss is 0.0015559602761641145\n",
      "epoch: 2 step: 1861, loss is 0.0103347422555089\n",
      "epoch: 2 step: 1862, loss is 0.1329645812511444\n",
      "epoch: 2 step: 1863, loss is 0.033390987664461136\n",
      "epoch: 2 step: 1864, loss is 0.3031005561351776\n",
      "epoch: 2 step: 1865, loss is 0.04543868079781532\n",
      "epoch: 2 step: 1866, loss is 0.08278382569551468\n",
      "epoch: 2 step: 1867, loss is 0.03739984706044197\n",
      "epoch: 2 step: 1868, loss is 0.011438035406172276\n",
      "epoch: 2 step: 1869, loss is 0.00112473638728261\n",
      "epoch: 2 step: 1870, loss is 0.005937457084655762\n",
      "epoch: 2 step: 1871, loss is 0.08131913840770721\n",
      "epoch: 2 step: 1872, loss is 0.21574467420578003\n",
      "epoch: 2 step: 1873, loss is 0.027766723185777664\n",
      "epoch: 2 step: 1874, loss is 0.00106087327003479\n",
      "epoch: 2 step: 1875, loss is 0.007588233798742294\n",
      "epoch: 3 step: 1, loss is 0.00020680755551438779\n",
      "epoch: 3 step: 2, loss is 0.00402115797623992\n",
      "epoch: 3 step: 3, loss is 0.1093394011259079\n",
      "epoch: 3 step: 4, loss is 0.009623211808502674\n",
      "epoch: 3 step: 5, loss is 0.006988648325204849\n",
      "epoch: 3 step: 6, loss is 0.0016195651842281222\n",
      "epoch: 3 step: 7, loss is 0.08416935056447983\n",
      "epoch: 3 step: 8, loss is 0.10274907946586609\n",
      "epoch: 3 step: 9, loss is 0.0015944072511047125\n",
      "epoch: 3 step: 10, loss is 0.0362391360104084\n",
      "epoch: 3 step: 11, loss is 0.0012001791037619114\n",
      "epoch: 3 step: 12, loss is 0.11453600227832794\n",
      "epoch: 3 step: 13, loss is 0.001390457502566278\n",
      "epoch: 3 step: 14, loss is 0.0017165341414511204\n",
      "epoch: 3 step: 15, loss is 0.02320544421672821\n",
      "epoch: 3 step: 16, loss is 0.00041067309211939573\n",
      "epoch: 3 step: 17, loss is 0.07024672627449036\n",
      "epoch: 3 step: 18, loss is 0.16755345463752747\n",
      "epoch: 3 step: 19, loss is 0.046940743923187256\n",
      "epoch: 3 step: 20, loss is 0.013559272512793541\n",
      "epoch: 3 step: 21, loss is 0.02268272265791893\n",
      "epoch: 3 step: 22, loss is 0.004949316848069429\n",
      "epoch: 3 step: 23, loss is 0.00043445482151582837\n",
      "epoch: 3 step: 24, loss is 0.1262272596359253\n",
      "epoch: 3 step: 25, loss is 0.2141113579273224\n",
      "epoch: 3 step: 26, loss is 0.039550986140966415\n",
      "epoch: 3 step: 27, loss is 0.014147540554404259\n",
      "epoch: 3 step: 28, loss is 0.00172604585532099\n",
      "epoch: 3 step: 29, loss is 0.03511713817715645\n",
      "epoch: 3 step: 30, loss is 0.01715843565762043\n",
      "epoch: 3 step: 31, loss is 0.03447827696800232\n",
      "epoch: 3 step: 32, loss is 0.0037994671147316694\n",
      "epoch: 3 step: 33, loss is 0.0028076129965484142\n",
      "epoch: 3 step: 34, loss is 0.051667582243680954\n",
      "epoch: 3 step: 35, loss is 0.015932828187942505\n",
      "epoch: 3 step: 36, loss is 0.014370243065059185\n",
      "epoch: 3 step: 37, loss is 0.0013849405804648995\n",
      "epoch: 3 step: 38, loss is 0.0071340203285217285\n",
      "epoch: 3 step: 39, loss is 0.07257378846406937\n",
      "epoch: 3 step: 40, loss is 0.012845184653997421\n",
      "epoch: 3 step: 41, loss is 0.009780763648450375\n",
      "epoch: 3 step: 42, loss is 0.006628362461924553\n",
      "epoch: 3 step: 43, loss is 0.020073790103197098\n",
      "epoch: 3 step: 44, loss is 0.007826033048331738\n",
      "epoch: 3 step: 45, loss is 0.013981351628899574\n",
      "epoch: 3 step: 46, loss is 0.023637423291802406\n",
      "epoch: 3 step: 47, loss is 0.005721193738281727\n",
      "epoch: 3 step: 48, loss is 0.0362754687666893\n",
      "epoch: 3 step: 49, loss is 0.015077618882060051\n",
      "epoch: 3 step: 50, loss is 0.10424008965492249\n",
      "epoch: 3 step: 51, loss is 0.00031792905065231025\n",
      "epoch: 3 step: 52, loss is 0.060184888541698456\n",
      "epoch: 3 step: 53, loss is 0.02024001069366932\n",
      "epoch: 3 step: 54, loss is 0.015860067680478096\n",
      "epoch: 3 step: 55, loss is 0.0009135700529441237\n",
      "epoch: 3 step: 56, loss is 0.07261442393064499\n",
      "epoch: 3 step: 57, loss is 0.05632010102272034\n",
      "epoch: 3 step: 58, loss is 0.017672142013907433\n",
      "epoch: 3 step: 59, loss is 0.1419183611869812\n",
      "epoch: 3 step: 60, loss is 0.007543650455772877\n",
      "epoch: 3 step: 61, loss is 0.07544340938329697\n",
      "epoch: 3 step: 62, loss is 0.0010705835884436965\n",
      "epoch: 3 step: 63, loss is 0.0008361803484149277\n",
      "epoch: 3 step: 64, loss is 0.5620309710502625\n",
      "epoch: 3 step: 65, loss is 0.002993733389303088\n",
      "epoch: 3 step: 66, loss is 0.0003982351627200842\n",
      "epoch: 3 step: 67, loss is 0.2039659023284912\n",
      "epoch: 3 step: 68, loss is 0.0016168169677257538\n",
      "epoch: 3 step: 69, loss is 0.014551520347595215\n",
      "epoch: 3 step: 70, loss is 0.0028429676312953234\n",
      "epoch: 3 step: 71, loss is 0.01676635630428791\n",
      "epoch: 3 step: 72, loss is 0.01277840230613947\n",
      "epoch: 3 step: 73, loss is 0.005294607020914555\n",
      "epoch: 3 step: 74, loss is 0.006833937484771013\n",
      "epoch: 3 step: 75, loss is 0.006393561605364084\n",
      "epoch: 3 step: 76, loss is 0.0037508546374738216\n",
      "epoch: 3 step: 77, loss is 0.14699776470661163\n",
      "epoch: 3 step: 78, loss is 0.007686059921979904\n",
      "epoch: 3 step: 79, loss is 0.008625630289316177\n",
      "epoch: 3 step: 80, loss is 0.004342256113886833\n",
      "epoch: 3 step: 81, loss is 0.004313534125685692\n",
      "epoch: 3 step: 82, loss is 0.006480738520622253\n",
      "epoch: 3 step: 83, loss is 0.09451274573802948\n",
      "epoch: 3 step: 84, loss is 0.004031050018966198\n",
      "epoch: 3 step: 85, loss is 0.0481925792992115\n",
      "epoch: 3 step: 86, loss is 0.0025651154574006796\n",
      "epoch: 3 step: 87, loss is 0.00549051770940423\n",
      "epoch: 3 step: 88, loss is 0.011689683422446251\n",
      "epoch: 3 step: 89, loss is 0.006207836791872978\n",
      "epoch: 3 step: 90, loss is 0.14684830605983734\n",
      "epoch: 3 step: 91, loss is 0.08102351427078247\n",
      "epoch: 3 step: 92, loss is 0.15589655935764313\n",
      "epoch: 3 step: 93, loss is 0.0010733403032645583\n",
      "epoch: 3 step: 94, loss is 0.002356549957767129\n",
      "epoch: 3 step: 95, loss is 0.001598580158315599\n",
      "epoch: 3 step: 96, loss is 0.014439250342547894\n",
      "epoch: 3 step: 97, loss is 0.0018298879731446505\n",
      "epoch: 3 step: 98, loss is 0.057555828243494034\n",
      "epoch: 3 step: 99, loss is 0.006824990268796682\n",
      "epoch: 3 step: 100, loss is 0.06721363216638565\n",
      "epoch: 3 step: 101, loss is 0.32424694299697876\n",
      "epoch: 3 step: 102, loss is 0.04255276918411255\n",
      "epoch: 3 step: 103, loss is 0.022807985544204712\n",
      "epoch: 3 step: 104, loss is 0.09228000044822693\n",
      "epoch: 3 step: 105, loss is 0.013110398314893246\n",
      "epoch: 3 step: 106, loss is 0.014195112511515617\n",
      "epoch: 3 step: 107, loss is 0.015290671959519386\n",
      "epoch: 3 step: 108, loss is 0.0585603304207325\n",
      "epoch: 3 step: 109, loss is 0.027013933286070824\n",
      "epoch: 3 step: 110, loss is 0.012082522734999657\n",
      "epoch: 3 step: 111, loss is 0.006829643156379461\n",
      "epoch: 3 step: 112, loss is 0.002867475850507617\n",
      "epoch: 3 step: 113, loss is 0.0555395744740963\n",
      "epoch: 3 step: 114, loss is 0.021268397569656372\n",
      "epoch: 3 step: 115, loss is 0.013823645189404488\n",
      "epoch: 3 step: 116, loss is 0.00181578251067549\n",
      "epoch: 3 step: 117, loss is 0.025042319670319557\n",
      "epoch: 3 step: 118, loss is 0.0038312776014208794\n",
      "epoch: 3 step: 119, loss is 0.05970388650894165\n",
      "epoch: 3 step: 120, loss is 0.01943255588412285\n",
      "epoch: 3 step: 121, loss is 0.129020556807518\n",
      "epoch: 3 step: 122, loss is 0.005956308450549841\n",
      "epoch: 3 step: 123, loss is 0.01247894112020731\n",
      "epoch: 3 step: 124, loss is 0.026420781388878822\n",
      "epoch: 3 step: 125, loss is 0.0005086281453259289\n",
      "epoch: 3 step: 126, loss is 0.0014998047845438123\n",
      "epoch: 3 step: 127, loss is 0.04679533839225769\n",
      "epoch: 3 step: 128, loss is 0.00018634191656019539\n",
      "epoch: 3 step: 129, loss is 0.009891023859381676\n",
      "epoch: 3 step: 130, loss is 0.030791012570261955\n",
      "epoch: 3 step: 131, loss is 0.02889348566532135\n",
      "epoch: 3 step: 132, loss is 0.006369828712195158\n",
      "epoch: 3 step: 133, loss is 0.06551013141870499\n",
      "epoch: 3 step: 134, loss is 0.12109694629907608\n",
      "epoch: 3 step: 135, loss is 0.015062867663800716\n",
      "epoch: 3 step: 136, loss is 0.0024424586445093155\n",
      "epoch: 3 step: 137, loss is 0.12940362095832825\n",
      "epoch: 3 step: 138, loss is 0.006168322172015905\n",
      "epoch: 3 step: 139, loss is 0.0007390722166746855\n",
      "epoch: 3 step: 140, loss is 0.001834316411986947\n",
      "epoch: 3 step: 141, loss is 0.018718019127845764\n",
      "epoch: 3 step: 142, loss is 0.18203288316726685\n",
      "epoch: 3 step: 143, loss is 0.00745173916220665\n",
      "epoch: 3 step: 144, loss is 0.02091459184885025\n",
      "epoch: 3 step: 145, loss is 0.2019256353378296\n",
      "epoch: 3 step: 146, loss is 0.059200137853622437\n",
      "epoch: 3 step: 147, loss is 0.003454049816355109\n",
      "epoch: 3 step: 148, loss is 0.12791325151920319\n",
      "epoch: 3 step: 149, loss is 0.013954024761915207\n",
      "epoch: 3 step: 150, loss is 0.06323917955160141\n",
      "epoch: 3 step: 151, loss is 0.028043225407600403\n",
      "epoch: 3 step: 152, loss is 0.005577436648309231\n",
      "epoch: 3 step: 153, loss is 0.021668678149580956\n",
      "epoch: 3 step: 154, loss is 0.014050932601094246\n",
      "epoch: 3 step: 155, loss is 0.2647514045238495\n",
      "epoch: 3 step: 156, loss is 0.2838274836540222\n",
      "epoch: 3 step: 157, loss is 0.0026807670947164297\n",
      "epoch: 3 step: 158, loss is 0.0024759317748248577\n",
      "epoch: 3 step: 159, loss is 0.047858066856861115\n",
      "epoch: 3 step: 160, loss is 0.05738778039813042\n",
      "epoch: 3 step: 161, loss is 0.004009236581623554\n",
      "epoch: 3 step: 162, loss is 0.01467527449131012\n",
      "epoch: 3 step: 163, loss is 0.020411601290106773\n",
      "epoch: 3 step: 164, loss is 0.036774083971977234\n",
      "epoch: 3 step: 165, loss is 0.0025189134757965803\n",
      "epoch: 3 step: 166, loss is 0.09250667691230774\n",
      "epoch: 3 step: 167, loss is 0.01523963175714016\n",
      "epoch: 3 step: 168, loss is 0.027219703420996666\n",
      "epoch: 3 step: 169, loss is 0.00655807601287961\n",
      "epoch: 3 step: 170, loss is 0.005369130056351423\n",
      "epoch: 3 step: 171, loss is 0.003167389426380396\n",
      "epoch: 3 step: 172, loss is 0.1614321917295456\n",
      "epoch: 3 step: 173, loss is 0.12996318936347961\n",
      "epoch: 3 step: 174, loss is 0.04045150429010391\n",
      "epoch: 3 step: 175, loss is 0.06729507446289062\n",
      "epoch: 3 step: 176, loss is 0.0016967661213129759\n",
      "epoch: 3 step: 177, loss is 0.007007824257016182\n",
      "epoch: 3 step: 178, loss is 0.001205340144224465\n",
      "epoch: 3 step: 179, loss is 0.2829424738883972\n",
      "epoch: 3 step: 180, loss is 0.0027436779346317053\n",
      "epoch: 3 step: 181, loss is 0.05351805314421654\n",
      "epoch: 3 step: 182, loss is 0.0017038224032148719\n",
      "epoch: 3 step: 183, loss is 0.07626836746931076\n",
      "epoch: 3 step: 184, loss is 0.017734993249177933\n",
      "epoch: 3 step: 185, loss is 0.06453703343868256\n",
      "epoch: 3 step: 186, loss is 0.05197395756840706\n",
      "epoch: 3 step: 187, loss is 0.011384251527488232\n",
      "epoch: 3 step: 188, loss is 0.014099293388426304\n",
      "epoch: 3 step: 189, loss is 0.036694351583719254\n",
      "epoch: 3 step: 190, loss is 0.10016118735074997\n",
      "epoch: 3 step: 191, loss is 0.1364842653274536\n",
      "epoch: 3 step: 192, loss is 0.004130085930228233\n",
      "epoch: 3 step: 193, loss is 0.0021772338077425957\n",
      "epoch: 3 step: 194, loss is 0.003178405575454235\n",
      "epoch: 3 step: 195, loss is 0.021879304200410843\n",
      "epoch: 3 step: 196, loss is 0.009577983058989048\n",
      "epoch: 3 step: 197, loss is 0.06407561898231506\n",
      "epoch: 3 step: 198, loss is 0.010715028271079063\n",
      "epoch: 3 step: 199, loss is 0.0011148278135806322\n",
      "epoch: 3 step: 200, loss is 0.009304272010922432\n",
      "epoch: 3 step: 201, loss is 0.026506319642066956\n",
      "epoch: 3 step: 202, loss is 0.48638972640037537\n",
      "epoch: 3 step: 203, loss is 0.03390522673726082\n",
      "epoch: 3 step: 204, loss is 0.058436110615730286\n",
      "epoch: 3 step: 205, loss is 0.026279190555214882\n",
      "epoch: 3 step: 206, loss is 0.04797603562474251\n",
      "epoch: 3 step: 207, loss is 0.006676000077277422\n",
      "epoch: 3 step: 208, loss is 0.004742722492665052\n",
      "epoch: 3 step: 209, loss is 0.003244799328967929\n",
      "epoch: 3 step: 210, loss is 0.022541576996445656\n",
      "epoch: 3 step: 211, loss is 0.005474959500133991\n",
      "epoch: 3 step: 212, loss is 0.012265118770301342\n",
      "epoch: 3 step: 213, loss is 0.03094787709414959\n",
      "epoch: 3 step: 214, loss is 0.023391909897327423\n",
      "epoch: 3 step: 215, loss is 0.006570504046976566\n",
      "epoch: 3 step: 216, loss is 0.005462343338876963\n",
      "epoch: 3 step: 217, loss is 0.000862934160977602\n",
      "epoch: 3 step: 218, loss is 0.0020023053511977196\n",
      "epoch: 3 step: 219, loss is 0.12395249307155609\n",
      "epoch: 3 step: 220, loss is 0.01110280305147171\n",
      "epoch: 3 step: 221, loss is 0.16696378588676453\n",
      "epoch: 3 step: 222, loss is 0.07911811769008636\n",
      "epoch: 3 step: 223, loss is 0.19159120321273804\n",
      "epoch: 3 step: 224, loss is 0.014453488402068615\n",
      "epoch: 3 step: 225, loss is 0.008525480516254902\n",
      "epoch: 3 step: 226, loss is 0.024113664403557777\n",
      "epoch: 3 step: 227, loss is 0.008525299839675426\n",
      "epoch: 3 step: 228, loss is 0.0029560497496277094\n",
      "epoch: 3 step: 229, loss is 0.011962438002228737\n",
      "epoch: 3 step: 230, loss is 0.07789505273103714\n",
      "epoch: 3 step: 231, loss is 0.01351241022348404\n",
      "epoch: 3 step: 232, loss is 0.0009638522169552743\n",
      "epoch: 3 step: 233, loss is 0.006190227810293436\n",
      "epoch: 3 step: 234, loss is 0.014507406391203403\n",
      "epoch: 3 step: 235, loss is 0.03884446993470192\n",
      "epoch: 3 step: 236, loss is 0.08169713616371155\n",
      "epoch: 3 step: 237, loss is 0.000939729274250567\n",
      "epoch: 3 step: 238, loss is 0.012180241756141186\n",
      "epoch: 3 step: 239, loss is 0.009613883681595325\n",
      "epoch: 3 step: 240, loss is 0.047367602586746216\n",
      "epoch: 3 step: 241, loss is 0.01580229587852955\n",
      "epoch: 3 step: 242, loss is 0.003019338473677635\n",
      "epoch: 3 step: 243, loss is 0.048930030316114426\n",
      "epoch: 3 step: 244, loss is 0.008002489805221558\n",
      "epoch: 3 step: 245, loss is 0.005049217492341995\n",
      "epoch: 3 step: 246, loss is 0.007696596905589104\n",
      "epoch: 3 step: 247, loss is 0.024290578439831734\n",
      "epoch: 3 step: 248, loss is 0.0639096051454544\n",
      "epoch: 3 step: 249, loss is 0.1254400610923767\n",
      "epoch: 3 step: 250, loss is 0.001036039786413312\n",
      "epoch: 3 step: 251, loss is 0.0021201299969106913\n",
      "epoch: 3 step: 252, loss is 0.2725816071033478\n",
      "epoch: 3 step: 253, loss is 0.004173125606030226\n",
      "epoch: 3 step: 254, loss is 0.009094550274312496\n",
      "epoch: 3 step: 255, loss is 0.12129957228899002\n",
      "epoch: 3 step: 256, loss is 0.18595416843891144\n",
      "epoch: 3 step: 257, loss is 0.15068498253822327\n",
      "epoch: 3 step: 258, loss is 0.0011884557316079736\n",
      "epoch: 3 step: 259, loss is 0.029189834371209145\n",
      "epoch: 3 step: 260, loss is 0.03772996366024017\n",
      "epoch: 3 step: 261, loss is 0.004926230758428574\n",
      "epoch: 3 step: 262, loss is 0.002812917111441493\n",
      "epoch: 3 step: 263, loss is 0.04698732867836952\n",
      "epoch: 3 step: 264, loss is 0.009703517891466618\n",
      "epoch: 3 step: 265, loss is 0.22014929354190826\n",
      "epoch: 3 step: 266, loss is 0.02384948544204235\n",
      "epoch: 3 step: 267, loss is 0.003544761100783944\n",
      "epoch: 3 step: 268, loss is 0.021056629717350006\n",
      "epoch: 3 step: 269, loss is 0.032676421105861664\n",
      "epoch: 3 step: 270, loss is 0.031861864030361176\n",
      "epoch: 3 step: 271, loss is 0.006180573254823685\n",
      "epoch: 3 step: 272, loss is 0.003127465955913067\n",
      "epoch: 3 step: 273, loss is 0.027987314388155937\n",
      "epoch: 3 step: 274, loss is 0.001497302670031786\n",
      "epoch: 3 step: 275, loss is 0.0042044115252792835\n",
      "epoch: 3 step: 276, loss is 0.003924037329852581\n",
      "epoch: 3 step: 277, loss is 0.0036073948722332716\n",
      "epoch: 3 step: 278, loss is 0.03314046934247017\n",
      "epoch: 3 step: 279, loss is 0.043546490371227264\n",
      "epoch: 3 step: 280, loss is 0.02225976623594761\n",
      "epoch: 3 step: 281, loss is 0.007276976481080055\n",
      "epoch: 3 step: 282, loss is 0.043705958873033524\n",
      "epoch: 3 step: 283, loss is 0.006968464702367783\n",
      "epoch: 3 step: 284, loss is 0.007819557562470436\n",
      "epoch: 3 step: 285, loss is 0.0018391365883871913\n",
      "epoch: 3 step: 286, loss is 0.18886525928974152\n",
      "epoch: 3 step: 287, loss is 0.0038804099895060062\n",
      "epoch: 3 step: 288, loss is 0.05416315421462059\n",
      "epoch: 3 step: 289, loss is 0.06290068477392197\n",
      "epoch: 3 step: 290, loss is 0.014499911107122898\n",
      "epoch: 3 step: 291, loss is 0.0030021811835467815\n",
      "epoch: 3 step: 292, loss is 0.09247734397649765\n",
      "epoch: 3 step: 293, loss is 0.014800572767853737\n",
      "epoch: 3 step: 294, loss is 0.015581054612994194\n",
      "epoch: 3 step: 295, loss is 0.0018316323403269053\n",
      "epoch: 3 step: 296, loss is 0.0015545922797173262\n",
      "epoch: 3 step: 297, loss is 0.16100698709487915\n",
      "epoch: 3 step: 298, loss is 0.0037597271148115396\n",
      "epoch: 3 step: 299, loss is 0.008306785486638546\n",
      "epoch: 3 step: 300, loss is 0.005293729715049267\n",
      "epoch: 3 step: 301, loss is 0.040212973952293396\n",
      "epoch: 3 step: 302, loss is 0.00857590138912201\n",
      "epoch: 3 step: 303, loss is 0.01862119697034359\n",
      "epoch: 3 step: 304, loss is 0.01582474447786808\n",
      "epoch: 3 step: 305, loss is 0.005994148086756468\n",
      "epoch: 3 step: 306, loss is 0.00669903215020895\n",
      "epoch: 3 step: 307, loss is 0.001430338597856462\n",
      "epoch: 3 step: 308, loss is 0.05075879395008087\n",
      "epoch: 3 step: 309, loss is 0.013335459865629673\n",
      "epoch: 3 step: 310, loss is 0.003666142001748085\n",
      "epoch: 3 step: 311, loss is 0.003024542238563299\n",
      "epoch: 3 step: 312, loss is 0.00378699554130435\n",
      "epoch: 3 step: 313, loss is 0.007645345758646727\n",
      "epoch: 3 step: 314, loss is 0.01789645291864872\n",
      "epoch: 3 step: 315, loss is 0.08951082080602646\n",
      "epoch: 3 step: 316, loss is 0.008958041667938232\n",
      "epoch: 3 step: 317, loss is 0.02998327650129795\n",
      "epoch: 3 step: 318, loss is 0.0033061823341995478\n",
      "epoch: 3 step: 319, loss is 0.0026410426944494247\n",
      "epoch: 3 step: 320, loss is 0.0010851171100512147\n",
      "epoch: 3 step: 321, loss is 0.006384117528796196\n",
      "epoch: 3 step: 322, loss is 0.07542734593153\n",
      "epoch: 3 step: 323, loss is 0.02365049161016941\n",
      "epoch: 3 step: 324, loss is 0.012466100044548512\n",
      "epoch: 3 step: 325, loss is 0.0062845307402312756\n",
      "epoch: 3 step: 326, loss is 0.010896383784711361\n",
      "epoch: 3 step: 327, loss is 0.009002854116261005\n",
      "epoch: 3 step: 328, loss is 0.001714181387796998\n",
      "epoch: 3 step: 329, loss is 0.006455165799707174\n",
      "epoch: 3 step: 330, loss is 0.007309925276786089\n",
      "epoch: 3 step: 331, loss is 0.015320822596549988\n",
      "epoch: 3 step: 332, loss is 0.021280022338032722\n",
      "epoch: 3 step: 333, loss is 0.004979605320841074\n",
      "epoch: 3 step: 334, loss is 0.019096139818429947\n",
      "epoch: 3 step: 335, loss is 0.1922421157360077\n",
      "epoch: 3 step: 336, loss is 0.0019158606883138418\n",
      "epoch: 3 step: 337, loss is 0.009652712382376194\n",
      "epoch: 3 step: 338, loss is 0.003058982314541936\n",
      "epoch: 3 step: 339, loss is 0.0015771602047607303\n",
      "epoch: 3 step: 340, loss is 0.002425956539809704\n",
      "epoch: 3 step: 341, loss is 0.008906539529561996\n",
      "epoch: 3 step: 342, loss is 0.005285803694278002\n",
      "epoch: 3 step: 343, loss is 0.15745218098163605\n",
      "epoch: 3 step: 344, loss is 0.007553459145128727\n",
      "epoch: 3 step: 345, loss is 0.01706480048596859\n",
      "epoch: 3 step: 346, loss is 0.021279845386743546\n",
      "epoch: 3 step: 347, loss is 0.002762551885098219\n",
      "epoch: 3 step: 348, loss is 0.011667571030557156\n",
      "epoch: 3 step: 349, loss is 0.16029725968837738\n",
      "epoch: 3 step: 350, loss is 0.02766430377960205\n",
      "epoch: 3 step: 351, loss is 0.0010221877600997686\n",
      "epoch: 3 step: 352, loss is 0.0012851982610300183\n",
      "epoch: 3 step: 353, loss is 0.015609686262905598\n",
      "epoch: 3 step: 354, loss is 0.00022095345775596797\n",
      "epoch: 3 step: 355, loss is 0.016989056020975113\n",
      "epoch: 3 step: 356, loss is 0.013068901374936104\n",
      "epoch: 3 step: 357, loss is 0.044237468391656876\n",
      "epoch: 3 step: 358, loss is 0.016179613769054413\n",
      "epoch: 3 step: 359, loss is 0.0003471647796686739\n",
      "epoch: 3 step: 360, loss is 0.01831168867647648\n",
      "epoch: 3 step: 361, loss is 0.01652534492313862\n",
      "epoch: 3 step: 362, loss is 0.0030288114212453365\n",
      "epoch: 3 step: 363, loss is 0.02495325170457363\n",
      "epoch: 3 step: 364, loss is 0.02661200612783432\n",
      "epoch: 3 step: 365, loss is 0.022557245567440987\n",
      "epoch: 3 step: 366, loss is 0.029981639236211777\n",
      "epoch: 3 step: 367, loss is 0.015415013767778873\n",
      "epoch: 3 step: 368, loss is 0.21982860565185547\n",
      "epoch: 3 step: 369, loss is 0.002272247103974223\n",
      "epoch: 3 step: 370, loss is 0.005842776503413916\n",
      "epoch: 3 step: 371, loss is 0.030900387093424797\n",
      "epoch: 3 step: 372, loss is 0.0005437822546809912\n",
      "epoch: 3 step: 373, loss is 0.46064192056655884\n",
      "epoch: 3 step: 374, loss is 0.01807391084730625\n",
      "epoch: 3 step: 375, loss is 0.04430658742785454\n",
      "epoch: 3 step: 376, loss is 0.003585393540561199\n",
      "epoch: 3 step: 377, loss is 0.00019183853873983026\n",
      "epoch: 3 step: 378, loss is 0.007162490393966436\n",
      "epoch: 3 step: 379, loss is 0.0781007632613182\n",
      "epoch: 3 step: 380, loss is 0.006835004780441523\n",
      "epoch: 3 step: 381, loss is 0.07815904170274734\n",
      "epoch: 3 step: 382, loss is 0.019574139267206192\n",
      "epoch: 3 step: 383, loss is 0.006310564465820789\n",
      "epoch: 3 step: 384, loss is 0.06125335767865181\n",
      "epoch: 3 step: 385, loss is 0.0020631765946745872\n",
      "epoch: 3 step: 386, loss is 0.0008842512615956366\n",
      "epoch: 3 step: 387, loss is 0.13164985179901123\n",
      "epoch: 3 step: 388, loss is 0.03139648586511612\n",
      "epoch: 3 step: 389, loss is 0.017572633922100067\n",
      "epoch: 3 step: 390, loss is 0.0019419747404754162\n",
      "epoch: 3 step: 391, loss is 0.05747620388865471\n",
      "epoch: 3 step: 392, loss is 0.00291338749229908\n",
      "epoch: 3 step: 393, loss is 0.004912633448839188\n",
      "epoch: 3 step: 394, loss is 0.006786269601434469\n",
      "epoch: 3 step: 395, loss is 0.005755990277975798\n",
      "epoch: 3 step: 396, loss is 0.035290129482746124\n",
      "epoch: 3 step: 397, loss is 0.008995164185762405\n",
      "epoch: 3 step: 398, loss is 0.003239865181967616\n",
      "epoch: 3 step: 399, loss is 0.007861169055104256\n",
      "epoch: 3 step: 400, loss is 0.10411321371793747\n",
      "epoch: 3 step: 401, loss is 0.02942829206585884\n",
      "epoch: 3 step: 402, loss is 0.12818986177444458\n",
      "epoch: 3 step: 403, loss is 0.009504539892077446\n",
      "epoch: 3 step: 404, loss is 0.005524063482880592\n",
      "epoch: 3 step: 405, loss is 0.006163178943097591\n",
      "epoch: 3 step: 406, loss is 0.011381038464605808\n",
      "epoch: 3 step: 407, loss is 0.09969240427017212\n",
      "epoch: 3 step: 408, loss is 0.0005888274172320962\n",
      "epoch: 3 step: 409, loss is 0.06578916311264038\n",
      "epoch: 3 step: 410, loss is 0.005219960585236549\n",
      "epoch: 3 step: 411, loss is 0.09959644824266434\n",
      "epoch: 3 step: 412, loss is 0.01961773820221424\n",
      "epoch: 3 step: 413, loss is 0.021012550219893456\n",
      "epoch: 3 step: 414, loss is 0.008787193335592747\n",
      "epoch: 3 step: 415, loss is 0.0014566117897629738\n",
      "epoch: 3 step: 416, loss is 0.002111656591296196\n",
      "epoch: 3 step: 417, loss is 0.0016169873997569084\n",
      "epoch: 3 step: 418, loss is 0.00032527008443139493\n",
      "epoch: 3 step: 419, loss is 0.061536598950624466\n",
      "epoch: 3 step: 420, loss is 0.04600007086992264\n",
      "epoch: 3 step: 421, loss is 0.06327605992555618\n",
      "epoch: 3 step: 422, loss is 0.00254977960139513\n",
      "epoch: 3 step: 423, loss is 0.28318488597869873\n",
      "epoch: 3 step: 424, loss is 0.022351179271936417\n",
      "epoch: 3 step: 425, loss is 0.13537783920764923\n",
      "epoch: 3 step: 426, loss is 0.011928520165383816\n",
      "epoch: 3 step: 427, loss is 0.038435183465480804\n",
      "epoch: 3 step: 428, loss is 0.008619619533419609\n",
      "epoch: 3 step: 429, loss is 0.006868002936244011\n",
      "epoch: 3 step: 430, loss is 0.22636033594608307\n",
      "epoch: 3 step: 431, loss is 0.04092101380228996\n",
      "epoch: 3 step: 432, loss is 0.04769405722618103\n",
      "epoch: 3 step: 433, loss is 0.019719412550330162\n",
      "epoch: 3 step: 434, loss is 0.0029715467244386673\n",
      "epoch: 3 step: 435, loss is 0.004406187683343887\n",
      "epoch: 3 step: 436, loss is 0.004375981166958809\n",
      "epoch: 3 step: 437, loss is 0.023035652935504913\n",
      "epoch: 3 step: 438, loss is 0.037881702184677124\n",
      "epoch: 3 step: 439, loss is 0.018295958638191223\n",
      "epoch: 3 step: 440, loss is 0.0787193551659584\n",
      "epoch: 3 step: 441, loss is 0.14798520505428314\n",
      "epoch: 3 step: 442, loss is 0.014640863053500652\n",
      "epoch: 3 step: 443, loss is 0.03125971183180809\n",
      "epoch: 3 step: 444, loss is 0.01607861928641796\n",
      "epoch: 3 step: 445, loss is 0.13421964645385742\n",
      "epoch: 3 step: 446, loss is 0.006137915886938572\n",
      "epoch: 3 step: 447, loss is 0.033939946442842484\n",
      "epoch: 3 step: 448, loss is 0.0041380831971764565\n",
      "epoch: 3 step: 449, loss is 0.01133642066270113\n",
      "epoch: 3 step: 450, loss is 0.0029657306149601936\n",
      "epoch: 3 step: 451, loss is 0.2356809377670288\n",
      "epoch: 3 step: 452, loss is 0.011669231578707695\n",
      "epoch: 3 step: 453, loss is 0.06275369226932526\n",
      "epoch: 3 step: 454, loss is 0.0070780133828520775\n",
      "epoch: 3 step: 455, loss is 0.04597816243767738\n",
      "epoch: 3 step: 456, loss is 0.016514351591467857\n",
      "epoch: 3 step: 457, loss is 0.008814694359898567\n",
      "epoch: 3 step: 458, loss is 0.03787185251712799\n",
      "epoch: 3 step: 459, loss is 0.08964387327432632\n",
      "epoch: 3 step: 460, loss is 0.059439148753881454\n",
      "epoch: 3 step: 461, loss is 0.003641220973804593\n",
      "epoch: 3 step: 462, loss is 0.0009472871315665543\n",
      "epoch: 3 step: 463, loss is 0.0635325014591217\n",
      "epoch: 3 step: 464, loss is 0.028434978798031807\n",
      "epoch: 3 step: 465, loss is 0.0020788523834198713\n",
      "epoch: 3 step: 466, loss is 0.0639234110713005\n",
      "epoch: 3 step: 467, loss is 0.05418109893798828\n",
      "epoch: 3 step: 468, loss is 0.004205636214464903\n",
      "epoch: 3 step: 469, loss is 0.0007310934597626328\n",
      "epoch: 3 step: 470, loss is 0.02688727155327797\n",
      "epoch: 3 step: 471, loss is 0.001880584517493844\n",
      "epoch: 3 step: 472, loss is 0.0042121573351323605\n",
      "epoch: 3 step: 473, loss is 0.024857327342033386\n",
      "epoch: 3 step: 474, loss is 0.14575077593326569\n",
      "epoch: 3 step: 475, loss is 0.006123049184679985\n",
      "epoch: 3 step: 476, loss is 0.018430791795253754\n",
      "epoch: 3 step: 477, loss is 0.0841275155544281\n",
      "epoch: 3 step: 478, loss is 0.011668644845485687\n",
      "epoch: 3 step: 479, loss is 0.12983138859272003\n",
      "epoch: 3 step: 480, loss is 0.20865990221500397\n",
      "epoch: 3 step: 481, loss is 0.0020908277947455645\n",
      "epoch: 3 step: 482, loss is 0.020881591364741325\n",
      "epoch: 3 step: 483, loss is 0.1753651350736618\n",
      "epoch: 3 step: 484, loss is 0.031243784353137016\n",
      "epoch: 3 step: 485, loss is 0.015985220670700073\n",
      "epoch: 3 step: 486, loss is 0.010308327153325081\n",
      "epoch: 3 step: 487, loss is 0.07253240048885345\n",
      "epoch: 3 step: 488, loss is 0.01203954964876175\n",
      "epoch: 3 step: 489, loss is 0.025957603007555008\n",
      "epoch: 3 step: 490, loss is 0.18455812335014343\n",
      "epoch: 3 step: 491, loss is 0.0036501637659966946\n",
      "epoch: 3 step: 492, loss is 0.002304593799635768\n",
      "epoch: 3 step: 493, loss is 0.023421378806233406\n",
      "epoch: 3 step: 494, loss is 0.0297645702958107\n",
      "epoch: 3 step: 495, loss is 0.010617492720484734\n",
      "epoch: 3 step: 496, loss is 0.0724572017788887\n",
      "epoch: 3 step: 497, loss is 0.01319369301199913\n",
      "epoch: 3 step: 498, loss is 0.002648059045895934\n",
      "epoch: 3 step: 499, loss is 0.001736654550768435\n",
      "epoch: 3 step: 500, loss is 0.006403694860637188\n",
      "epoch: 3 step: 501, loss is 0.004307138733565807\n",
      "epoch: 3 step: 502, loss is 0.03502283990383148\n",
      "epoch: 3 step: 503, loss is 0.002838510787114501\n",
      "epoch: 3 step: 504, loss is 0.006254193838685751\n",
      "epoch: 3 step: 505, loss is 0.07787474244832993\n",
      "epoch: 3 step: 506, loss is 0.1209133192896843\n",
      "epoch: 3 step: 507, loss is 0.009625748731195927\n",
      "epoch: 3 step: 508, loss is 0.01967126876115799\n",
      "epoch: 3 step: 509, loss is 0.05560920760035515\n",
      "epoch: 3 step: 510, loss is 0.013895075768232346\n",
      "epoch: 3 step: 511, loss is 0.06212729588150978\n",
      "epoch: 3 step: 512, loss is 0.001306109712459147\n",
      "epoch: 3 step: 513, loss is 0.07581496238708496\n",
      "epoch: 3 step: 514, loss is 0.032910872250795364\n",
      "epoch: 3 step: 515, loss is 0.002976094139739871\n",
      "epoch: 3 step: 516, loss is 0.02129581943154335\n",
      "epoch: 3 step: 517, loss is 0.0034702869597822428\n",
      "epoch: 3 step: 518, loss is 0.04900221899151802\n",
      "epoch: 3 step: 519, loss is 0.0032814510632306337\n",
      "epoch: 3 step: 520, loss is 0.01710396632552147\n",
      "epoch: 3 step: 521, loss is 0.006224320735782385\n",
      "epoch: 3 step: 522, loss is 0.003354606917127967\n",
      "epoch: 3 step: 523, loss is 0.0017778066685423255\n",
      "epoch: 3 step: 524, loss is 0.005575455259531736\n",
      "epoch: 3 step: 525, loss is 0.0038542451802641153\n",
      "epoch: 3 step: 526, loss is 0.023520907387137413\n",
      "epoch: 3 step: 527, loss is 0.006426797714084387\n",
      "epoch: 3 step: 528, loss is 0.01889772154390812\n",
      "epoch: 3 step: 529, loss is 0.11877456307411194\n",
      "epoch: 3 step: 530, loss is 0.2979445457458496\n",
      "epoch: 3 step: 531, loss is 0.04391014203429222\n",
      "epoch: 3 step: 532, loss is 0.14673112332820892\n",
      "epoch: 3 step: 533, loss is 0.0362674780189991\n",
      "epoch: 3 step: 534, loss is 0.0011567915789783\n",
      "epoch: 3 step: 535, loss is 0.04731791839003563\n",
      "epoch: 3 step: 536, loss is 0.13060857355594635\n",
      "epoch: 3 step: 537, loss is 0.00705384137108922\n",
      "epoch: 3 step: 538, loss is 0.0030353772453963757\n",
      "epoch: 3 step: 539, loss is 0.013497034087777138\n",
      "epoch: 3 step: 540, loss is 0.16698630154132843\n",
      "epoch: 3 step: 541, loss is 0.002470653736963868\n",
      "epoch: 3 step: 542, loss is 0.0019989190623164177\n",
      "epoch: 3 step: 543, loss is 0.0005999721470288932\n",
      "epoch: 3 step: 544, loss is 0.000286607799353078\n",
      "epoch: 3 step: 545, loss is 0.002181325340643525\n",
      "epoch: 3 step: 546, loss is 0.001987508498132229\n",
      "epoch: 3 step: 547, loss is 0.1413099318742752\n",
      "epoch: 3 step: 548, loss is 0.03673383966088295\n",
      "epoch: 3 step: 549, loss is 0.023328591138124466\n",
      "epoch: 3 step: 550, loss is 0.0032425648532807827\n",
      "epoch: 3 step: 551, loss is 0.23876608908176422\n",
      "epoch: 3 step: 552, loss is 0.06338340789079666\n",
      "epoch: 3 step: 553, loss is 0.05912359431385994\n",
      "epoch: 3 step: 554, loss is 0.013647804968059063\n",
      "epoch: 3 step: 555, loss is 0.007440166547894478\n",
      "epoch: 3 step: 556, loss is 0.17473188042640686\n",
      "epoch: 3 step: 557, loss is 0.0014236664865165949\n",
      "epoch: 3 step: 558, loss is 0.0011359562631696463\n",
      "epoch: 3 step: 559, loss is 0.016872501000761986\n",
      "epoch: 3 step: 560, loss is 0.0019897245801985264\n",
      "epoch: 3 step: 561, loss is 0.20074491202831268\n",
      "epoch: 3 step: 562, loss is 0.006253889761865139\n",
      "epoch: 3 step: 563, loss is 0.004625567235052586\n",
      "epoch: 3 step: 564, loss is 0.01646917499601841\n",
      "epoch: 3 step: 565, loss is 0.09771028906106949\n",
      "epoch: 3 step: 566, loss is 0.013035379350185394\n",
      "epoch: 3 step: 567, loss is 0.0015001962892711163\n",
      "epoch: 3 step: 568, loss is 0.048954833298921585\n",
      "epoch: 3 step: 569, loss is 0.022555600851774216\n",
      "epoch: 3 step: 570, loss is 0.006378575228154659\n",
      "epoch: 3 step: 571, loss is 0.0812264159321785\n",
      "epoch: 3 step: 572, loss is 0.07006823271512985\n",
      "epoch: 3 step: 573, loss is 0.41784849762916565\n",
      "epoch: 3 step: 574, loss is 0.038255419582128525\n",
      "epoch: 3 step: 575, loss is 0.017176754772663116\n",
      "epoch: 3 step: 576, loss is 0.03619472309947014\n",
      "epoch: 3 step: 577, loss is 0.02795596979558468\n",
      "epoch: 3 step: 578, loss is 0.0021905682515352964\n",
      "epoch: 3 step: 579, loss is 0.14907121658325195\n",
      "epoch: 3 step: 580, loss is 0.01199495978653431\n",
      "epoch: 3 step: 581, loss is 0.0013051980640739202\n",
      "epoch: 3 step: 582, loss is 0.07418771833181381\n",
      "epoch: 3 step: 583, loss is 0.016996309161186218\n",
      "epoch: 3 step: 584, loss is 0.06493739783763885\n",
      "epoch: 3 step: 585, loss is 0.04847343638539314\n",
      "epoch: 3 step: 586, loss is 0.024998506531119347\n",
      "epoch: 3 step: 587, loss is 0.05146494135260582\n",
      "epoch: 3 step: 588, loss is 0.0641852393746376\n",
      "epoch: 3 step: 589, loss is 0.04009166359901428\n",
      "epoch: 3 step: 590, loss is 0.062481001019477844\n",
      "epoch: 3 step: 591, loss is 0.02178550697863102\n",
      "epoch: 3 step: 592, loss is 0.007107922341674566\n",
      "epoch: 3 step: 593, loss is 0.002549337688833475\n",
      "epoch: 3 step: 594, loss is 0.06203801557421684\n",
      "epoch: 3 step: 595, loss is 0.026028791442513466\n",
      "epoch: 3 step: 596, loss is 0.03575294837355614\n",
      "epoch: 3 step: 597, loss is 0.31021565198898315\n",
      "epoch: 3 step: 598, loss is 0.02278248965740204\n",
      "epoch: 3 step: 599, loss is 0.02256368286907673\n",
      "epoch: 3 step: 600, loss is 0.00972919724881649\n",
      "epoch: 3 step: 601, loss is 0.04567132517695427\n",
      "epoch: 3 step: 602, loss is 0.23140189051628113\n",
      "epoch: 3 step: 603, loss is 0.053710173815488815\n",
      "epoch: 3 step: 604, loss is 0.1841195821762085\n",
      "epoch: 3 step: 605, loss is 0.0333409309387207\n",
      "epoch: 3 step: 606, loss is 0.025091521441936493\n",
      "epoch: 3 step: 607, loss is 0.020374232903122902\n",
      "epoch: 3 step: 608, loss is 0.022239787504076958\n",
      "epoch: 3 step: 609, loss is 0.003915153909474611\n",
      "epoch: 3 step: 610, loss is 0.1342076063156128\n",
      "epoch: 3 step: 611, loss is 0.06231541559100151\n",
      "epoch: 3 step: 612, loss is 0.0033315520267933607\n",
      "epoch: 3 step: 613, loss is 0.13726352155208588\n",
      "epoch: 3 step: 614, loss is 0.008487720042467117\n",
      "epoch: 3 step: 615, loss is 0.007547248620539904\n",
      "epoch: 3 step: 616, loss is 0.19300326704978943\n",
      "epoch: 3 step: 617, loss is 0.09617601335048676\n",
      "epoch: 3 step: 618, loss is 0.08069232106208801\n",
      "epoch: 3 step: 619, loss is 0.02404254861176014\n",
      "epoch: 3 step: 620, loss is 0.08655890077352524\n",
      "epoch: 3 step: 621, loss is 0.044898465275764465\n",
      "epoch: 3 step: 622, loss is 0.005904736462980509\n",
      "epoch: 3 step: 623, loss is 0.13719429075717926\n",
      "epoch: 3 step: 624, loss is 0.1956554651260376\n",
      "epoch: 3 step: 625, loss is 0.007706713862717152\n",
      "epoch: 3 step: 626, loss is 0.051972150802612305\n",
      "epoch: 3 step: 627, loss is 0.0027462225407361984\n",
      "epoch: 3 step: 628, loss is 0.012122346088290215\n",
      "epoch: 3 step: 629, loss is 0.003244570456445217\n",
      "epoch: 3 step: 630, loss is 0.0021094561088830233\n",
      "epoch: 3 step: 631, loss is 0.010276516899466515\n",
      "epoch: 3 step: 632, loss is 0.052227992564439774\n",
      "epoch: 3 step: 633, loss is 0.10307376086711884\n",
      "epoch: 3 step: 634, loss is 0.014163724146783352\n",
      "epoch: 3 step: 635, loss is 0.052821848541498184\n",
      "epoch: 3 step: 636, loss is 0.020006010308861732\n",
      "epoch: 3 step: 637, loss is 0.009386262856423855\n",
      "epoch: 3 step: 638, loss is 0.02437388338148594\n",
      "epoch: 3 step: 639, loss is 0.07614479213953018\n",
      "epoch: 3 step: 640, loss is 0.049882229417562485\n",
      "epoch: 3 step: 641, loss is 0.23264583945274353\n",
      "epoch: 3 step: 642, loss is 0.010310353711247444\n",
      "epoch: 3 step: 643, loss is 0.07325152307748795\n",
      "epoch: 3 step: 644, loss is 0.005868293344974518\n",
      "epoch: 3 step: 645, loss is 0.003859246615320444\n",
      "epoch: 3 step: 646, loss is 0.07921722531318665\n",
      "epoch: 3 step: 647, loss is 0.016430536285042763\n",
      "epoch: 3 step: 648, loss is 0.0008775733876973391\n",
      "epoch: 3 step: 649, loss is 0.1306011974811554\n",
      "epoch: 3 step: 650, loss is 0.05141712725162506\n",
      "epoch: 3 step: 651, loss is 0.03854282200336456\n",
      "epoch: 3 step: 652, loss is 0.0046304273419082165\n",
      "epoch: 3 step: 653, loss is 0.0029617557302117348\n",
      "epoch: 3 step: 654, loss is 0.0019481675699353218\n",
      "epoch: 3 step: 655, loss is 0.0018261974910274148\n",
      "epoch: 3 step: 656, loss is 0.021784162148833275\n",
      "epoch: 3 step: 657, loss is 0.0045318808406591415\n",
      "epoch: 3 step: 658, loss is 0.00561158824712038\n",
      "epoch: 3 step: 659, loss is 0.017180301249027252\n",
      "epoch: 3 step: 660, loss is 0.18262965977191925\n",
      "epoch: 3 step: 661, loss is 0.07946059852838516\n",
      "epoch: 3 step: 662, loss is 0.014118455350399017\n",
      "epoch: 3 step: 663, loss is 0.04253669083118439\n",
      "epoch: 3 step: 664, loss is 0.0011318920878693461\n",
      "epoch: 3 step: 665, loss is 0.006496559362858534\n",
      "epoch: 3 step: 666, loss is 0.015987884253263474\n",
      "epoch: 3 step: 667, loss is 0.20279665291309357\n",
      "epoch: 3 step: 668, loss is 0.005686463322490454\n",
      "epoch: 3 step: 669, loss is 0.005324700381606817\n",
      "epoch: 3 step: 670, loss is 0.0896177738904953\n",
      "epoch: 3 step: 671, loss is 0.05997677147388458\n",
      "epoch: 3 step: 672, loss is 0.07299084216356277\n",
      "epoch: 3 step: 673, loss is 0.02423468977212906\n",
      "epoch: 3 step: 674, loss is 0.006763733923435211\n",
      "epoch: 3 step: 675, loss is 0.0028375883121043444\n",
      "epoch: 3 step: 676, loss is 0.00018354100757278502\n",
      "epoch: 3 step: 677, loss is 0.06646838039159775\n",
      "epoch: 3 step: 678, loss is 0.19040824472904205\n",
      "epoch: 3 step: 679, loss is 0.07099394500255585\n",
      "epoch: 3 step: 680, loss is 0.28592607378959656\n",
      "epoch: 3 step: 681, loss is 0.020984074100852013\n",
      "epoch: 3 step: 682, loss is 0.0011731783160939813\n",
      "epoch: 3 step: 683, loss is 0.003480614395812154\n",
      "epoch: 3 step: 684, loss is 0.08793417364358902\n",
      "epoch: 3 step: 685, loss is 0.017855465412139893\n",
      "epoch: 3 step: 686, loss is 0.015054814517498016\n",
      "epoch: 3 step: 687, loss is 0.10083272308111191\n",
      "epoch: 3 step: 688, loss is 0.1859908103942871\n",
      "epoch: 3 step: 689, loss is 0.002090525347739458\n",
      "epoch: 3 step: 690, loss is 0.03470291569828987\n",
      "epoch: 3 step: 691, loss is 0.0031171035952866077\n",
      "epoch: 3 step: 692, loss is 0.0211512241512537\n",
      "epoch: 3 step: 693, loss is 0.047362156212329865\n",
      "epoch: 3 step: 694, loss is 0.03098391927778721\n",
      "epoch: 3 step: 695, loss is 0.12962059676647186\n",
      "epoch: 3 step: 696, loss is 0.136158749461174\n",
      "epoch: 3 step: 697, loss is 0.012611893005669117\n",
      "epoch: 3 step: 698, loss is 0.023280687630176544\n",
      "epoch: 3 step: 699, loss is 0.0024446856696158648\n",
      "epoch: 3 step: 700, loss is 0.0005876855575479567\n",
      "epoch: 3 step: 701, loss is 0.00047779257874935865\n",
      "epoch: 3 step: 702, loss is 0.0031429424416273832\n",
      "epoch: 3 step: 703, loss is 0.053707223385572433\n",
      "epoch: 3 step: 704, loss is 0.05758354440331459\n",
      "epoch: 3 step: 705, loss is 0.554620623588562\n",
      "epoch: 3 step: 706, loss is 0.003271206747740507\n",
      "epoch: 3 step: 707, loss is 0.0005103365401737392\n",
      "epoch: 3 step: 708, loss is 0.018908141180872917\n",
      "epoch: 3 step: 709, loss is 0.006613046396523714\n",
      "epoch: 3 step: 710, loss is 0.003974660765379667\n",
      "epoch: 3 step: 711, loss is 0.0030752429738640785\n",
      "epoch: 3 step: 712, loss is 0.0005452545010484755\n",
      "epoch: 3 step: 713, loss is 0.06684061139822006\n",
      "epoch: 3 step: 714, loss is 0.07411292940378189\n",
      "epoch: 3 step: 715, loss is 0.013416042551398277\n",
      "epoch: 3 step: 716, loss is 0.011758189648389816\n",
      "epoch: 3 step: 717, loss is 0.0036213917192071676\n",
      "epoch: 3 step: 718, loss is 0.11747045814990997\n",
      "epoch: 3 step: 719, loss is 0.031265366822481155\n",
      "epoch: 3 step: 720, loss is 0.0008758762269280851\n",
      "epoch: 3 step: 721, loss is 0.04207538813352585\n",
      "epoch: 3 step: 722, loss is 0.0017348391702398658\n",
      "epoch: 3 step: 723, loss is 0.005918280221521854\n",
      "epoch: 3 step: 724, loss is 0.40433257818222046\n",
      "epoch: 3 step: 725, loss is 0.004103315528482199\n",
      "epoch: 3 step: 726, loss is 0.0202854685485363\n",
      "epoch: 3 step: 727, loss is 0.02726060338318348\n",
      "epoch: 3 step: 728, loss is 0.038501668721437454\n",
      "epoch: 3 step: 729, loss is 0.004239711444824934\n",
      "epoch: 3 step: 730, loss is 0.0012607144890353084\n",
      "epoch: 3 step: 731, loss is 0.011169817298650742\n",
      "epoch: 3 step: 732, loss is 0.058594755828380585\n",
      "epoch: 3 step: 733, loss is 0.0017004016553983092\n",
      "epoch: 3 step: 734, loss is 0.05237578973174095\n",
      "epoch: 3 step: 735, loss is 0.056069210171699524\n",
      "epoch: 3 step: 736, loss is 0.005904153920710087\n",
      "epoch: 3 step: 737, loss is 0.004138750024139881\n",
      "epoch: 3 step: 738, loss is 0.05020759254693985\n",
      "epoch: 3 step: 739, loss is 0.06212722882628441\n",
      "epoch: 3 step: 740, loss is 0.004333266988396645\n",
      "epoch: 3 step: 741, loss is 0.010098381899297237\n",
      "epoch: 3 step: 742, loss is 0.05154460668563843\n",
      "epoch: 3 step: 743, loss is 0.060523901134729385\n",
      "epoch: 3 step: 744, loss is 0.013513702899217606\n",
      "epoch: 3 step: 745, loss is 0.0030205429065972567\n",
      "epoch: 3 step: 746, loss is 0.05971092730760574\n",
      "epoch: 3 step: 747, loss is 0.0698176920413971\n",
      "epoch: 3 step: 748, loss is 0.008040418848395348\n",
      "epoch: 3 step: 749, loss is 0.11335833370685577\n",
      "epoch: 3 step: 750, loss is 0.005820535123348236\n",
      "epoch: 3 step: 751, loss is 0.03930725157260895\n",
      "epoch: 3 step: 752, loss is 0.007801910862326622\n",
      "epoch: 3 step: 753, loss is 0.39123547077178955\n",
      "epoch: 3 step: 754, loss is 0.12161717563867569\n",
      "epoch: 3 step: 755, loss is 0.032180305570364\n",
      "epoch: 3 step: 756, loss is 0.0059273624792695045\n",
      "epoch: 3 step: 757, loss is 0.005919360090047121\n",
      "epoch: 3 step: 758, loss is 0.06564071774482727\n",
      "epoch: 3 step: 759, loss is 0.03562844172120094\n",
      "epoch: 3 step: 760, loss is 0.0018750804010778666\n",
      "epoch: 3 step: 761, loss is 0.1718028038740158\n",
      "epoch: 3 step: 762, loss is 0.0012184067163616419\n",
      "epoch: 3 step: 763, loss is 0.0424354150891304\n",
      "epoch: 3 step: 764, loss is 0.00486400444060564\n",
      "epoch: 3 step: 765, loss is 0.006034182384610176\n",
      "epoch: 3 step: 766, loss is 0.006215212866663933\n",
      "epoch: 3 step: 767, loss is 0.002841276815161109\n",
      "epoch: 3 step: 768, loss is 0.002587235765531659\n",
      "epoch: 3 step: 769, loss is 0.004676754120737314\n",
      "epoch: 3 step: 770, loss is 0.05779511481523514\n",
      "epoch: 3 step: 771, loss is 0.009381039999425411\n",
      "epoch: 3 step: 772, loss is 0.142153799533844\n",
      "epoch: 3 step: 773, loss is 0.06671684980392456\n",
      "epoch: 3 step: 774, loss is 0.11770065128803253\n",
      "epoch: 3 step: 775, loss is 0.14717985689640045\n",
      "epoch: 3 step: 776, loss is 0.010441670194268227\n",
      "epoch: 3 step: 777, loss is 0.017563097178936005\n",
      "epoch: 3 step: 778, loss is 0.006090924609452486\n",
      "epoch: 3 step: 779, loss is 0.0020899567753076553\n",
      "epoch: 3 step: 780, loss is 0.11607188731431961\n",
      "epoch: 3 step: 781, loss is 0.038731738924980164\n",
      "epoch: 3 step: 782, loss is 0.04473787173628807\n",
      "epoch: 3 step: 783, loss is 0.030210420489311218\n",
      "epoch: 3 step: 784, loss is 0.133597731590271\n",
      "epoch: 3 step: 785, loss is 0.003699998138472438\n",
      "epoch: 3 step: 786, loss is 0.11117559671401978\n",
      "epoch: 3 step: 787, loss is 0.012761273421347141\n",
      "epoch: 3 step: 788, loss is 0.04099546745419502\n",
      "epoch: 3 step: 789, loss is 0.005243674851953983\n",
      "epoch: 3 step: 790, loss is 0.0021066272165626287\n",
      "epoch: 3 step: 791, loss is 0.010348469950258732\n",
      "epoch: 3 step: 792, loss is 0.0066341497004032135\n",
      "epoch: 3 step: 793, loss is 0.0024284052196890116\n",
      "epoch: 3 step: 794, loss is 0.004343820735812187\n",
      "epoch: 3 step: 795, loss is 0.019860006868839264\n",
      "epoch: 3 step: 796, loss is 0.013525232672691345\n",
      "epoch: 3 step: 797, loss is 0.005513003561645746\n",
      "epoch: 3 step: 798, loss is 0.004872153513133526\n",
      "epoch: 3 step: 799, loss is 0.0373699814081192\n",
      "epoch: 3 step: 800, loss is 0.017096566036343575\n",
      "epoch: 3 step: 801, loss is 0.04410431161522865\n",
      "epoch: 3 step: 802, loss is 0.02541620284318924\n",
      "epoch: 3 step: 803, loss is 0.1683768481016159\n",
      "epoch: 3 step: 804, loss is 0.026112765073776245\n",
      "epoch: 3 step: 805, loss is 0.1136644259095192\n",
      "epoch: 3 step: 806, loss is 0.023541908711194992\n",
      "epoch: 3 step: 807, loss is 0.05327644199132919\n",
      "epoch: 3 step: 808, loss is 0.00013567377754952759\n",
      "epoch: 3 step: 809, loss is 0.019334111362695694\n",
      "epoch: 3 step: 810, loss is 0.0012746598804369569\n",
      "epoch: 3 step: 811, loss is 0.05425303801894188\n",
      "epoch: 3 step: 812, loss is 0.005234729032963514\n",
      "epoch: 3 step: 813, loss is 0.014053933322429657\n",
      "epoch: 3 step: 814, loss is 0.000618255406152457\n",
      "epoch: 3 step: 815, loss is 0.025111829861998558\n",
      "epoch: 3 step: 816, loss is 0.002714856993407011\n",
      "epoch: 3 step: 817, loss is 0.0029204885940998793\n",
      "epoch: 3 step: 818, loss is 0.001135872327722609\n",
      "epoch: 3 step: 819, loss is 0.006168413907289505\n",
      "epoch: 3 step: 820, loss is 0.001973363570868969\n",
      "epoch: 3 step: 821, loss is 0.11885149031877518\n",
      "epoch: 3 step: 822, loss is 0.13207995891571045\n",
      "epoch: 3 step: 823, loss is 0.14484450221061707\n",
      "epoch: 3 step: 824, loss is 0.003756387624889612\n",
      "epoch: 3 step: 825, loss is 0.0020684266928583384\n",
      "epoch: 3 step: 826, loss is 0.003399295499548316\n",
      "epoch: 3 step: 827, loss is 0.10039184242486954\n",
      "epoch: 3 step: 828, loss is 0.005020728800445795\n",
      "epoch: 3 step: 829, loss is 0.007894094102084637\n",
      "epoch: 3 step: 830, loss is 0.02430284582078457\n",
      "epoch: 3 step: 831, loss is 0.023711957037448883\n",
      "epoch: 3 step: 832, loss is 0.2823895812034607\n",
      "epoch: 3 step: 833, loss is 0.03847885504364967\n",
      "epoch: 3 step: 834, loss is 0.04023648053407669\n",
      "epoch: 3 step: 835, loss is 0.002313275821506977\n",
      "epoch: 3 step: 836, loss is 0.12076600641012192\n",
      "epoch: 3 step: 837, loss is 0.003374542109668255\n",
      "epoch: 3 step: 838, loss is 0.0029552257619798183\n",
      "epoch: 3 step: 839, loss is 0.03465692698955536\n",
      "epoch: 3 step: 840, loss is 0.09848228096961975\n",
      "epoch: 3 step: 841, loss is 0.011413265019655228\n",
      "epoch: 3 step: 842, loss is 0.006982609163969755\n",
      "epoch: 3 step: 843, loss is 0.030603593215346336\n",
      "epoch: 3 step: 844, loss is 0.004378796089440584\n",
      "epoch: 3 step: 845, loss is 0.022747520357370377\n",
      "epoch: 3 step: 846, loss is 0.03742973506450653\n",
      "epoch: 3 step: 847, loss is 0.1190231591463089\n",
      "epoch: 3 step: 848, loss is 0.004568776581436396\n",
      "epoch: 3 step: 849, loss is 0.034747857600450516\n",
      "epoch: 3 step: 850, loss is 0.0056260391138494015\n",
      "epoch: 3 step: 851, loss is 0.03379278630018234\n",
      "epoch: 3 step: 852, loss is 0.028219768777489662\n",
      "epoch: 3 step: 853, loss is 0.06791951507329941\n",
      "epoch: 3 step: 854, loss is 0.021259332075715065\n",
      "epoch: 3 step: 855, loss is 0.060811497271060944\n",
      "epoch: 3 step: 856, loss is 0.05993993952870369\n",
      "epoch: 3 step: 857, loss is 0.10488159954547882\n",
      "epoch: 3 step: 858, loss is 0.0019889872055500746\n",
      "epoch: 3 step: 859, loss is 0.09371904283761978\n",
      "epoch: 3 step: 860, loss is 0.015820994973182678\n",
      "epoch: 3 step: 861, loss is 0.0014649239601567388\n",
      "epoch: 3 step: 862, loss is 0.002040131250396371\n",
      "epoch: 3 step: 863, loss is 0.16426938772201538\n",
      "epoch: 3 step: 864, loss is 0.0060551101341843605\n",
      "epoch: 3 step: 865, loss is 0.019496187567710876\n",
      "epoch: 3 step: 866, loss is 0.1039411872625351\n",
      "epoch: 3 step: 867, loss is 0.2681858539581299\n",
      "epoch: 3 step: 868, loss is 0.001730145770125091\n",
      "epoch: 3 step: 869, loss is 0.02535502426326275\n",
      "epoch: 3 step: 870, loss is 0.0022984296083450317\n",
      "epoch: 3 step: 871, loss is 0.004289757926017046\n",
      "epoch: 3 step: 872, loss is 0.01783173158764839\n",
      "epoch: 3 step: 873, loss is 0.000985921360552311\n",
      "epoch: 3 step: 874, loss is 0.034359630197286606\n",
      "epoch: 3 step: 875, loss is 0.11250554770231247\n",
      "epoch: 3 step: 876, loss is 0.12242266535758972\n",
      "epoch: 3 step: 877, loss is 0.00479473453015089\n",
      "epoch: 3 step: 878, loss is 0.019119564443826675\n",
      "epoch: 3 step: 879, loss is 0.04037657380104065\n",
      "epoch: 3 step: 880, loss is 0.014368868432939053\n",
      "epoch: 3 step: 881, loss is 0.05601705610752106\n",
      "epoch: 3 step: 882, loss is 0.038884878158569336\n",
      "epoch: 3 step: 883, loss is 0.04218500852584839\n",
      "epoch: 3 step: 884, loss is 0.00556460116058588\n",
      "epoch: 3 step: 885, loss is 0.0012378266546875238\n",
      "epoch: 3 step: 886, loss is 0.0005028563318774104\n",
      "epoch: 3 step: 887, loss is 0.004696384537965059\n",
      "epoch: 3 step: 888, loss is 0.032536569982767105\n",
      "epoch: 3 step: 889, loss is 0.004365892149507999\n",
      "epoch: 3 step: 890, loss is 0.005071493797004223\n",
      "epoch: 3 step: 891, loss is 0.1274309903383255\n",
      "epoch: 3 step: 892, loss is 0.1073225736618042\n",
      "epoch: 3 step: 893, loss is 0.0193952564150095\n",
      "epoch: 3 step: 894, loss is 0.15859106183052063\n",
      "epoch: 3 step: 895, loss is 0.005428139120340347\n",
      "epoch: 3 step: 896, loss is 0.029059773311018944\n",
      "epoch: 3 step: 897, loss is 0.26532694697380066\n",
      "epoch: 3 step: 898, loss is 0.0009453512029722333\n",
      "epoch: 3 step: 899, loss is 0.0012729387963190675\n",
      "epoch: 3 step: 900, loss is 0.0021241323556751013\n",
      "epoch: 3 step: 901, loss is 0.024659384042024612\n",
      "epoch: 3 step: 902, loss is 0.08727071434259415\n",
      "epoch: 3 step: 903, loss is 0.0016674262005835772\n",
      "epoch: 3 step: 904, loss is 0.0077199554070830345\n",
      "epoch: 3 step: 905, loss is 0.0056580123491585255\n",
      "epoch: 3 step: 906, loss is 0.0177423357963562\n",
      "epoch: 3 step: 907, loss is 0.15943920612335205\n",
      "epoch: 3 step: 908, loss is 0.05344332754611969\n",
      "epoch: 3 step: 909, loss is 0.036620695143938065\n",
      "epoch: 3 step: 910, loss is 0.0003430247015785426\n",
      "epoch: 3 step: 911, loss is 0.009125490672886372\n",
      "epoch: 3 step: 912, loss is 0.08433869481086731\n",
      "epoch: 3 step: 913, loss is 0.01999346911907196\n",
      "epoch: 3 step: 914, loss is 0.025584986433386803\n",
      "epoch: 3 step: 915, loss is 0.015410302206873894\n",
      "epoch: 3 step: 916, loss is 0.008931711316108704\n",
      "epoch: 3 step: 917, loss is 0.0732029527425766\n",
      "epoch: 3 step: 918, loss is 0.07414020597934723\n",
      "epoch: 3 step: 919, loss is 0.01841041073203087\n",
      "epoch: 3 step: 920, loss is 0.02058878168463707\n",
      "epoch: 3 step: 921, loss is 0.03298823535442352\n",
      "epoch: 3 step: 922, loss is 0.005891671869903803\n",
      "epoch: 3 step: 923, loss is 0.0029598206747323275\n",
      "epoch: 3 step: 924, loss is 0.005144487135112286\n",
      "epoch: 3 step: 925, loss is 0.028696948662400246\n",
      "epoch: 3 step: 926, loss is 0.024305244907736778\n",
      "epoch: 3 step: 927, loss is 0.0008185188635252416\n",
      "epoch: 3 step: 928, loss is 0.17938783764839172\n",
      "epoch: 3 step: 929, loss is 0.0272065419703722\n",
      "epoch: 3 step: 930, loss is 0.06746964901685715\n",
      "epoch: 3 step: 931, loss is 0.003940862137824297\n",
      "epoch: 3 step: 932, loss is 0.07028691470623016\n",
      "epoch: 3 step: 933, loss is 0.00048575294204056263\n",
      "epoch: 3 step: 934, loss is 0.10027091950178146\n",
      "epoch: 3 step: 935, loss is 0.00028677110094577074\n",
      "epoch: 3 step: 936, loss is 0.0006944360793568194\n",
      "epoch: 3 step: 937, loss is 0.0839368999004364\n",
      "epoch: 3 step: 938, loss is 0.045848965644836426\n",
      "epoch: 3 step: 939, loss is 0.0010439270408824086\n",
      "epoch: 3 step: 940, loss is 0.008426709100604057\n",
      "epoch: 3 step: 941, loss is 0.013068410567939281\n",
      "epoch: 3 step: 942, loss is 0.04245024546980858\n",
      "epoch: 3 step: 943, loss is 0.010527543723583221\n",
      "epoch: 3 step: 944, loss is 0.12762507796287537\n",
      "epoch: 3 step: 945, loss is 0.006481141317635775\n",
      "epoch: 3 step: 946, loss is 0.14645680785179138\n",
      "epoch: 3 step: 947, loss is 0.005965632386505604\n",
      "epoch: 3 step: 948, loss is 0.018500033766031265\n",
      "epoch: 3 step: 949, loss is 0.04869404435157776\n",
      "epoch: 3 step: 950, loss is 0.013211041688919067\n",
      "epoch: 3 step: 951, loss is 0.004000425338745117\n",
      "epoch: 3 step: 952, loss is 0.004877746105194092\n",
      "epoch: 3 step: 953, loss is 0.06155979633331299\n",
      "epoch: 3 step: 954, loss is 0.009563309140503407\n",
      "epoch: 3 step: 955, loss is 0.22956620156764984\n",
      "epoch: 3 step: 956, loss is 0.002503662370145321\n",
      "epoch: 3 step: 957, loss is 0.03003406524658203\n",
      "epoch: 3 step: 958, loss is 0.09794540703296661\n",
      "epoch: 3 step: 959, loss is 0.00779850035905838\n",
      "epoch: 3 step: 960, loss is 0.012847167439758778\n",
      "epoch: 3 step: 961, loss is 0.010369231924414635\n",
      "epoch: 3 step: 962, loss is 0.014079953543841839\n",
      "epoch: 3 step: 963, loss is 0.10037392377853394\n",
      "epoch: 3 step: 964, loss is 0.06674399971961975\n",
      "epoch: 3 step: 965, loss is 0.1777181625366211\n",
      "epoch: 3 step: 966, loss is 0.013156206347048283\n",
      "epoch: 3 step: 967, loss is 0.07370173186063766\n",
      "epoch: 3 step: 968, loss is 0.019764497876167297\n",
      "epoch: 3 step: 969, loss is 0.016058826819062233\n",
      "epoch: 3 step: 970, loss is 0.2043328881263733\n",
      "epoch: 3 step: 971, loss is 0.060267411172389984\n",
      "epoch: 3 step: 972, loss is 0.006867014802992344\n",
      "epoch: 3 step: 973, loss is 0.003925214055925608\n",
      "epoch: 3 step: 974, loss is 0.06921989470720291\n",
      "epoch: 3 step: 975, loss is 0.001233714516274631\n",
      "epoch: 3 step: 976, loss is 0.08715786784887314\n",
      "epoch: 3 step: 977, loss is 0.0016457505989819765\n",
      "epoch: 3 step: 978, loss is 0.023915011435747147\n",
      "epoch: 3 step: 979, loss is 0.10714472085237503\n",
      "epoch: 3 step: 980, loss is 0.002991965739056468\n",
      "epoch: 3 step: 981, loss is 0.1979849636554718\n",
      "epoch: 3 step: 982, loss is 0.0022214711643755436\n",
      "epoch: 3 step: 983, loss is 0.12877744436264038\n",
      "epoch: 3 step: 984, loss is 0.21341095864772797\n",
      "epoch: 3 step: 985, loss is 0.0017955879447981715\n",
      "epoch: 3 step: 986, loss is 0.03065537102520466\n",
      "epoch: 3 step: 987, loss is 0.031327392905950546\n",
      "epoch: 3 step: 988, loss is 0.03979013115167618\n",
      "epoch: 3 step: 989, loss is 0.024048499763011932\n",
      "epoch: 3 step: 990, loss is 0.00724757369607687\n",
      "epoch: 3 step: 991, loss is 0.024959778413176537\n",
      "epoch: 3 step: 992, loss is 0.046993568539619446\n",
      "epoch: 3 step: 993, loss is 0.11292421817779541\n",
      "epoch: 3 step: 994, loss is 0.028726693242788315\n",
      "epoch: 3 step: 995, loss is 0.002979885321110487\n",
      "epoch: 3 step: 996, loss is 0.16129055619239807\n",
      "epoch: 3 step: 997, loss is 0.005150731652975082\n",
      "epoch: 3 step: 998, loss is 0.05460364744067192\n",
      "epoch: 3 step: 999, loss is 0.08052762597799301\n",
      "epoch: 3 step: 1000, loss is 0.006235763430595398\n",
      "epoch: 3 step: 1001, loss is 0.019584983587265015\n",
      "epoch: 3 step: 1002, loss is 0.08813735097646713\n",
      "epoch: 3 step: 1003, loss is 0.01725180074572563\n",
      "epoch: 3 step: 1004, loss is 0.005232240539044142\n",
      "epoch: 3 step: 1005, loss is 0.08027955889701843\n",
      "epoch: 3 step: 1006, loss is 0.03138967603445053\n",
      "epoch: 3 step: 1007, loss is 0.001302163815125823\n",
      "epoch: 3 step: 1008, loss is 0.0015739568043500185\n",
      "epoch: 3 step: 1009, loss is 0.05292229354381561\n",
      "epoch: 3 step: 1010, loss is 0.09337782114744186\n",
      "epoch: 3 step: 1011, loss is 0.0012564943172037601\n",
      "epoch: 3 step: 1012, loss is 0.023307621479034424\n",
      "epoch: 3 step: 1013, loss is 0.09906444698572159\n",
      "epoch: 3 step: 1014, loss is 0.001231311121955514\n",
      "epoch: 3 step: 1015, loss is 0.14699968695640564\n",
      "epoch: 3 step: 1016, loss is 0.012733661569654942\n",
      "epoch: 3 step: 1017, loss is 0.04363258183002472\n",
      "epoch: 3 step: 1018, loss is 0.004995523951947689\n",
      "epoch: 3 step: 1019, loss is 0.22217869758605957\n",
      "epoch: 3 step: 1020, loss is 0.0013313036179170012\n",
      "epoch: 3 step: 1021, loss is 0.050424974411726\n",
      "epoch: 3 step: 1022, loss is 0.009352801367640495\n",
      "epoch: 3 step: 1023, loss is 0.003477641614153981\n",
      "epoch: 3 step: 1024, loss is 0.1509493589401245\n",
      "epoch: 3 step: 1025, loss is 0.00347976665943861\n",
      "epoch: 3 step: 1026, loss is 0.007384238298982382\n",
      "epoch: 3 step: 1027, loss is 0.14313550293445587\n",
      "epoch: 3 step: 1028, loss is 0.009480618871748447\n",
      "epoch: 3 step: 1029, loss is 0.0010372414253652096\n",
      "epoch: 3 step: 1030, loss is 0.07362347841262817\n",
      "epoch: 3 step: 1031, loss is 0.004362939391285181\n",
      "epoch: 3 step: 1032, loss is 0.02728205919265747\n",
      "epoch: 3 step: 1033, loss is 0.019168097525835037\n",
      "epoch: 3 step: 1034, loss is 0.004494323395192623\n",
      "epoch: 3 step: 1035, loss is 0.004556187894195318\n",
      "epoch: 3 step: 1036, loss is 0.030817247927188873\n",
      "epoch: 3 step: 1037, loss is 0.002131430432200432\n",
      "epoch: 3 step: 1038, loss is 0.04597422853112221\n",
      "epoch: 3 step: 1039, loss is 0.03528016805648804\n",
      "epoch: 3 step: 1040, loss is 0.0027136264834553003\n",
      "epoch: 3 step: 1041, loss is 0.028137538582086563\n",
      "epoch: 3 step: 1042, loss is 0.055206526070833206\n",
      "epoch: 3 step: 1043, loss is 0.0012261525262147188\n",
      "epoch: 3 step: 1044, loss is 0.007503837812691927\n",
      "epoch: 3 step: 1045, loss is 0.0056251962669193745\n",
      "epoch: 3 step: 1046, loss is 0.197123721241951\n",
      "epoch: 3 step: 1047, loss is 0.0021714232861995697\n",
      "epoch: 3 step: 1048, loss is 0.015618471428751945\n",
      "epoch: 3 step: 1049, loss is 0.013138939626514912\n",
      "epoch: 3 step: 1050, loss is 0.040724411606788635\n",
      "epoch: 3 step: 1051, loss is 0.05861549824476242\n",
      "epoch: 3 step: 1052, loss is 0.04258285090327263\n",
      "epoch: 3 step: 1053, loss is 0.03131229430437088\n",
      "epoch: 3 step: 1054, loss is 0.016306374222040176\n",
      "epoch: 3 step: 1055, loss is 0.028567101806402206\n",
      "epoch: 3 step: 1056, loss is 0.01618935540318489\n",
      "epoch: 3 step: 1057, loss is 0.02502119168639183\n",
      "epoch: 3 step: 1058, loss is 0.018737733364105225\n",
      "epoch: 3 step: 1059, loss is 0.0006144522922113538\n",
      "epoch: 3 step: 1060, loss is 0.003956784028559923\n",
      "epoch: 3 step: 1061, loss is 0.041438713669776917\n",
      "epoch: 3 step: 1062, loss is 0.003314175410196185\n",
      "epoch: 3 step: 1063, loss is 0.0068063982762396336\n",
      "epoch: 3 step: 1064, loss is 0.09129007905721664\n",
      "epoch: 3 step: 1065, loss is 0.0046071200631558895\n",
      "epoch: 3 step: 1066, loss is 0.15903924405574799\n",
      "epoch: 3 step: 1067, loss is 0.00972936861217022\n",
      "epoch: 3 step: 1068, loss is 0.0023853483144193888\n",
      "epoch: 3 step: 1069, loss is 0.0059563759714365005\n",
      "epoch: 3 step: 1070, loss is 0.0010756561532616615\n",
      "epoch: 3 step: 1071, loss is 0.0026358896866440773\n",
      "epoch: 3 step: 1072, loss is 0.03154357895255089\n",
      "epoch: 3 step: 1073, loss is 0.014674454927444458\n",
      "epoch: 3 step: 1074, loss is 0.004885111004114151\n",
      "epoch: 3 step: 1075, loss is 0.0005609572981484234\n",
      "epoch: 3 step: 1076, loss is 0.011861151084303856\n",
      "epoch: 3 step: 1077, loss is 0.0915064811706543\n",
      "epoch: 3 step: 1078, loss is 0.01654393970966339\n",
      "epoch: 3 step: 1079, loss is 0.00557196419686079\n",
      "epoch: 3 step: 1080, loss is 0.049542587250471115\n",
      "epoch: 3 step: 1081, loss is 0.047081731259822845\n",
      "epoch: 3 step: 1082, loss is 0.0014609926147386432\n",
      "epoch: 3 step: 1083, loss is 0.06615868210792542\n",
      "epoch: 3 step: 1084, loss is 0.0012672283919528127\n",
      "epoch: 3 step: 1085, loss is 0.013852972537279129\n",
      "epoch: 3 step: 1086, loss is 0.0024242629297077656\n",
      "epoch: 3 step: 1087, loss is 0.08923403918743134\n",
      "epoch: 3 step: 1088, loss is 0.0011856985511258245\n",
      "epoch: 3 step: 1089, loss is 0.0055447653867304325\n",
      "epoch: 3 step: 1090, loss is 0.0007072507869452238\n",
      "epoch: 3 step: 1091, loss is 0.01097412034869194\n",
      "epoch: 3 step: 1092, loss is 0.0029306469950824976\n",
      "epoch: 3 step: 1093, loss is 0.24815581738948822\n",
      "epoch: 3 step: 1094, loss is 0.10358188301324844\n",
      "epoch: 3 step: 1095, loss is 0.0797906294465065\n",
      "epoch: 3 step: 1096, loss is 0.021946875378489494\n",
      "epoch: 3 step: 1097, loss is 0.022084534168243408\n",
      "epoch: 3 step: 1098, loss is 0.058990899473428726\n",
      "epoch: 3 step: 1099, loss is 0.02778518944978714\n",
      "epoch: 3 step: 1100, loss is 0.015291593037545681\n",
      "epoch: 3 step: 1101, loss is 0.047177575528621674\n",
      "epoch: 3 step: 1102, loss is 0.018175087869167328\n",
      "epoch: 3 step: 1103, loss is 0.005428284406661987\n",
      "epoch: 3 step: 1104, loss is 0.08734531700611115\n",
      "epoch: 3 step: 1105, loss is 0.0035268375650048256\n",
      "epoch: 3 step: 1106, loss is 0.056537121534347534\n",
      "epoch: 3 step: 1107, loss is 0.00044879497727379203\n",
      "epoch: 3 step: 1108, loss is 0.021594569087028503\n",
      "epoch: 3 step: 1109, loss is 0.0030047299806028605\n",
      "epoch: 3 step: 1110, loss is 0.0012458391720429063\n",
      "epoch: 3 step: 1111, loss is 0.001305433688685298\n",
      "epoch: 3 step: 1112, loss is 0.06087670847773552\n",
      "epoch: 3 step: 1113, loss is 0.0899614691734314\n",
      "epoch: 3 step: 1114, loss is 0.004130885470658541\n",
      "epoch: 3 step: 1115, loss is 0.006996994372457266\n",
      "epoch: 3 step: 1116, loss is 0.004547019023448229\n",
      "epoch: 3 step: 1117, loss is 0.061893265694379807\n",
      "epoch: 3 step: 1118, loss is 0.0011980808340013027\n",
      "epoch: 3 step: 1119, loss is 0.0010016998276114464\n",
      "epoch: 3 step: 1120, loss is 0.12636639177799225\n",
      "epoch: 3 step: 1121, loss is 0.08398520201444626\n",
      "epoch: 3 step: 1122, loss is 0.0007665669545531273\n",
      "epoch: 3 step: 1123, loss is 8.350967982551083e-05\n",
      "epoch: 3 step: 1124, loss is 0.0031673931516706944\n",
      "epoch: 3 step: 1125, loss is 0.1654086410999298\n",
      "epoch: 3 step: 1126, loss is 0.38845089077949524\n",
      "epoch: 3 step: 1127, loss is 0.0032654989045113325\n",
      "epoch: 3 step: 1128, loss is 0.012614553794264793\n",
      "epoch: 3 step: 1129, loss is 0.004296424333006144\n",
      "epoch: 3 step: 1130, loss is 0.2060847133398056\n",
      "epoch: 3 step: 1131, loss is 0.12650653719902039\n",
      "epoch: 3 step: 1132, loss is 0.18604359030723572\n",
      "epoch: 3 step: 1133, loss is 0.19279126822948456\n",
      "epoch: 3 step: 1134, loss is 0.04988590255379677\n",
      "epoch: 3 step: 1135, loss is 0.06511883437633514\n",
      "epoch: 3 step: 1136, loss is 0.029564442113041878\n",
      "epoch: 3 step: 1137, loss is 0.030290652066469193\n",
      "epoch: 3 step: 1138, loss is 0.0014080433174967766\n",
      "epoch: 3 step: 1139, loss is 0.08816324174404144\n",
      "epoch: 3 step: 1140, loss is 0.0028001146856695414\n",
      "epoch: 3 step: 1141, loss is 0.04748712107539177\n",
      "epoch: 3 step: 1142, loss is 0.048289477825164795\n",
      "epoch: 3 step: 1143, loss is 0.007200780790299177\n",
      "epoch: 3 step: 1144, loss is 0.005874284543097019\n",
      "epoch: 3 step: 1145, loss is 0.04482130706310272\n",
      "epoch: 3 step: 1146, loss is 0.021372364833950996\n",
      "epoch: 3 step: 1147, loss is 0.00385856069624424\n",
      "epoch: 3 step: 1148, loss is 0.0022884774953126907\n",
      "epoch: 3 step: 1149, loss is 0.0422842837870121\n",
      "epoch: 3 step: 1150, loss is 0.009774484671652317\n",
      "epoch: 3 step: 1151, loss is 0.0015398452524095774\n",
      "epoch: 3 step: 1152, loss is 0.03961694985628128\n",
      "epoch: 3 step: 1153, loss is 0.0006567294476553798\n",
      "epoch: 3 step: 1154, loss is 0.15906716883182526\n",
      "epoch: 3 step: 1155, loss is 0.0038770902901887894\n",
      "epoch: 3 step: 1156, loss is 0.12953683733940125\n",
      "epoch: 3 step: 1157, loss is 0.1455221325159073\n",
      "epoch: 3 step: 1158, loss is 0.002524719340726733\n",
      "epoch: 3 step: 1159, loss is 0.008044596761465073\n",
      "epoch: 3 step: 1160, loss is 0.11622316390275955\n",
      "epoch: 3 step: 1161, loss is 0.053755152970552444\n",
      "epoch: 3 step: 1162, loss is 0.0024199564941227436\n",
      "epoch: 3 step: 1163, loss is 0.0014821903314441442\n",
      "epoch: 3 step: 1164, loss is 0.006598243024200201\n",
      "epoch: 3 step: 1165, loss is 0.001272686873562634\n",
      "epoch: 3 step: 1166, loss is 0.06288542598485947\n",
      "epoch: 3 step: 1167, loss is 0.4980242848396301\n",
      "epoch: 3 step: 1168, loss is 0.15110504627227783\n",
      "epoch: 3 step: 1169, loss is 0.03470689803361893\n",
      "epoch: 3 step: 1170, loss is 0.08920639008283615\n",
      "epoch: 3 step: 1171, loss is 0.15043756365776062\n",
      "epoch: 3 step: 1172, loss is 0.0164328683167696\n",
      "epoch: 3 step: 1173, loss is 0.02189720794558525\n",
      "epoch: 3 step: 1174, loss is 0.03255521506071091\n",
      "epoch: 3 step: 1175, loss is 0.0009103023912757635\n",
      "epoch: 3 step: 1176, loss is 0.0026501601096242666\n",
      "epoch: 3 step: 1177, loss is 0.0012679430656135082\n",
      "epoch: 3 step: 1178, loss is 0.00186093186493963\n",
      "epoch: 3 step: 1179, loss is 0.09143343567848206\n",
      "epoch: 3 step: 1180, loss is 0.1981392800807953\n",
      "epoch: 3 step: 1181, loss is 0.03944389522075653\n",
      "epoch: 3 step: 1182, loss is 0.04679431393742561\n",
      "epoch: 3 step: 1183, loss is 0.027345554903149605\n",
      "epoch: 3 step: 1184, loss is 0.14687596261501312\n",
      "epoch: 3 step: 1185, loss is 0.20055532455444336\n",
      "epoch: 3 step: 1186, loss is 0.0012419165577739477\n",
      "epoch: 3 step: 1187, loss is 0.12425792962312698\n",
      "epoch: 3 step: 1188, loss is 0.01242478471249342\n",
      "epoch: 3 step: 1189, loss is 0.005200052633881569\n",
      "epoch: 3 step: 1190, loss is 0.03459840640425682\n",
      "epoch: 3 step: 1191, loss is 0.034952618181705475\n",
      "epoch: 3 step: 1192, loss is 0.03490634635090828\n",
      "epoch: 3 step: 1193, loss is 0.05079018697142601\n",
      "epoch: 3 step: 1194, loss is 0.02940729819238186\n",
      "epoch: 3 step: 1195, loss is 0.01253550499677658\n",
      "epoch: 3 step: 1196, loss is 0.35284659266471863\n",
      "epoch: 3 step: 1197, loss is 0.010936104692518711\n",
      "epoch: 3 step: 1198, loss is 0.24092574417591095\n",
      "epoch: 3 step: 1199, loss is 0.023392103612422943\n",
      "epoch: 3 step: 1200, loss is 0.008470024913549423\n",
      "epoch: 3 step: 1201, loss is 0.08027563244104385\n",
      "epoch: 3 step: 1202, loss is 0.017358416691422462\n",
      "epoch: 3 step: 1203, loss is 0.036159612238407135\n",
      "epoch: 3 step: 1204, loss is 0.0016248211031779647\n",
      "epoch: 3 step: 1205, loss is 0.0008955615339800715\n",
      "epoch: 3 step: 1206, loss is 0.006634075194597244\n",
      "epoch: 3 step: 1207, loss is 0.016002479940652847\n",
      "epoch: 3 step: 1208, loss is 0.05386262387037277\n",
      "epoch: 3 step: 1209, loss is 0.050230108201503754\n",
      "epoch: 3 step: 1210, loss is 0.031959980726242065\n",
      "epoch: 3 step: 1211, loss is 0.018515830859541893\n",
      "epoch: 3 step: 1212, loss is 0.024786245077848434\n",
      "epoch: 3 step: 1213, loss is 0.15866588056087494\n",
      "epoch: 3 step: 1214, loss is 0.019730938598513603\n",
      "epoch: 3 step: 1215, loss is 0.1969749480485916\n",
      "epoch: 3 step: 1216, loss is 0.07350160926580429\n",
      "epoch: 3 step: 1217, loss is 0.05802478641271591\n",
      "epoch: 3 step: 1218, loss is 0.09374350309371948\n",
      "epoch: 3 step: 1219, loss is 0.010548115707933903\n",
      "epoch: 3 step: 1220, loss is 0.008428781293332577\n",
      "epoch: 3 step: 1221, loss is 0.03575424477458\n",
      "epoch: 3 step: 1222, loss is 0.011945576407015324\n",
      "epoch: 3 step: 1223, loss is 0.048531271517276764\n",
      "epoch: 3 step: 1224, loss is 0.012569502927362919\n",
      "epoch: 3 step: 1225, loss is 0.0045603602193295956\n",
      "epoch: 3 step: 1226, loss is 0.011789108626544476\n",
      "epoch: 3 step: 1227, loss is 0.026958975940942764\n",
      "epoch: 3 step: 1228, loss is 0.02068055234849453\n",
      "epoch: 3 step: 1229, loss is 0.005496135447174311\n",
      "epoch: 3 step: 1230, loss is 0.014249907806515694\n",
      "epoch: 3 step: 1231, loss is 0.07647714018821716\n",
      "epoch: 3 step: 1232, loss is 0.005485668778419495\n",
      "epoch: 3 step: 1233, loss is 0.0024389729369431734\n",
      "epoch: 3 step: 1234, loss is 0.032870031893253326\n",
      "epoch: 3 step: 1235, loss is 0.12792082130908966\n",
      "epoch: 3 step: 1236, loss is 0.0018616453744471073\n",
      "epoch: 3 step: 1237, loss is 0.08841723948717117\n",
      "epoch: 3 step: 1238, loss is 0.006177617236971855\n",
      "epoch: 3 step: 1239, loss is 0.02152331732213497\n",
      "epoch: 3 step: 1240, loss is 0.015723902732133865\n",
      "epoch: 3 step: 1241, loss is 0.0650433599948883\n",
      "epoch: 3 step: 1242, loss is 0.002179137198254466\n",
      "epoch: 3 step: 1243, loss is 0.018827129155397415\n",
      "epoch: 3 step: 1244, loss is 0.0020884722471237183\n",
      "epoch: 3 step: 1245, loss is 0.2736741304397583\n",
      "epoch: 3 step: 1246, loss is 0.011246978305280209\n",
      "epoch: 3 step: 1247, loss is 0.0015807999297976494\n",
      "epoch: 3 step: 1248, loss is 0.05724495276808739\n",
      "epoch: 3 step: 1249, loss is 0.07465904951095581\n",
      "epoch: 3 step: 1250, loss is 0.08847660571336746\n",
      "epoch: 3 step: 1251, loss is 0.010341834276914597\n",
      "epoch: 3 step: 1252, loss is 0.137258842587471\n",
      "epoch: 3 step: 1253, loss is 0.010852078907191753\n",
      "epoch: 3 step: 1254, loss is 0.01204737089574337\n",
      "epoch: 3 step: 1255, loss is 0.05845518782734871\n",
      "epoch: 3 step: 1256, loss is 0.04879958555102348\n",
      "epoch: 3 step: 1257, loss is 0.03143700957298279\n",
      "epoch: 3 step: 1258, loss is 0.037999339401721954\n",
      "epoch: 3 step: 1259, loss is 0.006610742770135403\n",
      "epoch: 3 step: 1260, loss is 0.06009141728281975\n",
      "epoch: 3 step: 1261, loss is 0.04399392381310463\n",
      "epoch: 3 step: 1262, loss is 0.07336796820163727\n",
      "epoch: 3 step: 1263, loss is 0.01858212612569332\n",
      "epoch: 3 step: 1264, loss is 0.15571437776088715\n",
      "epoch: 3 step: 1265, loss is 0.008691892959177494\n",
      "epoch: 3 step: 1266, loss is 0.02515045739710331\n",
      "epoch: 3 step: 1267, loss is 0.0075131733901798725\n",
      "epoch: 3 step: 1268, loss is 0.004027893766760826\n",
      "epoch: 3 step: 1269, loss is 0.03310248628258705\n",
      "epoch: 3 step: 1270, loss is 0.012705372646450996\n",
      "epoch: 3 step: 1271, loss is 0.01648300513625145\n",
      "epoch: 3 step: 1272, loss is 0.036880698055028915\n",
      "epoch: 3 step: 1273, loss is 0.053864993155002594\n",
      "epoch: 3 step: 1274, loss is 0.09874199330806732\n",
      "epoch: 3 step: 1275, loss is 0.02075919881463051\n",
      "epoch: 3 step: 1276, loss is 0.0021359233651310205\n",
      "epoch: 3 step: 1277, loss is 0.0005065288860350847\n",
      "epoch: 3 step: 1278, loss is 0.011689076200127602\n",
      "epoch: 3 step: 1279, loss is 0.181863471865654\n",
      "epoch: 3 step: 1280, loss is 0.007671927101910114\n",
      "epoch: 3 step: 1281, loss is 0.01939910091459751\n",
      "epoch: 3 step: 1282, loss is 0.005191416013985872\n",
      "epoch: 3 step: 1283, loss is 0.05678773298859596\n",
      "epoch: 3 step: 1284, loss is 0.003242319216951728\n",
      "epoch: 3 step: 1285, loss is 0.09441040456295013\n",
      "epoch: 3 step: 1286, loss is 0.05390775948762894\n",
      "epoch: 3 step: 1287, loss is 0.0013848574599251151\n",
      "epoch: 3 step: 1288, loss is 0.012873668223619461\n",
      "epoch: 3 step: 1289, loss is 0.011996496468782425\n",
      "epoch: 3 step: 1290, loss is 0.0031428406946361065\n",
      "epoch: 3 step: 1291, loss is 0.003559319069609046\n",
      "epoch: 3 step: 1292, loss is 0.007197405677288771\n",
      "epoch: 3 step: 1293, loss is 0.002093557035550475\n",
      "epoch: 3 step: 1294, loss is 0.017962217330932617\n",
      "epoch: 3 step: 1295, loss is 0.10300581902265549\n",
      "epoch: 3 step: 1296, loss is 0.0404752679169178\n",
      "epoch: 3 step: 1297, loss is 0.0026093788910657167\n",
      "epoch: 3 step: 1298, loss is 0.04586787521839142\n",
      "epoch: 3 step: 1299, loss is 0.07040306180715561\n",
      "epoch: 3 step: 1300, loss is 0.010963048785924911\n",
      "epoch: 3 step: 1301, loss is 0.007209900300949812\n",
      "epoch: 3 step: 1302, loss is 0.01591005176305771\n",
      "epoch: 3 step: 1303, loss is 0.003155754180625081\n",
      "epoch: 3 step: 1304, loss is 0.1637764722108841\n",
      "epoch: 3 step: 1305, loss is 0.006603540852665901\n",
      "epoch: 3 step: 1306, loss is 0.011590818874537945\n",
      "epoch: 3 step: 1307, loss is 0.011771527118980885\n",
      "epoch: 3 step: 1308, loss is 0.0008895851206034422\n",
      "epoch: 3 step: 1309, loss is 0.012848717160522938\n",
      "epoch: 3 step: 1310, loss is 0.0003197352634742856\n",
      "epoch: 3 step: 1311, loss is 0.020409392192959785\n",
      "epoch: 3 step: 1312, loss is 0.0007713248487561941\n",
      "epoch: 3 step: 1313, loss is 0.04843815043568611\n",
      "epoch: 3 step: 1314, loss is 0.07152912020683289\n",
      "epoch: 3 step: 1315, loss is 0.13771404325962067\n",
      "epoch: 3 step: 1316, loss is 0.0008051612530834973\n",
      "epoch: 3 step: 1317, loss is 0.0013847110094502568\n",
      "epoch: 3 step: 1318, loss is 0.007757734507322311\n",
      "epoch: 3 step: 1319, loss is 0.006354954559355974\n",
      "epoch: 3 step: 1320, loss is 0.06589486449956894\n",
      "epoch: 3 step: 1321, loss is 0.0026687949430197477\n",
      "epoch: 3 step: 1322, loss is 0.072702556848526\n",
      "epoch: 3 step: 1323, loss is 0.014295230619609356\n",
      "epoch: 3 step: 1324, loss is 0.24008969962596893\n",
      "epoch: 3 step: 1325, loss is 0.1536814421415329\n",
      "epoch: 3 step: 1326, loss is 0.0122594079002738\n",
      "epoch: 3 step: 1327, loss is 0.18598581850528717\n",
      "epoch: 3 step: 1328, loss is 0.0009182504145428538\n",
      "epoch: 3 step: 1329, loss is 0.08230749517679214\n",
      "epoch: 3 step: 1330, loss is 0.01216889824718237\n",
      "epoch: 3 step: 1331, loss is 0.12016335874795914\n",
      "epoch: 3 step: 1332, loss is 0.02040044590830803\n",
      "epoch: 3 step: 1333, loss is 0.021261340007185936\n",
      "epoch: 3 step: 1334, loss is 0.017007475718855858\n",
      "epoch: 3 step: 1335, loss is 0.05811602994799614\n",
      "epoch: 3 step: 1336, loss is 0.15552102029323578\n",
      "epoch: 3 step: 1337, loss is 0.029825959354639053\n",
      "epoch: 3 step: 1338, loss is 0.001321293879300356\n",
      "epoch: 3 step: 1339, loss is 0.13104566931724548\n",
      "epoch: 3 step: 1340, loss is 0.003770360490307212\n",
      "epoch: 3 step: 1341, loss is 0.002076497534289956\n",
      "epoch: 3 step: 1342, loss is 0.002911069430410862\n",
      "epoch: 3 step: 1343, loss is 0.012288694269955158\n",
      "epoch: 3 step: 1344, loss is 0.0036349294241517782\n",
      "epoch: 3 step: 1345, loss is 0.10508159548044205\n",
      "epoch: 3 step: 1346, loss is 0.011532084085047245\n",
      "epoch: 3 step: 1347, loss is 0.2871687710285187\n",
      "epoch: 3 step: 1348, loss is 0.016005422919988632\n",
      "epoch: 3 step: 1349, loss is 0.007251139730215073\n",
      "epoch: 3 step: 1350, loss is 0.0012141664046794176\n",
      "epoch: 3 step: 1351, loss is 0.0037043248303234577\n",
      "epoch: 3 step: 1352, loss is 0.035321276634931564\n",
      "epoch: 3 step: 1353, loss is 0.06833525002002716\n",
      "epoch: 3 step: 1354, loss is 0.03237948939204216\n",
      "epoch: 3 step: 1355, loss is 0.0037986361421644688\n",
      "epoch: 3 step: 1356, loss is 0.03671596571803093\n",
      "epoch: 3 step: 1357, loss is 0.05606692656874657\n",
      "epoch: 3 step: 1358, loss is 0.02314826101064682\n",
      "epoch: 3 step: 1359, loss is 0.14991353452205658\n",
      "epoch: 3 step: 1360, loss is 0.051765576004981995\n",
      "epoch: 3 step: 1361, loss is 0.006536326836794615\n",
      "epoch: 3 step: 1362, loss is 0.05344653129577637\n",
      "epoch: 3 step: 1363, loss is 0.003940592519938946\n",
      "epoch: 3 step: 1364, loss is 0.0314004085958004\n",
      "epoch: 3 step: 1365, loss is 0.03283056244254112\n",
      "epoch: 3 step: 1366, loss is 0.03920372948050499\n",
      "epoch: 3 step: 1367, loss is 0.08013913780450821\n",
      "epoch: 3 step: 1368, loss is 0.008502925746142864\n",
      "epoch: 3 step: 1369, loss is 0.07237552851438522\n",
      "epoch: 3 step: 1370, loss is 0.005359594244509935\n",
      "epoch: 3 step: 1371, loss is 0.0018555833958089352\n",
      "epoch: 3 step: 1372, loss is 0.01493407879024744\n",
      "epoch: 3 step: 1373, loss is 0.007066070102155209\n",
      "epoch: 3 step: 1374, loss is 0.09893984347581863\n",
      "epoch: 3 step: 1375, loss is 0.005873389542102814\n",
      "epoch: 3 step: 1376, loss is 0.039965253323316574\n",
      "epoch: 3 step: 1377, loss is 0.10388711839914322\n",
      "epoch: 3 step: 1378, loss is 0.04516123980283737\n",
      "epoch: 3 step: 1379, loss is 0.0011813165619969368\n",
      "epoch: 3 step: 1380, loss is 0.0561785064637661\n",
      "epoch: 3 step: 1381, loss is 0.00029486228595487773\n",
      "epoch: 3 step: 1382, loss is 0.00888014119118452\n",
      "epoch: 3 step: 1383, loss is 0.0031140665523707867\n",
      "epoch: 3 step: 1384, loss is 0.04546737298369408\n",
      "epoch: 3 step: 1385, loss is 0.012497829273343086\n",
      "epoch: 3 step: 1386, loss is 0.12973172962665558\n",
      "epoch: 3 step: 1387, loss is 0.3729793429374695\n",
      "epoch: 3 step: 1388, loss is 0.10258251428604126\n",
      "epoch: 3 step: 1389, loss is 0.027735261246562004\n",
      "epoch: 3 step: 1390, loss is 0.006388366688042879\n",
      "epoch: 3 step: 1391, loss is 0.07912430167198181\n",
      "epoch: 3 step: 1392, loss is 0.0041784546338021755\n",
      "epoch: 3 step: 1393, loss is 0.0032662502489984035\n",
      "epoch: 3 step: 1394, loss is 0.024339187890291214\n",
      "epoch: 3 step: 1395, loss is 0.0073846918530762196\n",
      "epoch: 3 step: 1396, loss is 0.006019804161041975\n",
      "epoch: 3 step: 1397, loss is 0.3420769274234772\n",
      "epoch: 3 step: 1398, loss is 0.01056092418730259\n",
      "epoch: 3 step: 1399, loss is 0.007931074127554893\n",
      "epoch: 3 step: 1400, loss is 0.023207608610391617\n",
      "epoch: 3 step: 1401, loss is 0.06817208975553513\n",
      "epoch: 3 step: 1402, loss is 0.028021186590194702\n",
      "epoch: 3 step: 1403, loss is 0.0031408618669956923\n",
      "epoch: 3 step: 1404, loss is 0.024799270555377007\n",
      "epoch: 3 step: 1405, loss is 0.04044796898961067\n",
      "epoch: 3 step: 1406, loss is 0.020090896636247635\n",
      "epoch: 3 step: 1407, loss is 0.22099857032299042\n",
      "epoch: 3 step: 1408, loss is 0.043200425803661346\n",
      "epoch: 3 step: 1409, loss is 0.024915646761655807\n",
      "epoch: 3 step: 1410, loss is 0.20332564413547516\n",
      "epoch: 3 step: 1411, loss is 0.003965889103710651\n",
      "epoch: 3 step: 1412, loss is 0.0035645405296236277\n",
      "epoch: 3 step: 1413, loss is 0.04514927789568901\n",
      "epoch: 3 step: 1414, loss is 0.004544898401945829\n",
      "epoch: 3 step: 1415, loss is 0.10904059559106827\n",
      "epoch: 3 step: 1416, loss is 0.02323906123638153\n",
      "epoch: 3 step: 1417, loss is 0.0028354590758681297\n",
      "epoch: 3 step: 1418, loss is 0.06586884707212448\n",
      "epoch: 3 step: 1419, loss is 0.0300582442432642\n",
      "epoch: 3 step: 1420, loss is 0.09561868757009506\n",
      "epoch: 3 step: 1421, loss is 0.04611944407224655\n",
      "epoch: 3 step: 1422, loss is 0.005671598482877016\n",
      "epoch: 3 step: 1423, loss is 0.018266461789608\n",
      "epoch: 3 step: 1424, loss is 0.15000027418136597\n",
      "epoch: 3 step: 1425, loss is 0.005636813584715128\n",
      "epoch: 3 step: 1426, loss is 0.019972681999206543\n",
      "epoch: 3 step: 1427, loss is 0.03683614730834961\n",
      "epoch: 3 step: 1428, loss is 0.046651244163513184\n",
      "epoch: 3 step: 1429, loss is 0.05353142321109772\n",
      "epoch: 3 step: 1430, loss is 0.0029156252276152372\n",
      "epoch: 3 step: 1431, loss is 0.01891188696026802\n",
      "epoch: 3 step: 1432, loss is 0.17828230559825897\n",
      "epoch: 3 step: 1433, loss is 0.1217077299952507\n",
      "epoch: 3 step: 1434, loss is 0.0007747404742985964\n",
      "epoch: 3 step: 1435, loss is 0.0013259121915325522\n",
      "epoch: 3 step: 1436, loss is 0.015314808115363121\n",
      "epoch: 3 step: 1437, loss is 0.055871762335300446\n",
      "epoch: 3 step: 1438, loss is 0.07677137106657028\n",
      "epoch: 3 step: 1439, loss is 0.11879526823759079\n",
      "epoch: 3 step: 1440, loss is 0.012384377419948578\n",
      "epoch: 3 step: 1441, loss is 0.007535517681390047\n",
      "epoch: 3 step: 1442, loss is 0.011247983202338219\n",
      "epoch: 3 step: 1443, loss is 0.0014113413635641336\n",
      "epoch: 3 step: 1444, loss is 0.011568333022296429\n",
      "epoch: 3 step: 1445, loss is 0.13158777356147766\n",
      "epoch: 3 step: 1446, loss is 0.0288827046751976\n",
      "epoch: 3 step: 1447, loss is 0.17897963523864746\n",
      "epoch: 3 step: 1448, loss is 0.012520397081971169\n",
      "epoch: 3 step: 1449, loss is 0.007620812393724918\n",
      "epoch: 3 step: 1450, loss is 0.001053106039762497\n",
      "epoch: 3 step: 1451, loss is 0.13047531247138977\n",
      "epoch: 3 step: 1452, loss is 0.009299831464886665\n",
      "epoch: 3 step: 1453, loss is 0.014476673677563667\n",
      "epoch: 3 step: 1454, loss is 0.001713659381493926\n",
      "epoch: 3 step: 1455, loss is 0.0017883377149701118\n",
      "epoch: 3 step: 1456, loss is 0.030499350279569626\n",
      "epoch: 3 step: 1457, loss is 0.016335491091012955\n",
      "epoch: 3 step: 1458, loss is 0.0005825388361699879\n",
      "epoch: 3 step: 1459, loss is 0.009684423916041851\n",
      "epoch: 3 step: 1460, loss is 0.0018963705515488982\n",
      "epoch: 3 step: 1461, loss is 0.028675606474280357\n",
      "epoch: 3 step: 1462, loss is 0.012666741386055946\n",
      "epoch: 3 step: 1463, loss is 0.01118251122534275\n",
      "epoch: 3 step: 1464, loss is 0.1095760241150856\n",
      "epoch: 3 step: 1465, loss is 0.1146082878112793\n",
      "epoch: 3 step: 1466, loss is 0.0012799804098904133\n",
      "epoch: 3 step: 1467, loss is 0.0038419777993112803\n",
      "epoch: 3 step: 1468, loss is 0.00422920985147357\n",
      "epoch: 3 step: 1469, loss is 0.011800182983279228\n",
      "epoch: 3 step: 1470, loss is 0.0823235884308815\n",
      "epoch: 3 step: 1471, loss is 0.08103807270526886\n",
      "epoch: 3 step: 1472, loss is 0.13686694204807281\n",
      "epoch: 3 step: 1473, loss is 0.006423309911042452\n",
      "epoch: 3 step: 1474, loss is 0.001696263556368649\n",
      "epoch: 3 step: 1475, loss is 0.02299761213362217\n",
      "epoch: 3 step: 1476, loss is 0.032246965914964676\n",
      "epoch: 3 step: 1477, loss is 0.005572263617068529\n",
      "epoch: 3 step: 1478, loss is 0.0026562486309558153\n",
      "epoch: 3 step: 1479, loss is 0.002277107210829854\n",
      "epoch: 3 step: 1480, loss is 0.014799059368669987\n",
      "epoch: 3 step: 1481, loss is 0.06170641630887985\n",
      "epoch: 3 step: 1482, loss is 0.027096718549728394\n",
      "epoch: 3 step: 1483, loss is 0.0018354393541812897\n",
      "epoch: 3 step: 1484, loss is 0.3288224935531616\n",
      "epoch: 3 step: 1485, loss is 0.11853470653295517\n",
      "epoch: 3 step: 1486, loss is 0.014129018411040306\n",
      "epoch: 3 step: 1487, loss is 0.05047455430030823\n",
      "epoch: 3 step: 1488, loss is 0.006501913070678711\n",
      "epoch: 3 step: 1489, loss is 0.017029840499162674\n",
      "epoch: 3 step: 1490, loss is 0.009343222714960575\n",
      "epoch: 3 step: 1491, loss is 0.04164285957813263\n",
      "epoch: 3 step: 1492, loss is 0.07624126970767975\n",
      "epoch: 3 step: 1493, loss is 0.014446418732404709\n",
      "epoch: 3 step: 1494, loss is 0.02879827283322811\n",
      "epoch: 3 step: 1495, loss is 0.04588332399725914\n",
      "epoch: 3 step: 1496, loss is 0.004042108543217182\n",
      "epoch: 3 step: 1497, loss is 0.001326118246652186\n",
      "epoch: 3 step: 1498, loss is 0.0032851225696504116\n",
      "epoch: 3 step: 1499, loss is 0.006141250487416983\n",
      "epoch: 3 step: 1500, loss is 0.012337695807218552\n",
      "epoch: 3 step: 1501, loss is 0.004007253330200911\n",
      "epoch: 3 step: 1502, loss is 0.022679638117551804\n",
      "epoch: 3 step: 1503, loss is 0.0013817442813888192\n",
      "epoch: 3 step: 1504, loss is 0.01489343587309122\n",
      "epoch: 3 step: 1505, loss is 0.008940511383116245\n",
      "epoch: 3 step: 1506, loss is 0.0012256084010004997\n",
      "epoch: 3 step: 1507, loss is 0.010607310570776463\n",
      "epoch: 3 step: 1508, loss is 0.03711536526679993\n",
      "epoch: 3 step: 1509, loss is 0.0010594833875074983\n",
      "epoch: 3 step: 1510, loss is 0.019525399431586266\n",
      "epoch: 3 step: 1511, loss is 0.003191125113517046\n",
      "epoch: 3 step: 1512, loss is 0.017845282331109047\n",
      "epoch: 3 step: 1513, loss is 0.016260402277112007\n",
      "epoch: 3 step: 1514, loss is 0.002015351550653577\n",
      "epoch: 3 step: 1515, loss is 0.002653915900737047\n",
      "epoch: 3 step: 1516, loss is 0.017026158049702644\n",
      "epoch: 3 step: 1517, loss is 0.0003504370688460767\n",
      "epoch: 3 step: 1518, loss is 0.21711008250713348\n",
      "epoch: 3 step: 1519, loss is 0.046029239892959595\n",
      "epoch: 3 step: 1520, loss is 0.08715088665485382\n",
      "epoch: 3 step: 1521, loss is 0.0023331164848059416\n",
      "epoch: 3 step: 1522, loss is 0.03236941248178482\n",
      "epoch: 3 step: 1523, loss is 0.002609140705317259\n",
      "epoch: 3 step: 1524, loss is 0.0030695602763444185\n",
      "epoch: 3 step: 1525, loss is 0.0039304569363594055\n",
      "epoch: 3 step: 1526, loss is 0.020025081932544708\n",
      "epoch: 3 step: 1527, loss is 0.0048821489326655865\n",
      "epoch: 3 step: 1528, loss is 0.048053495585918427\n",
      "epoch: 3 step: 1529, loss is 0.007145700976252556\n",
      "epoch: 3 step: 1530, loss is 0.001935550943017006\n",
      "epoch: 3 step: 1531, loss is 0.10920356214046478\n",
      "epoch: 3 step: 1532, loss is 0.054297033697366714\n",
      "epoch: 3 step: 1533, loss is 0.09866809844970703\n",
      "epoch: 3 step: 1534, loss is 0.20723088085651398\n",
      "epoch: 3 step: 1535, loss is 0.004271435085684061\n",
      "epoch: 3 step: 1536, loss is 0.0005864679114893079\n",
      "epoch: 3 step: 1537, loss is 0.01583205908536911\n",
      "epoch: 3 step: 1538, loss is 0.2661221921443939\n",
      "epoch: 3 step: 1539, loss is 0.0008496530936099589\n",
      "epoch: 3 step: 1540, loss is 0.0011617479613050818\n",
      "epoch: 3 step: 1541, loss is 0.09002680331468582\n",
      "epoch: 3 step: 1542, loss is 0.000782469694968313\n",
      "epoch: 3 step: 1543, loss is 0.0058953105472028255\n",
      "epoch: 3 step: 1544, loss is 0.0179486945271492\n",
      "epoch: 3 step: 1545, loss is 0.04540163651108742\n",
      "epoch: 3 step: 1546, loss is 0.002893880009651184\n",
      "epoch: 3 step: 1547, loss is 0.024857133626937866\n",
      "epoch: 3 step: 1548, loss is 0.0067319804802536964\n",
      "epoch: 3 step: 1549, loss is 0.13479097187519073\n",
      "epoch: 3 step: 1550, loss is 0.021750830113887787\n",
      "epoch: 3 step: 1551, loss is 0.01129801757633686\n",
      "epoch: 3 step: 1552, loss is 0.11675388365983963\n",
      "epoch: 3 step: 1553, loss is 0.034518834203481674\n",
      "epoch: 3 step: 1554, loss is 0.008466235361993313\n",
      "epoch: 3 step: 1555, loss is 0.13829182088375092\n",
      "epoch: 3 step: 1556, loss is 0.003131417790427804\n",
      "epoch: 3 step: 1557, loss is 0.052465204149484634\n",
      "epoch: 3 step: 1558, loss is 0.002059631049633026\n",
      "epoch: 3 step: 1559, loss is 0.0025328113697469234\n",
      "epoch: 3 step: 1560, loss is 0.0927344411611557\n",
      "epoch: 3 step: 1561, loss is 0.0004059111524838954\n",
      "epoch: 3 step: 1562, loss is 0.0020892927423119545\n",
      "epoch: 3 step: 1563, loss is 0.012831931002438068\n",
      "epoch: 3 step: 1564, loss is 0.011447948403656483\n",
      "epoch: 3 step: 1565, loss is 0.010661186650395393\n",
      "epoch: 3 step: 1566, loss is 0.21810787916183472\n",
      "epoch: 3 step: 1567, loss is 0.001817392185330391\n",
      "epoch: 3 step: 1568, loss is 0.021812740713357925\n",
      "epoch: 3 step: 1569, loss is 0.12537303566932678\n",
      "epoch: 3 step: 1570, loss is 0.035156749188899994\n",
      "epoch: 3 step: 1571, loss is 0.12340747565031052\n",
      "epoch: 3 step: 1572, loss is 0.004909575916826725\n",
      "epoch: 3 step: 1573, loss is 0.0045420387759804726\n",
      "epoch: 3 step: 1574, loss is 0.012450565584003925\n",
      "epoch: 3 step: 1575, loss is 0.0004573086625896394\n",
      "epoch: 3 step: 1576, loss is 0.018711993470788002\n",
      "epoch: 3 step: 1577, loss is 0.06510590016841888\n",
      "epoch: 3 step: 1578, loss is 0.0334891751408577\n",
      "epoch: 3 step: 1579, loss is 0.07652787864208221\n",
      "epoch: 3 step: 1580, loss is 0.0018897114787250757\n",
      "epoch: 3 step: 1581, loss is 0.021749231964349747\n",
      "epoch: 3 step: 1582, loss is 0.0020090891048312187\n",
      "epoch: 3 step: 1583, loss is 0.004782564472407103\n",
      "epoch: 3 step: 1584, loss is 0.04011810943484306\n",
      "epoch: 3 step: 1585, loss is 0.08038262277841568\n",
      "epoch: 3 step: 1586, loss is 0.002656788332387805\n",
      "epoch: 3 step: 1587, loss is 0.024475250393152237\n",
      "epoch: 3 step: 1588, loss is 0.07228207588195801\n",
      "epoch: 3 step: 1589, loss is 0.05473589152097702\n",
      "epoch: 3 step: 1590, loss is 0.0017093784408643842\n",
      "epoch: 3 step: 1591, loss is 0.0013551505981013179\n",
      "epoch: 3 step: 1592, loss is 0.11257109045982361\n",
      "epoch: 3 step: 1593, loss is 0.16436925530433655\n",
      "epoch: 3 step: 1594, loss is 0.022428050637245178\n",
      "epoch: 3 step: 1595, loss is 0.0058721741661429405\n",
      "epoch: 3 step: 1596, loss is 0.11614639312028885\n",
      "epoch: 3 step: 1597, loss is 0.010377476923167706\n",
      "epoch: 3 step: 1598, loss is 0.17657148838043213\n",
      "epoch: 3 step: 1599, loss is 0.04485675320029259\n",
      "epoch: 3 step: 1600, loss is 0.01708071306347847\n",
      "epoch: 3 step: 1601, loss is 0.0025629522278904915\n",
      "epoch: 3 step: 1602, loss is 0.013539649546146393\n",
      "epoch: 3 step: 1603, loss is 0.0019261230481788516\n",
      "epoch: 3 step: 1604, loss is 0.07987526059150696\n",
      "epoch: 3 step: 1605, loss is 0.018901588395237923\n",
      "epoch: 3 step: 1606, loss is 0.02947196550667286\n",
      "epoch: 3 step: 1607, loss is 0.11922869831323624\n",
      "epoch: 3 step: 1608, loss is 0.08188670873641968\n",
      "epoch: 3 step: 1609, loss is 0.0008648926159366965\n",
      "epoch: 3 step: 1610, loss is 0.0003876563860103488\n",
      "epoch: 3 step: 1611, loss is 0.005950808059424162\n",
      "epoch: 3 step: 1612, loss is 0.001764592481777072\n",
      "epoch: 3 step: 1613, loss is 0.12831079959869385\n",
      "epoch: 3 step: 1614, loss is 0.005976133979856968\n",
      "epoch: 3 step: 1615, loss is 0.022143572568893433\n",
      "epoch: 3 step: 1616, loss is 0.0008748610853217542\n",
      "epoch: 3 step: 1617, loss is 0.005076611880213022\n",
      "epoch: 3 step: 1618, loss is 0.024054551497101784\n",
      "epoch: 3 step: 1619, loss is 0.0005602312157861888\n",
      "epoch: 3 step: 1620, loss is 0.06349164992570877\n",
      "epoch: 3 step: 1621, loss is 0.0928986668586731\n",
      "epoch: 3 step: 1622, loss is 0.01475865114480257\n",
      "epoch: 3 step: 1623, loss is 0.002698784926906228\n",
      "epoch: 3 step: 1624, loss is 0.016228273510932922\n",
      "epoch: 3 step: 1625, loss is 0.02480148710310459\n",
      "epoch: 3 step: 1626, loss is 0.0016461302293464541\n",
      "epoch: 3 step: 1627, loss is 0.20374371111392975\n",
      "epoch: 3 step: 1628, loss is 0.012472635135054588\n",
      "epoch: 3 step: 1629, loss is 0.006183004938066006\n",
      "epoch: 3 step: 1630, loss is 0.1231602281332016\n",
      "epoch: 3 step: 1631, loss is 0.10206274688243866\n",
      "epoch: 3 step: 1632, loss is 0.06765498965978622\n",
      "epoch: 3 step: 1633, loss is 0.1416754275560379\n",
      "epoch: 3 step: 1634, loss is 0.041348837316036224\n",
      "epoch: 3 step: 1635, loss is 0.02043396234512329\n",
      "epoch: 3 step: 1636, loss is 0.24899481236934662\n",
      "epoch: 3 step: 1637, loss is 0.2853180170059204\n",
      "epoch: 3 step: 1638, loss is 0.040574267506599426\n",
      "epoch: 3 step: 1639, loss is 0.02407386712729931\n",
      "epoch: 3 step: 1640, loss is 0.016251277178525925\n",
      "epoch: 3 step: 1641, loss is 0.006422408856451511\n",
      "epoch: 3 step: 1642, loss is 0.03532329574227333\n",
      "epoch: 3 step: 1643, loss is 0.18484294414520264\n",
      "epoch: 3 step: 1644, loss is 0.011470175348222256\n",
      "epoch: 3 step: 1645, loss is 0.009180801920592785\n",
      "epoch: 3 step: 1646, loss is 0.009016742929816246\n",
      "epoch: 3 step: 1647, loss is 0.0049479310400784016\n",
      "epoch: 3 step: 1648, loss is 0.010861287824809551\n",
      "epoch: 3 step: 1649, loss is 0.0011653258698061109\n",
      "epoch: 3 step: 1650, loss is 0.06077095493674278\n",
      "epoch: 3 step: 1651, loss is 0.001825219951570034\n",
      "epoch: 3 step: 1652, loss is 0.015049859881401062\n",
      "epoch: 3 step: 1653, loss is 0.03984034061431885\n",
      "epoch: 3 step: 1654, loss is 0.014059138484299183\n",
      "epoch: 3 step: 1655, loss is 0.018989048898220062\n",
      "epoch: 3 step: 1656, loss is 0.005038038361817598\n",
      "epoch: 3 step: 1657, loss is 0.04873744398355484\n",
      "epoch: 3 step: 1658, loss is 0.0009280721424147487\n",
      "epoch: 3 step: 1659, loss is 0.005906735081225634\n",
      "epoch: 3 step: 1660, loss is 0.006829281337559223\n",
      "epoch: 3 step: 1661, loss is 0.022062530741095543\n",
      "epoch: 3 step: 1662, loss is 0.00927612092345953\n",
      "epoch: 3 step: 1663, loss is 0.003218661993741989\n",
      "epoch: 3 step: 1664, loss is 0.06271916627883911\n",
      "epoch: 3 step: 1665, loss is 0.00463807163760066\n",
      "epoch: 3 step: 1666, loss is 0.043580375611782074\n",
      "epoch: 3 step: 1667, loss is 0.053745463490486145\n",
      "epoch: 3 step: 1668, loss is 0.0032122195698320866\n",
      "epoch: 3 step: 1669, loss is 0.0006318808300420642\n",
      "epoch: 3 step: 1670, loss is 0.016290651634335518\n",
      "epoch: 3 step: 1671, loss is 0.0013722932199016213\n",
      "epoch: 3 step: 1672, loss is 0.12619492411613464\n",
      "epoch: 3 step: 1673, loss is 0.0015762404073029757\n",
      "epoch: 3 step: 1674, loss is 0.008723543956875801\n",
      "epoch: 3 step: 1675, loss is 0.0017779468325898051\n",
      "epoch: 3 step: 1676, loss is 0.002721576951444149\n",
      "epoch: 3 step: 1677, loss is 0.001161949709057808\n",
      "epoch: 3 step: 1678, loss is 0.017064686864614487\n",
      "epoch: 3 step: 1679, loss is 0.0567844994366169\n",
      "epoch: 3 step: 1680, loss is 0.22336716949939728\n",
      "epoch: 3 step: 1681, loss is 0.01956603117287159\n",
      "epoch: 3 step: 1682, loss is 0.0014112329808995128\n",
      "epoch: 3 step: 1683, loss is 0.04644881561398506\n",
      "epoch: 3 step: 1684, loss is 0.02276606112718582\n",
      "epoch: 3 step: 1685, loss is 0.058973923325538635\n",
      "epoch: 3 step: 1686, loss is 0.0013181731337681413\n",
      "epoch: 3 step: 1687, loss is 0.0027502698358148336\n",
      "epoch: 3 step: 1688, loss is 0.0017200878355652094\n",
      "epoch: 3 step: 1689, loss is 0.13900938630104065\n",
      "epoch: 3 step: 1690, loss is 0.009029902517795563\n",
      "epoch: 3 step: 1691, loss is 0.06428936868906021\n",
      "epoch: 3 step: 1692, loss is 0.08315552026033401\n",
      "epoch: 3 step: 1693, loss is 0.06559182703495026\n",
      "epoch: 3 step: 1694, loss is 0.14701274037361145\n",
      "epoch: 3 step: 1695, loss is 0.21807216107845306\n",
      "epoch: 3 step: 1696, loss is 0.006077158730477095\n",
      "epoch: 3 step: 1697, loss is 0.001307023921981454\n",
      "epoch: 3 step: 1698, loss is 0.1452154517173767\n",
      "epoch: 3 step: 1699, loss is 0.0008686917717568576\n",
      "epoch: 3 step: 1700, loss is 0.0054967268370091915\n",
      "epoch: 3 step: 1701, loss is 0.04682723060250282\n",
      "epoch: 3 step: 1702, loss is 0.07600163668394089\n",
      "epoch: 3 step: 1703, loss is 0.09451375156641006\n",
      "epoch: 3 step: 1704, loss is 0.0017062124097719789\n",
      "epoch: 3 step: 1705, loss is 0.02655588649213314\n",
      "epoch: 3 step: 1706, loss is 0.011413658037781715\n",
      "epoch: 3 step: 1707, loss is 0.07719490677118301\n",
      "epoch: 3 step: 1708, loss is 0.05734691768884659\n",
      "epoch: 3 step: 1709, loss is 0.22108003497123718\n",
      "epoch: 3 step: 1710, loss is 0.032936904579401016\n",
      "epoch: 3 step: 1711, loss is 0.06729111820459366\n",
      "epoch: 3 step: 1712, loss is 0.09403178840875626\n",
      "epoch: 3 step: 1713, loss is 0.017264794558286667\n",
      "epoch: 3 step: 1714, loss is 0.0014429658185690641\n",
      "epoch: 3 step: 1715, loss is 0.11042392253875732\n",
      "epoch: 3 step: 1716, loss is 0.2198866307735443\n",
      "epoch: 3 step: 1717, loss is 0.007550213020294905\n",
      "epoch: 3 step: 1718, loss is 0.0038501194212585688\n",
      "epoch: 3 step: 1719, loss is 0.016375698149204254\n",
      "epoch: 3 step: 1720, loss is 0.14149253070354462\n",
      "epoch: 3 step: 1721, loss is 0.03529070317745209\n",
      "epoch: 3 step: 1722, loss is 0.11738231033086777\n",
      "epoch: 3 step: 1723, loss is 0.2188524752855301\n",
      "epoch: 3 step: 1724, loss is 0.03846781328320503\n",
      "epoch: 3 step: 1725, loss is 0.09824888408184052\n",
      "epoch: 3 step: 1726, loss is 0.1258956789970398\n",
      "epoch: 3 step: 1727, loss is 0.07162397354841232\n",
      "epoch: 3 step: 1728, loss is 0.0300784632563591\n",
      "epoch: 3 step: 1729, loss is 0.02068578265607357\n",
      "epoch: 3 step: 1730, loss is 0.010375747457146645\n",
      "epoch: 3 step: 1731, loss is 0.009762471541762352\n",
      "epoch: 3 step: 1732, loss is 0.013657414354383945\n",
      "epoch: 3 step: 1733, loss is 0.004818769637495279\n",
      "epoch: 3 step: 1734, loss is 0.0010985585395246744\n",
      "epoch: 3 step: 1735, loss is 0.08937164396047592\n",
      "epoch: 3 step: 1736, loss is 0.13511842489242554\n",
      "epoch: 3 step: 1737, loss is 0.08912477642297745\n",
      "epoch: 3 step: 1738, loss is 0.10123662650585175\n",
      "epoch: 3 step: 1739, loss is 0.027460671961307526\n",
      "epoch: 3 step: 1740, loss is 0.00252802693285048\n",
      "epoch: 3 step: 1741, loss is 0.1211506724357605\n",
      "epoch: 3 step: 1742, loss is 0.006627836264669895\n",
      "epoch: 3 step: 1743, loss is 0.19669459760189056\n",
      "epoch: 3 step: 1744, loss is 0.0290468018501997\n",
      "epoch: 3 step: 1745, loss is 0.08580245822668076\n",
      "epoch: 3 step: 1746, loss is 0.021274084225296974\n",
      "epoch: 3 step: 1747, loss is 0.07124309241771698\n",
      "epoch: 3 step: 1748, loss is 0.030040143057703972\n",
      "epoch: 3 step: 1749, loss is 0.0029650989454239607\n",
      "epoch: 3 step: 1750, loss is 0.015776045620441437\n",
      "epoch: 3 step: 1751, loss is 0.016934357583522797\n",
      "epoch: 3 step: 1752, loss is 0.004804888274520636\n",
      "epoch: 3 step: 1753, loss is 0.027574289590120316\n",
      "epoch: 3 step: 1754, loss is 0.007809908129274845\n",
      "epoch: 3 step: 1755, loss is 0.0035250154323875904\n",
      "epoch: 3 step: 1756, loss is 0.011063560843467712\n",
      "epoch: 3 step: 1757, loss is 0.025370847433805466\n",
      "epoch: 3 step: 1758, loss is 0.001845505554229021\n",
      "epoch: 3 step: 1759, loss is 0.010047146119177341\n",
      "epoch: 3 step: 1760, loss is 0.274259477853775\n",
      "epoch: 3 step: 1761, loss is 0.019873501732945442\n",
      "epoch: 3 step: 1762, loss is 0.04802291467785835\n",
      "epoch: 3 step: 1763, loss is 0.0038436995819211006\n",
      "epoch: 3 step: 1764, loss is 0.01669994927942753\n",
      "epoch: 3 step: 1765, loss is 0.12818318605422974\n",
      "epoch: 3 step: 1766, loss is 0.2062724530696869\n",
      "epoch: 3 step: 1767, loss is 0.010978317819535732\n",
      "epoch: 3 step: 1768, loss is 0.1916966736316681\n",
      "epoch: 3 step: 1769, loss is 0.08023254573345184\n",
      "epoch: 3 step: 1770, loss is 0.008711998350918293\n",
      "epoch: 3 step: 1771, loss is 0.09948708117008209\n",
      "epoch: 3 step: 1772, loss is 0.000667053391225636\n",
      "epoch: 3 step: 1773, loss is 0.01548027340322733\n",
      "epoch: 3 step: 1774, loss is 0.014696425758302212\n",
      "epoch: 3 step: 1775, loss is 0.19860386848449707\n",
      "epoch: 3 step: 1776, loss is 0.01906515844166279\n",
      "epoch: 3 step: 1777, loss is 0.04139705374836922\n",
      "epoch: 3 step: 1778, loss is 0.024360759183764458\n",
      "epoch: 3 step: 1779, loss is 0.00293840398080647\n",
      "epoch: 3 step: 1780, loss is 0.06198103353381157\n",
      "epoch: 3 step: 1781, loss is 0.0905471071600914\n",
      "epoch: 3 step: 1782, loss is 0.019062010571360588\n",
      "epoch: 3 step: 1783, loss is 0.012615922838449478\n",
      "epoch: 3 step: 1784, loss is 0.006815518718212843\n",
      "epoch: 3 step: 1785, loss is 0.03592395782470703\n",
      "epoch: 3 step: 1786, loss is 0.05890890955924988\n",
      "epoch: 3 step: 1787, loss is 0.010539060458540916\n",
      "epoch: 3 step: 1788, loss is 0.007110642734915018\n",
      "epoch: 3 step: 1789, loss is 0.008130660280585289\n",
      "epoch: 3 step: 1790, loss is 0.001799950608983636\n",
      "epoch: 3 step: 1791, loss is 0.030769258737564087\n",
      "epoch: 3 step: 1792, loss is 0.0019137831404805183\n",
      "epoch: 3 step: 1793, loss is 0.002288990654051304\n",
      "epoch: 3 step: 1794, loss is 0.016552545130252838\n",
      "epoch: 3 step: 1795, loss is 0.07715949416160583\n",
      "epoch: 3 step: 1796, loss is 0.02789956144988537\n",
      "epoch: 3 step: 1797, loss is 0.0033800331875681877\n",
      "epoch: 3 step: 1798, loss is 0.0017664155457168818\n",
      "epoch: 3 step: 1799, loss is 0.002082293387502432\n",
      "epoch: 3 step: 1800, loss is 0.005509279668331146\n",
      "epoch: 3 step: 1801, loss is 0.04197325557470322\n",
      "epoch: 3 step: 1802, loss is 0.010751897469162941\n",
      "epoch: 3 step: 1803, loss is 0.0020736935548484325\n",
      "epoch: 3 step: 1804, loss is 0.0005497485981322825\n",
      "epoch: 3 step: 1805, loss is 0.007839778438210487\n",
      "epoch: 3 step: 1806, loss is 0.02370651252567768\n",
      "epoch: 3 step: 1807, loss is 0.05164331942796707\n",
      "epoch: 3 step: 1808, loss is 0.0029485716950148344\n",
      "epoch: 3 step: 1809, loss is 0.12988843023777008\n",
      "epoch: 3 step: 1810, loss is 0.004798573441803455\n",
      "epoch: 3 step: 1811, loss is 0.09371379762887955\n",
      "epoch: 3 step: 1812, loss is 0.007830987684428692\n",
      "epoch: 3 step: 1813, loss is 0.1592380702495575\n",
      "epoch: 3 step: 1814, loss is 0.006055181846022606\n",
      "epoch: 3 step: 1815, loss is 0.006818362511694431\n",
      "epoch: 3 step: 1816, loss is 0.005352510139346123\n",
      "epoch: 3 step: 1817, loss is 0.001757283927872777\n",
      "epoch: 3 step: 1818, loss is 0.0019360712030902505\n",
      "epoch: 3 step: 1819, loss is 0.08878031373023987\n",
      "epoch: 3 step: 1820, loss is 0.025363218039274216\n",
      "epoch: 3 step: 1821, loss is 0.047886721789836884\n",
      "epoch: 3 step: 1822, loss is 0.1125810295343399\n",
      "epoch: 3 step: 1823, loss is 0.005179787985980511\n",
      "epoch: 3 step: 1824, loss is 0.018205394968390465\n",
      "epoch: 3 step: 1825, loss is 0.04040055349469185\n",
      "epoch: 3 step: 1826, loss is 0.009657198563218117\n",
      "epoch: 3 step: 1827, loss is 0.05734657123684883\n",
      "epoch: 3 step: 1828, loss is 0.00201632222160697\n",
      "epoch: 3 step: 1829, loss is 0.0015463262097910047\n",
      "epoch: 3 step: 1830, loss is 0.01044559758156538\n",
      "epoch: 3 step: 1831, loss is 0.09496535360813141\n",
      "epoch: 3 step: 1832, loss is 0.07259618490934372\n",
      "epoch: 3 step: 1833, loss is 0.0013296484248712659\n",
      "epoch: 3 step: 1834, loss is 0.16687478125095367\n",
      "epoch: 3 step: 1835, loss is 0.0077553605660796165\n",
      "epoch: 3 step: 1836, loss is 0.015625474974513054\n",
      "epoch: 3 step: 1837, loss is 0.044441793113946915\n",
      "epoch: 3 step: 1838, loss is 0.0033881973940879107\n",
      "epoch: 3 step: 1839, loss is 0.007727883290499449\n",
      "epoch: 3 step: 1840, loss is 0.08843334764242172\n",
      "epoch: 3 step: 1841, loss is 0.0013046973617747426\n",
      "epoch: 3 step: 1842, loss is 0.0013804236659780145\n",
      "epoch: 3 step: 1843, loss is 0.0012818003306165338\n",
      "epoch: 3 step: 1844, loss is 0.06438473612070084\n",
      "epoch: 3 step: 1845, loss is 0.00919537153095007\n",
      "epoch: 3 step: 1846, loss is 0.05859818309545517\n",
      "epoch: 3 step: 1847, loss is 0.0013978565111756325\n",
      "epoch: 3 step: 1848, loss is 0.0003843105514533818\n",
      "epoch: 3 step: 1849, loss is 0.07282312959432602\n",
      "epoch: 3 step: 1850, loss is 0.004218660295009613\n",
      "epoch: 3 step: 1851, loss is 0.0020409829448908567\n",
      "epoch: 3 step: 1852, loss is 0.02230648882687092\n",
      "epoch: 3 step: 1853, loss is 0.20439413189888\n",
      "epoch: 3 step: 1854, loss is 0.0056063272058963776\n",
      "epoch: 3 step: 1855, loss is 0.08589528501033783\n",
      "epoch: 3 step: 1856, loss is 0.006852297578006983\n",
      "epoch: 3 step: 1857, loss is 0.26637205481529236\n",
      "epoch: 3 step: 1858, loss is 0.0008955617086030543\n",
      "epoch: 3 step: 1859, loss is 0.0002598016581032425\n",
      "epoch: 3 step: 1860, loss is 0.0029861859511584044\n",
      "epoch: 3 step: 1861, loss is 0.0034255555365234613\n",
      "epoch: 3 step: 1862, loss is 0.010778284631669521\n",
      "epoch: 3 step: 1863, loss is 0.3720751404762268\n",
      "epoch: 3 step: 1864, loss is 0.0023487256839871407\n",
      "epoch: 3 step: 1865, loss is 0.08505614846944809\n",
      "epoch: 3 step: 1866, loss is 0.005803682375699282\n",
      "epoch: 3 step: 1867, loss is 0.07104101777076721\n",
      "epoch: 3 step: 1868, loss is 0.010629040189087391\n",
      "epoch: 3 step: 1869, loss is 0.05270237475633621\n",
      "epoch: 3 step: 1870, loss is 0.012203095480799675\n",
      "epoch: 3 step: 1871, loss is 0.282518208026886\n",
      "epoch: 3 step: 1872, loss is 0.025736508890986443\n",
      "epoch: 3 step: 1873, loss is 0.050666481256484985\n",
      "epoch: 3 step: 1874, loss is 0.00674571143463254\n",
      "epoch: 3 step: 1875, loss is 0.127663716673851\n",
      "epoch: 4 step: 1, loss is 0.00042765954276546836\n",
      "epoch: 4 step: 2, loss is 0.03595510497689247\n",
      "epoch: 4 step: 3, loss is 0.0016420910833403468\n",
      "epoch: 4 step: 4, loss is 0.0016852474072948098\n",
      "epoch: 4 step: 5, loss is 0.026416869834065437\n",
      "epoch: 4 step: 6, loss is 0.033512964844703674\n",
      "epoch: 4 step: 7, loss is 0.002613001735880971\n",
      "epoch: 4 step: 8, loss is 0.0663871094584465\n",
      "epoch: 4 step: 9, loss is 0.03429044410586357\n",
      "epoch: 4 step: 10, loss is 0.015400264412164688\n",
      "epoch: 4 step: 11, loss is 0.0626232698559761\n",
      "epoch: 4 step: 12, loss is 0.009798569604754448\n",
      "epoch: 4 step: 13, loss is 0.0417596735060215\n",
      "epoch: 4 step: 14, loss is 0.0028576466720551252\n",
      "epoch: 4 step: 15, loss is 0.0037360917776823044\n",
      "epoch: 4 step: 16, loss is 0.020391838625073433\n",
      "epoch: 4 step: 17, loss is 0.004132791887968779\n",
      "epoch: 4 step: 18, loss is 0.06095442920923233\n",
      "epoch: 4 step: 19, loss is 0.022560428828001022\n",
      "epoch: 4 step: 20, loss is 0.07308053225278854\n",
      "epoch: 4 step: 21, loss is 0.046198125928640366\n",
      "epoch: 4 step: 22, loss is 0.05317377299070358\n",
      "epoch: 4 step: 23, loss is 0.13456617295742035\n",
      "epoch: 4 step: 24, loss is 0.13172553479671478\n",
      "epoch: 4 step: 25, loss is 0.05214254930615425\n",
      "epoch: 4 step: 26, loss is 0.04241003841161728\n",
      "epoch: 4 step: 27, loss is 0.0016228680033236742\n",
      "epoch: 4 step: 28, loss is 0.007013818249106407\n",
      "epoch: 4 step: 29, loss is 0.0030370629392564297\n",
      "epoch: 4 step: 30, loss is 0.08952582627534866\n",
      "epoch: 4 step: 31, loss is 0.11336483806371689\n",
      "epoch: 4 step: 32, loss is 0.004774648696184158\n",
      "epoch: 4 step: 33, loss is 0.0006316899671219289\n",
      "epoch: 4 step: 34, loss is 0.02593879960477352\n",
      "epoch: 4 step: 35, loss is 0.18037325143814087\n",
      "epoch: 4 step: 36, loss is 0.008809957653284073\n",
      "epoch: 4 step: 37, loss is 0.011682417243719101\n",
      "epoch: 4 step: 38, loss is 0.002838867250829935\n",
      "epoch: 4 step: 39, loss is 0.0008891172474250197\n",
      "epoch: 4 step: 40, loss is 0.0926375538110733\n",
      "epoch: 4 step: 41, loss is 0.05444161221385002\n",
      "epoch: 4 step: 42, loss is 0.011923015117645264\n",
      "epoch: 4 step: 43, loss is 0.02287064678966999\n",
      "epoch: 4 step: 44, loss is 0.005060548894107342\n",
      "epoch: 4 step: 45, loss is 0.002323622116819024\n",
      "epoch: 4 step: 46, loss is 0.0019925828091800213\n",
      "epoch: 4 step: 47, loss is 0.0056093311868608\n",
      "epoch: 4 step: 48, loss is 0.0034011665266007185\n",
      "epoch: 4 step: 49, loss is 0.0009898574789986014\n",
      "epoch: 4 step: 50, loss is 0.0028240643441677094\n",
      "epoch: 4 step: 51, loss is 0.055803876370191574\n",
      "epoch: 4 step: 52, loss is 0.028275316581130028\n",
      "epoch: 4 step: 53, loss is 0.0052750022150576115\n",
      "epoch: 4 step: 54, loss is 0.014880271628499031\n",
      "epoch: 4 step: 55, loss is 0.0004787658399436623\n",
      "epoch: 4 step: 56, loss is 0.08641837537288666\n",
      "epoch: 4 step: 57, loss is 0.002065664390102029\n",
      "epoch: 4 step: 58, loss is 0.10195150226354599\n",
      "epoch: 4 step: 59, loss is 0.04668442904949188\n",
      "epoch: 4 step: 60, loss is 0.0018018557457253337\n",
      "epoch: 4 step: 61, loss is 0.0011574962409213185\n",
      "epoch: 4 step: 62, loss is 0.0032462626695632935\n",
      "epoch: 4 step: 63, loss is 0.08780135959386826\n",
      "epoch: 4 step: 64, loss is 0.004397661425173283\n",
      "epoch: 4 step: 65, loss is 0.004150041379034519\n",
      "epoch: 4 step: 66, loss is 0.020236488431692123\n",
      "epoch: 4 step: 67, loss is 0.1046808585524559\n",
      "epoch: 4 step: 68, loss is 0.008865613490343094\n",
      "epoch: 4 step: 69, loss is 0.020114874467253685\n",
      "epoch: 4 step: 70, loss is 0.007486515212804079\n",
      "epoch: 4 step: 71, loss is 0.014091568067669868\n",
      "epoch: 4 step: 72, loss is 0.027004146948456764\n",
      "epoch: 4 step: 73, loss is 0.04424364119768143\n",
      "epoch: 4 step: 74, loss is 0.07257313281297684\n",
      "epoch: 4 step: 75, loss is 0.0178841445595026\n",
      "epoch: 4 step: 76, loss is 0.012078742496669292\n",
      "epoch: 4 step: 77, loss is 0.014856823720037937\n",
      "epoch: 4 step: 78, loss is 0.003002873621881008\n",
      "epoch: 4 step: 79, loss is 0.1049070805311203\n",
      "epoch: 4 step: 80, loss is 0.1878814399242401\n",
      "epoch: 4 step: 81, loss is 0.029676735401153564\n",
      "epoch: 4 step: 82, loss is 0.009128171019256115\n",
      "epoch: 4 step: 83, loss is 0.005390825681388378\n",
      "epoch: 4 step: 84, loss is 0.0028220461681485176\n",
      "epoch: 4 step: 85, loss is 0.0014037725050002337\n",
      "epoch: 4 step: 86, loss is 0.0012819536495953798\n",
      "epoch: 4 step: 87, loss is 0.0035275062546133995\n",
      "epoch: 4 step: 88, loss is 0.02067619562149048\n",
      "epoch: 4 step: 89, loss is 0.047418396919965744\n",
      "epoch: 4 step: 90, loss is 0.11298266798257828\n",
      "epoch: 4 step: 91, loss is 0.0018400212284177542\n",
      "epoch: 4 step: 92, loss is 0.027837563306093216\n",
      "epoch: 4 step: 93, loss is 0.016932209953665733\n",
      "epoch: 4 step: 94, loss is 0.0012991124531254172\n",
      "epoch: 4 step: 95, loss is 0.04497566819190979\n",
      "epoch: 4 step: 96, loss is 0.0018697447376325727\n",
      "epoch: 4 step: 97, loss is 0.01462737750262022\n",
      "epoch: 4 step: 98, loss is 0.003808444831520319\n",
      "epoch: 4 step: 99, loss is 0.12183631211519241\n",
      "epoch: 4 step: 100, loss is 0.13352398574352264\n",
      "epoch: 4 step: 101, loss is 0.0007782594184391201\n",
      "epoch: 4 step: 102, loss is 0.01915750652551651\n",
      "epoch: 4 step: 103, loss is 0.0032735979184508324\n",
      "epoch: 4 step: 104, loss is 0.020732944831252098\n",
      "epoch: 4 step: 105, loss is 0.04712468758225441\n",
      "epoch: 4 step: 106, loss is 0.004323107656091452\n",
      "epoch: 4 step: 107, loss is 0.007494875229895115\n",
      "epoch: 4 step: 108, loss is 0.003060102928429842\n",
      "epoch: 4 step: 109, loss is 0.040047723799943924\n",
      "epoch: 4 step: 110, loss is 0.018023043870925903\n",
      "epoch: 4 step: 111, loss is 0.002556538674980402\n",
      "epoch: 4 step: 112, loss is 0.012545584701001644\n",
      "epoch: 4 step: 113, loss is 0.03073502890765667\n",
      "epoch: 4 step: 114, loss is 0.004866911564022303\n",
      "epoch: 4 step: 115, loss is 0.08535484224557877\n",
      "epoch: 4 step: 116, loss is 0.005883742589503527\n",
      "epoch: 4 step: 117, loss is 0.002903276588767767\n",
      "epoch: 4 step: 118, loss is 0.0006442418089136481\n",
      "epoch: 4 step: 119, loss is 0.2193884402513504\n",
      "epoch: 4 step: 120, loss is 0.08054244518280029\n",
      "epoch: 4 step: 121, loss is 0.0016987361013889313\n",
      "epoch: 4 step: 122, loss is 0.005444500129669905\n",
      "epoch: 4 step: 123, loss is 0.03729242831468582\n",
      "epoch: 4 step: 124, loss is 0.0026853966992348433\n",
      "epoch: 4 step: 125, loss is 0.06859467178583145\n",
      "epoch: 4 step: 126, loss is 0.02830127254128456\n",
      "epoch: 4 step: 127, loss is 0.005815271288156509\n",
      "epoch: 4 step: 128, loss is 0.0018852956127375364\n",
      "epoch: 4 step: 129, loss is 0.00281449593603611\n",
      "epoch: 4 step: 130, loss is 0.0027794125489890575\n",
      "epoch: 4 step: 131, loss is 0.004086408298462629\n",
      "epoch: 4 step: 132, loss is 0.005103436764329672\n",
      "epoch: 4 step: 133, loss is 0.016745399683713913\n",
      "epoch: 4 step: 134, loss is 0.00040018162690103054\n",
      "epoch: 4 step: 135, loss is 0.0003478668222669512\n",
      "epoch: 4 step: 136, loss is 0.011666283011436462\n",
      "epoch: 4 step: 137, loss is 0.0010219323448836803\n",
      "epoch: 4 step: 138, loss is 0.02116643451154232\n",
      "epoch: 4 step: 139, loss is 0.001928363461047411\n",
      "epoch: 4 step: 140, loss is 0.04359108582139015\n",
      "epoch: 4 step: 141, loss is 0.0002836163330357522\n",
      "epoch: 4 step: 142, loss is 0.14608114957809448\n",
      "epoch: 4 step: 143, loss is 0.1337534338235855\n",
      "epoch: 4 step: 144, loss is 0.11756828427314758\n",
      "epoch: 4 step: 145, loss is 0.0009096765425056219\n",
      "epoch: 4 step: 146, loss is 0.006789826322346926\n",
      "epoch: 4 step: 147, loss is 0.00047816865844652057\n",
      "epoch: 4 step: 148, loss is 0.013520056381821632\n",
      "epoch: 4 step: 149, loss is 0.0042982506565749645\n",
      "epoch: 4 step: 150, loss is 8.959796832641587e-05\n",
      "epoch: 4 step: 151, loss is 0.006955684628337622\n",
      "epoch: 4 step: 152, loss is 0.014342487789690495\n",
      "epoch: 4 step: 153, loss is 0.007962537929415703\n",
      "epoch: 4 step: 154, loss is 0.014839371666312218\n",
      "epoch: 4 step: 155, loss is 0.011170742101967335\n",
      "epoch: 4 step: 156, loss is 0.14376629889011383\n",
      "epoch: 4 step: 157, loss is 0.20711363852024078\n",
      "epoch: 4 step: 158, loss is 0.3533223271369934\n",
      "epoch: 4 step: 159, loss is 0.004976022057235241\n",
      "epoch: 4 step: 160, loss is 0.005934566725045443\n",
      "epoch: 4 step: 161, loss is 0.047003477811813354\n",
      "epoch: 4 step: 162, loss is 0.05482161417603493\n",
      "epoch: 4 step: 163, loss is 0.0008444489212706685\n",
      "epoch: 4 step: 164, loss is 0.0012142355553805828\n",
      "epoch: 4 step: 165, loss is 0.11770278960466385\n",
      "epoch: 4 step: 166, loss is 0.003386338707059622\n",
      "epoch: 4 step: 167, loss is 0.004502961412072182\n",
      "epoch: 4 step: 168, loss is 0.029812857508659363\n",
      "epoch: 4 step: 169, loss is 0.07005934417247772\n",
      "epoch: 4 step: 170, loss is 0.002037130994722247\n",
      "epoch: 4 step: 171, loss is 0.028902757912874222\n",
      "epoch: 4 step: 172, loss is 0.004339476116001606\n",
      "epoch: 4 step: 173, loss is 0.12589262425899506\n",
      "epoch: 4 step: 174, loss is 0.03264014050364494\n",
      "epoch: 4 step: 175, loss is 0.10562320798635483\n",
      "epoch: 4 step: 176, loss is 0.1659608781337738\n",
      "epoch: 4 step: 177, loss is 0.0011723552597686648\n",
      "epoch: 4 step: 178, loss is 0.0005848893197253346\n",
      "epoch: 4 step: 179, loss is 0.0013007092056795955\n",
      "epoch: 4 step: 180, loss is 0.0027391035109758377\n",
      "epoch: 4 step: 181, loss is 0.005092623643577099\n",
      "epoch: 4 step: 182, loss is 0.07847745716571808\n",
      "epoch: 4 step: 183, loss is 0.0015339298406615853\n",
      "epoch: 4 step: 184, loss is 0.010218916460871696\n",
      "epoch: 4 step: 185, loss is 0.03301588445901871\n",
      "epoch: 4 step: 186, loss is 0.0023910708259791136\n",
      "epoch: 4 step: 187, loss is 0.05271843820810318\n",
      "epoch: 4 step: 188, loss is 0.003347084391862154\n",
      "epoch: 4 step: 189, loss is 0.008843525312840939\n",
      "epoch: 4 step: 190, loss is 0.04746994376182556\n",
      "epoch: 4 step: 191, loss is 0.08313411474227905\n",
      "epoch: 4 step: 192, loss is 0.007800104096531868\n",
      "epoch: 4 step: 193, loss is 0.23819082975387573\n",
      "epoch: 4 step: 194, loss is 0.16128849983215332\n",
      "epoch: 4 step: 195, loss is 0.021391304209828377\n",
      "epoch: 4 step: 196, loss is 0.0003152028366457671\n",
      "epoch: 4 step: 197, loss is 0.00037246194551698864\n",
      "epoch: 4 step: 198, loss is 0.1897265464067459\n",
      "epoch: 4 step: 199, loss is 0.0035211031790822744\n",
      "epoch: 4 step: 200, loss is 0.010035271756350994\n",
      "epoch: 4 step: 201, loss is 0.005678559187799692\n",
      "epoch: 4 step: 202, loss is 0.0018798904493451118\n",
      "epoch: 4 step: 203, loss is 0.09310542047023773\n",
      "epoch: 4 step: 204, loss is 0.040775150060653687\n",
      "epoch: 4 step: 205, loss is 0.00036881089909002185\n",
      "epoch: 4 step: 206, loss is 0.0028290357440710068\n",
      "epoch: 4 step: 207, loss is 0.020474381744861603\n",
      "epoch: 4 step: 208, loss is 0.06769262999296188\n",
      "epoch: 4 step: 209, loss is 0.00648223701864481\n",
      "epoch: 4 step: 210, loss is 0.021679937839508057\n",
      "epoch: 4 step: 211, loss is 0.01672070100903511\n",
      "epoch: 4 step: 212, loss is 0.06465797126293182\n",
      "epoch: 4 step: 213, loss is 0.0025898427702486515\n",
      "epoch: 4 step: 214, loss is 0.13649721443653107\n",
      "epoch: 4 step: 215, loss is 0.09234466403722763\n",
      "epoch: 4 step: 216, loss is 0.0035652811639010906\n",
      "epoch: 4 step: 217, loss is 0.05003369227051735\n",
      "epoch: 4 step: 218, loss is 0.006786503363400698\n",
      "epoch: 4 step: 219, loss is 0.003052184823900461\n",
      "epoch: 4 step: 220, loss is 0.0014978820690885186\n",
      "epoch: 4 step: 221, loss is 0.007801421452313662\n",
      "epoch: 4 step: 222, loss is 0.042623065412044525\n",
      "epoch: 4 step: 223, loss is 0.01259590219706297\n",
      "epoch: 4 step: 224, loss is 0.008967163041234016\n",
      "epoch: 4 step: 225, loss is 0.010612104088068008\n",
      "epoch: 4 step: 226, loss is 0.0013536815531551838\n",
      "epoch: 4 step: 227, loss is 0.004836773034185171\n",
      "epoch: 4 step: 228, loss is 0.03572091832756996\n",
      "epoch: 4 step: 229, loss is 0.027324538677930832\n",
      "epoch: 4 step: 230, loss is 0.0008480640244670212\n",
      "epoch: 4 step: 231, loss is 0.03658873587846756\n",
      "epoch: 4 step: 232, loss is 0.035832494497299194\n",
      "epoch: 4 step: 233, loss is 0.0018446166068315506\n",
      "epoch: 4 step: 234, loss is 0.02743072248995304\n",
      "epoch: 4 step: 235, loss is 0.09582946449518204\n",
      "epoch: 4 step: 236, loss is 0.0017798723420128226\n",
      "epoch: 4 step: 237, loss is 0.03214764595031738\n",
      "epoch: 4 step: 238, loss is 0.002304781461134553\n",
      "epoch: 4 step: 239, loss is 0.10781820118427277\n",
      "epoch: 4 step: 240, loss is 0.0035984013229608536\n",
      "epoch: 4 step: 241, loss is 0.00396369956433773\n",
      "epoch: 4 step: 242, loss is 0.002701158169656992\n",
      "epoch: 4 step: 243, loss is 0.01431916281580925\n",
      "epoch: 4 step: 244, loss is 0.12244509160518646\n",
      "epoch: 4 step: 245, loss is 0.0003502814215607941\n",
      "epoch: 4 step: 246, loss is 0.011806434951722622\n",
      "epoch: 4 step: 247, loss is 0.004531309474259615\n",
      "epoch: 4 step: 248, loss is 0.004061778075993061\n",
      "epoch: 4 step: 249, loss is 0.1222667545080185\n",
      "epoch: 4 step: 250, loss is 0.055494263768196106\n",
      "epoch: 4 step: 251, loss is 0.012053868733346462\n",
      "epoch: 4 step: 252, loss is 0.006520448252558708\n",
      "epoch: 4 step: 253, loss is 0.013560937717556953\n",
      "epoch: 4 step: 254, loss is 0.003220638958737254\n",
      "epoch: 4 step: 255, loss is 0.0008393077878281474\n",
      "epoch: 4 step: 256, loss is 0.0005260143661871552\n",
      "epoch: 4 step: 257, loss is 0.08748534321784973\n",
      "epoch: 4 step: 258, loss is 0.008536718785762787\n",
      "epoch: 4 step: 259, loss is 0.0007325010374188423\n",
      "epoch: 4 step: 260, loss is 0.010228574275970459\n",
      "epoch: 4 step: 261, loss is 0.0032360947225242853\n",
      "epoch: 4 step: 262, loss is 0.004597620107233524\n",
      "epoch: 4 step: 263, loss is 0.006578176748007536\n",
      "epoch: 4 step: 264, loss is 0.021257851272821426\n",
      "epoch: 4 step: 265, loss is 0.10723404586315155\n",
      "epoch: 4 step: 266, loss is 0.031206952407956123\n",
      "epoch: 4 step: 267, loss is 0.12496386468410492\n",
      "epoch: 4 step: 268, loss is 0.006082660984247923\n",
      "epoch: 4 step: 269, loss is 0.0013321704464033246\n",
      "epoch: 4 step: 270, loss is 0.0038388746324926615\n",
      "epoch: 4 step: 271, loss is 0.00039030960761010647\n",
      "epoch: 4 step: 272, loss is 0.010466021485626698\n",
      "epoch: 4 step: 273, loss is 0.001311990199610591\n",
      "epoch: 4 step: 274, loss is 0.03596588969230652\n",
      "epoch: 4 step: 275, loss is 0.006190016865730286\n",
      "epoch: 4 step: 276, loss is 0.011188083328306675\n",
      "epoch: 4 step: 277, loss is 0.2454070746898651\n",
      "epoch: 4 step: 278, loss is 0.0025772058870643377\n",
      "epoch: 4 step: 279, loss is 0.06507651507854462\n",
      "epoch: 4 step: 280, loss is 0.0068467967212200165\n",
      "epoch: 4 step: 281, loss is 0.013525254093110561\n",
      "epoch: 4 step: 282, loss is 0.024753905832767487\n",
      "epoch: 4 step: 283, loss is 0.18791405856609344\n",
      "epoch: 4 step: 284, loss is 0.03730520233511925\n",
      "epoch: 4 step: 285, loss is 0.0054950229823589325\n",
      "epoch: 4 step: 286, loss is 0.0071350387297570705\n",
      "epoch: 4 step: 287, loss is 0.011878715828061104\n",
      "epoch: 4 step: 288, loss is 0.03703194856643677\n",
      "epoch: 4 step: 289, loss is 0.03704320639371872\n",
      "epoch: 4 step: 290, loss is 0.000359330209903419\n",
      "epoch: 4 step: 291, loss is 0.05233350396156311\n",
      "epoch: 4 step: 292, loss is 0.13723304867744446\n",
      "epoch: 4 step: 293, loss is 0.002930570160970092\n",
      "epoch: 4 step: 294, loss is 0.047299277037382126\n",
      "epoch: 4 step: 295, loss is 0.011448844335973263\n",
      "epoch: 4 step: 296, loss is 0.0200467798858881\n",
      "epoch: 4 step: 297, loss is 0.0021090793889015913\n",
      "epoch: 4 step: 298, loss is 0.001987166702747345\n",
      "epoch: 4 step: 299, loss is 0.012392755597829819\n",
      "epoch: 4 step: 300, loss is 0.026492945849895477\n",
      "epoch: 4 step: 301, loss is 0.0027613546699285507\n",
      "epoch: 4 step: 302, loss is 0.12395777553319931\n",
      "epoch: 4 step: 303, loss is 0.0018379633547738194\n",
      "epoch: 4 step: 304, loss is 0.0017822460504248738\n",
      "epoch: 4 step: 305, loss is 0.0012613825965672731\n",
      "epoch: 4 step: 306, loss is 0.04840945079922676\n",
      "epoch: 4 step: 307, loss is 0.24754036962985992\n",
      "epoch: 4 step: 308, loss is 0.017247354611754417\n",
      "epoch: 4 step: 309, loss is 0.20742151141166687\n",
      "epoch: 4 step: 310, loss is 0.0009546360815875232\n",
      "epoch: 4 step: 311, loss is 0.025126272812485695\n",
      "epoch: 4 step: 312, loss is 0.000656936492305249\n",
      "epoch: 4 step: 313, loss is 0.0005119353882037103\n",
      "epoch: 4 step: 314, loss is 0.05901382118463516\n",
      "epoch: 4 step: 315, loss is 0.015282699838280678\n",
      "epoch: 4 step: 316, loss is 0.07257002592086792\n",
      "epoch: 4 step: 317, loss is 0.126705139875412\n",
      "epoch: 4 step: 318, loss is 0.0003100376343354583\n",
      "epoch: 4 step: 319, loss is 0.04275856167078018\n",
      "epoch: 4 step: 320, loss is 0.0013983020326122642\n",
      "epoch: 4 step: 321, loss is 0.009583191014826298\n",
      "epoch: 4 step: 322, loss is 0.013238704763352871\n",
      "epoch: 4 step: 323, loss is 0.04778732359409332\n",
      "epoch: 4 step: 324, loss is 0.0004718056879937649\n",
      "epoch: 4 step: 325, loss is 0.0004791253595612943\n",
      "epoch: 4 step: 326, loss is 0.0008001235546544194\n",
      "epoch: 4 step: 327, loss is 0.0014033833285793662\n",
      "epoch: 4 step: 328, loss is 0.008232120424509048\n",
      "epoch: 4 step: 329, loss is 0.017067264765501022\n",
      "epoch: 4 step: 330, loss is 0.00319646461866796\n",
      "epoch: 4 step: 331, loss is 0.05316038057208061\n",
      "epoch: 4 step: 332, loss is 0.002992640482261777\n",
      "epoch: 4 step: 333, loss is 0.02516075409948826\n",
      "epoch: 4 step: 334, loss is 0.000490567646920681\n",
      "epoch: 4 step: 335, loss is 0.002179547678679228\n",
      "epoch: 4 step: 336, loss is 0.016811134293675423\n",
      "epoch: 4 step: 337, loss is 0.01966673694550991\n",
      "epoch: 4 step: 338, loss is 0.0019101910293102264\n",
      "epoch: 4 step: 339, loss is 0.009734352119266987\n",
      "epoch: 4 step: 340, loss is 0.0029902926180511713\n",
      "epoch: 4 step: 341, loss is 0.0028373822569847107\n",
      "epoch: 4 step: 342, loss is 0.0025393692776560783\n",
      "epoch: 4 step: 343, loss is 0.003502780105918646\n",
      "epoch: 4 step: 344, loss is 0.20376543700695038\n",
      "epoch: 4 step: 345, loss is 0.006431402172893286\n",
      "epoch: 4 step: 346, loss is 0.0014828515704721212\n",
      "epoch: 4 step: 347, loss is 0.014727450907230377\n",
      "epoch: 4 step: 348, loss is 0.011418567970395088\n",
      "epoch: 4 step: 349, loss is 0.0008182853343896568\n",
      "epoch: 4 step: 350, loss is 0.3221995234489441\n",
      "epoch: 4 step: 351, loss is 0.017369739711284637\n",
      "epoch: 4 step: 352, loss is 0.0002401858801022172\n",
      "epoch: 4 step: 353, loss is 0.0024824831634759903\n",
      "epoch: 4 step: 354, loss is 0.0015331482281908393\n",
      "epoch: 4 step: 355, loss is 0.0018195000011473894\n",
      "epoch: 4 step: 356, loss is 0.00040035005076788366\n",
      "epoch: 4 step: 357, loss is 0.017130233347415924\n",
      "epoch: 4 step: 358, loss is 0.004832981154322624\n",
      "epoch: 4 step: 359, loss is 0.018823623657226562\n",
      "epoch: 4 step: 360, loss is 0.006133250426501036\n",
      "epoch: 4 step: 361, loss is 0.053942564874887466\n",
      "epoch: 4 step: 362, loss is 0.0016721530118957162\n",
      "epoch: 4 step: 363, loss is 0.004088132176548243\n",
      "epoch: 4 step: 364, loss is 0.014683679677546024\n",
      "epoch: 4 step: 365, loss is 0.10201548784971237\n",
      "epoch: 4 step: 366, loss is 0.2175474762916565\n",
      "epoch: 4 step: 367, loss is 0.00260448781773448\n",
      "epoch: 4 step: 368, loss is 0.026141216978430748\n",
      "epoch: 4 step: 369, loss is 0.010099658742547035\n",
      "epoch: 4 step: 370, loss is 0.016618352383375168\n",
      "epoch: 4 step: 371, loss is 0.0036601710598915815\n",
      "epoch: 4 step: 372, loss is 0.0050069065764546394\n",
      "epoch: 4 step: 373, loss is 0.03597787022590637\n",
      "epoch: 4 step: 374, loss is 0.060568105429410934\n",
      "epoch: 4 step: 375, loss is 0.07453823834657669\n",
      "epoch: 4 step: 376, loss is 0.009170160628855228\n",
      "epoch: 4 step: 377, loss is 0.06466146558523178\n",
      "epoch: 4 step: 378, loss is 0.008326397277414799\n",
      "epoch: 4 step: 379, loss is 0.007322102319449186\n",
      "epoch: 4 step: 380, loss is 0.013680254109203815\n",
      "epoch: 4 step: 381, loss is 0.17892485857009888\n",
      "epoch: 4 step: 382, loss is 0.0032989897299557924\n",
      "epoch: 4 step: 383, loss is 0.0196374524384737\n",
      "epoch: 4 step: 384, loss is 0.007304210215806961\n",
      "epoch: 4 step: 385, loss is 0.009466688148677349\n",
      "epoch: 4 step: 386, loss is 0.12032237648963928\n",
      "epoch: 4 step: 387, loss is 0.08658158034086227\n",
      "epoch: 4 step: 388, loss is 0.05040336400270462\n",
      "epoch: 4 step: 389, loss is 0.22169381380081177\n",
      "epoch: 4 step: 390, loss is 0.011076932772994041\n",
      "epoch: 4 step: 391, loss is 0.016532164067029953\n",
      "epoch: 4 step: 392, loss is 0.002021274296566844\n",
      "epoch: 4 step: 393, loss is 0.017734911292791367\n",
      "epoch: 4 step: 394, loss is 0.004485215526074171\n",
      "epoch: 4 step: 395, loss is 0.05795323848724365\n",
      "epoch: 4 step: 396, loss is 0.08709055185317993\n",
      "epoch: 4 step: 397, loss is 0.0656207725405693\n",
      "epoch: 4 step: 398, loss is 0.001433178666047752\n",
      "epoch: 4 step: 399, loss is 0.021064676344394684\n",
      "epoch: 4 step: 400, loss is 0.005800630897283554\n",
      "epoch: 4 step: 401, loss is 0.06637468189001083\n",
      "epoch: 4 step: 402, loss is 0.028466418385505676\n",
      "epoch: 4 step: 403, loss is 0.006695038639008999\n",
      "epoch: 4 step: 404, loss is 0.0633285716176033\n",
      "epoch: 4 step: 405, loss is 0.014300189912319183\n",
      "epoch: 4 step: 406, loss is 0.27056124806404114\n",
      "epoch: 4 step: 407, loss is 0.005943101830780506\n",
      "epoch: 4 step: 408, loss is 0.03386855125427246\n",
      "epoch: 4 step: 409, loss is 0.007943214848637581\n",
      "epoch: 4 step: 410, loss is 0.023087384179234505\n",
      "epoch: 4 step: 411, loss is 0.015011363662779331\n",
      "epoch: 4 step: 412, loss is 0.019171027466654778\n",
      "epoch: 4 step: 413, loss is 0.0028815963305532932\n",
      "epoch: 4 step: 414, loss is 0.002348231850191951\n",
      "epoch: 4 step: 415, loss is 0.010854853317141533\n",
      "epoch: 4 step: 416, loss is 0.2500290274620056\n",
      "epoch: 4 step: 417, loss is 0.0026188709307461977\n",
      "epoch: 4 step: 418, loss is 0.0008704126230441034\n",
      "epoch: 4 step: 419, loss is 0.014423852786421776\n",
      "epoch: 4 step: 420, loss is 0.040729865431785583\n",
      "epoch: 4 step: 421, loss is 0.002830042038112879\n",
      "epoch: 4 step: 422, loss is 0.0013750941725447774\n",
      "epoch: 4 step: 423, loss is 0.0021421590354293585\n",
      "epoch: 4 step: 424, loss is 0.0006751855253241956\n",
      "epoch: 4 step: 425, loss is 0.0007378430454991758\n",
      "epoch: 4 step: 426, loss is 0.042644135653972626\n",
      "epoch: 4 step: 427, loss is 0.005341059062629938\n",
      "epoch: 4 step: 428, loss is 0.0012964877532795072\n",
      "epoch: 4 step: 429, loss is 0.09552447497844696\n",
      "epoch: 4 step: 430, loss is 0.002546195639297366\n",
      "epoch: 4 step: 431, loss is 0.01258363388478756\n",
      "epoch: 4 step: 432, loss is 0.005085519049316645\n",
      "epoch: 4 step: 433, loss is 0.04422169551253319\n",
      "epoch: 4 step: 434, loss is 0.032530855387449265\n",
      "epoch: 4 step: 435, loss is 0.007374380715191364\n",
      "epoch: 4 step: 436, loss is 0.0008373052114620805\n",
      "epoch: 4 step: 437, loss is 0.0008091203053481877\n",
      "epoch: 4 step: 438, loss is 0.002003657165914774\n",
      "epoch: 4 step: 439, loss is 0.004704631399363279\n",
      "epoch: 4 step: 440, loss is 0.0009257585625164211\n",
      "epoch: 4 step: 441, loss is 0.006761951372027397\n",
      "epoch: 4 step: 442, loss is 0.000322108156979084\n",
      "epoch: 4 step: 443, loss is 0.0034387519117444754\n",
      "epoch: 4 step: 444, loss is 0.0008628871291875839\n",
      "epoch: 4 step: 445, loss is 0.003947196062654257\n",
      "epoch: 4 step: 446, loss is 0.13025416433811188\n",
      "epoch: 4 step: 447, loss is 0.0006741823744960129\n",
      "epoch: 4 step: 448, loss is 0.0008760474738664925\n",
      "epoch: 4 step: 449, loss is 0.22270430624485016\n",
      "epoch: 4 step: 450, loss is 0.0026148343458771706\n",
      "epoch: 4 step: 451, loss is 0.016901960596442223\n",
      "epoch: 4 step: 452, loss is 0.06606613844633102\n",
      "epoch: 4 step: 453, loss is 0.012004479765892029\n",
      "epoch: 4 step: 454, loss is 0.06344645470380783\n",
      "epoch: 4 step: 455, loss is 0.004275881685316563\n",
      "epoch: 4 step: 456, loss is 0.3378181755542755\n",
      "epoch: 4 step: 457, loss is 0.013224784284830093\n",
      "epoch: 4 step: 458, loss is 0.005488957278430462\n",
      "epoch: 4 step: 459, loss is 0.008788572624325752\n",
      "epoch: 4 step: 460, loss is 0.007102920673787594\n",
      "epoch: 4 step: 461, loss is 0.005818371661007404\n",
      "epoch: 4 step: 462, loss is 0.006209075916558504\n",
      "epoch: 4 step: 463, loss is 0.012234385125339031\n",
      "epoch: 4 step: 464, loss is 0.024879494681954384\n",
      "epoch: 4 step: 465, loss is 0.21194665133953094\n",
      "epoch: 4 step: 466, loss is 0.0005165686598047614\n",
      "epoch: 4 step: 467, loss is 0.014349719509482384\n",
      "epoch: 4 step: 468, loss is 0.01697714254260063\n",
      "epoch: 4 step: 469, loss is 0.004222205374389887\n",
      "epoch: 4 step: 470, loss is 0.014969561249017715\n",
      "epoch: 4 step: 471, loss is 0.10455744713544846\n",
      "epoch: 4 step: 472, loss is 0.005302462261170149\n",
      "epoch: 4 step: 473, loss is 0.0004423208301886916\n",
      "epoch: 4 step: 474, loss is 0.0009892812231555581\n",
      "epoch: 4 step: 475, loss is 0.0003502607869450003\n",
      "epoch: 4 step: 476, loss is 0.22497551143169403\n",
      "epoch: 4 step: 477, loss is 0.09407447278499603\n",
      "epoch: 4 step: 478, loss is 0.00011624267062870786\n",
      "epoch: 4 step: 479, loss is 0.16775907576084137\n",
      "epoch: 4 step: 480, loss is 0.0652196854352951\n",
      "epoch: 4 step: 481, loss is 0.0030975767876952887\n",
      "epoch: 4 step: 482, loss is 0.13913677632808685\n",
      "epoch: 4 step: 483, loss is 0.0049675628542900085\n",
      "epoch: 4 step: 484, loss is 0.011344336904585361\n",
      "epoch: 4 step: 485, loss is 0.013933341018855572\n",
      "epoch: 4 step: 486, loss is 0.005785499699413776\n",
      "epoch: 4 step: 487, loss is 0.0007416823063977063\n",
      "epoch: 4 step: 488, loss is 0.003671305486932397\n",
      "epoch: 4 step: 489, loss is 0.00906454212963581\n",
      "epoch: 4 step: 490, loss is 0.003238278441131115\n",
      "epoch: 4 step: 491, loss is 0.039709072560071945\n",
      "epoch: 4 step: 492, loss is 0.025417696684598923\n",
      "epoch: 4 step: 493, loss is 0.002864967565983534\n",
      "epoch: 4 step: 494, loss is 0.01932375133037567\n",
      "epoch: 4 step: 495, loss is 0.012397626414895058\n",
      "epoch: 4 step: 496, loss is 0.0005034843343310058\n",
      "epoch: 4 step: 497, loss is 0.0016560070216655731\n",
      "epoch: 4 step: 498, loss is 0.002484545111656189\n",
      "epoch: 4 step: 499, loss is 0.03936074674129486\n",
      "epoch: 4 step: 500, loss is 0.022068191319704056\n",
      "epoch: 4 step: 501, loss is 0.0033356049098074436\n",
      "epoch: 4 step: 502, loss is 0.09784050285816193\n",
      "epoch: 4 step: 503, loss is 0.07788000255823135\n",
      "epoch: 4 step: 504, loss is 0.0026903196703642607\n",
      "epoch: 4 step: 505, loss is 0.0007419832400046289\n",
      "epoch: 4 step: 506, loss is 0.005503200925886631\n",
      "epoch: 4 step: 507, loss is 0.024727893993258476\n",
      "epoch: 4 step: 508, loss is 0.020691318437457085\n",
      "epoch: 4 step: 509, loss is 0.0009141067857854068\n",
      "epoch: 4 step: 510, loss is 0.02678881771862507\n",
      "epoch: 4 step: 511, loss is 0.09129311889410019\n",
      "epoch: 4 step: 512, loss is 0.001116074388846755\n",
      "epoch: 4 step: 513, loss is 0.00038043616223149\n",
      "epoch: 4 step: 514, loss is 0.03862832486629486\n",
      "epoch: 4 step: 515, loss is 0.0007774293189868331\n",
      "epoch: 4 step: 516, loss is 0.015217345207929611\n",
      "epoch: 4 step: 517, loss is 0.01770642399787903\n",
      "epoch: 4 step: 518, loss is 0.15892307460308075\n",
      "epoch: 4 step: 519, loss is 0.004219003487378359\n",
      "epoch: 4 step: 520, loss is 0.0037528383545577526\n",
      "epoch: 4 step: 521, loss is 0.015915773808956146\n",
      "epoch: 4 step: 522, loss is 0.03230553865432739\n",
      "epoch: 4 step: 523, loss is 0.07421667873859406\n",
      "epoch: 4 step: 524, loss is 0.09065006673336029\n",
      "epoch: 4 step: 525, loss is 0.027497293427586555\n",
      "epoch: 4 step: 526, loss is 0.07301740348339081\n",
      "epoch: 4 step: 527, loss is 0.0024267497938126326\n",
      "epoch: 4 step: 528, loss is 0.023423772305250168\n",
      "epoch: 4 step: 529, loss is 0.004276502411812544\n",
      "epoch: 4 step: 530, loss is 0.006645919755101204\n",
      "epoch: 4 step: 531, loss is 0.00445837015286088\n",
      "epoch: 4 step: 532, loss is 0.04314645007252693\n",
      "epoch: 4 step: 533, loss is 0.1376233845949173\n",
      "epoch: 4 step: 534, loss is 0.002171427011489868\n",
      "epoch: 4 step: 535, loss is 0.0016921181231737137\n",
      "epoch: 4 step: 536, loss is 0.0009160118643194437\n",
      "epoch: 4 step: 537, loss is 0.058379363268613815\n",
      "epoch: 4 step: 538, loss is 0.037929777055978775\n",
      "epoch: 4 step: 539, loss is 0.001885551493614912\n",
      "epoch: 4 step: 540, loss is 0.3191452920436859\n",
      "epoch: 4 step: 541, loss is 0.004114640410989523\n",
      "epoch: 4 step: 542, loss is 0.0028752607759088278\n",
      "epoch: 4 step: 543, loss is 0.000833885045722127\n",
      "epoch: 4 step: 544, loss is 0.023444008082151413\n",
      "epoch: 4 step: 545, loss is 0.04920104891061783\n",
      "epoch: 4 step: 546, loss is 0.0014531186316162348\n",
      "epoch: 4 step: 547, loss is 0.05127328261733055\n",
      "epoch: 4 step: 548, loss is 0.007467993069440126\n",
      "epoch: 4 step: 549, loss is 0.1474432796239853\n",
      "epoch: 4 step: 550, loss is 0.019133947789669037\n",
      "epoch: 4 step: 551, loss is 0.46862009167671204\n",
      "epoch: 4 step: 552, loss is 0.002551482291892171\n",
      "epoch: 4 step: 553, loss is 0.005437313113361597\n",
      "epoch: 4 step: 554, loss is 0.05150420591235161\n",
      "epoch: 4 step: 555, loss is 0.003084530122578144\n",
      "epoch: 4 step: 556, loss is 0.0020868508145213127\n",
      "epoch: 4 step: 557, loss is 0.006471933331340551\n",
      "epoch: 4 step: 558, loss is 0.07579502463340759\n",
      "epoch: 4 step: 559, loss is 0.0009515107376500964\n",
      "epoch: 4 step: 560, loss is 0.01042109727859497\n",
      "epoch: 4 step: 561, loss is 0.034983761608600616\n",
      "epoch: 4 step: 562, loss is 0.01666637510061264\n",
      "epoch: 4 step: 563, loss is 0.0020957905799150467\n",
      "epoch: 4 step: 564, loss is 0.005140116438269615\n",
      "epoch: 4 step: 565, loss is 0.027340766042470932\n",
      "epoch: 4 step: 566, loss is 0.011677882634103298\n",
      "epoch: 4 step: 567, loss is 0.01770898513495922\n",
      "epoch: 4 step: 568, loss is 0.023301977664232254\n",
      "epoch: 4 step: 569, loss is 0.0005210587987676263\n",
      "epoch: 4 step: 570, loss is 0.0027596408035606146\n",
      "epoch: 4 step: 571, loss is 0.001259975484572351\n",
      "epoch: 4 step: 572, loss is 0.016398051753640175\n",
      "epoch: 4 step: 573, loss is 0.0012684386456385255\n",
      "epoch: 4 step: 574, loss is 0.06637853384017944\n",
      "epoch: 4 step: 575, loss is 0.0017050119349732995\n",
      "epoch: 4 step: 576, loss is 0.0032394160516560078\n",
      "epoch: 4 step: 577, loss is 0.002761661307886243\n",
      "epoch: 4 step: 578, loss is 0.041513871401548386\n",
      "epoch: 4 step: 579, loss is 0.0019738725386559963\n",
      "epoch: 4 step: 580, loss is 0.07859023660421371\n",
      "epoch: 4 step: 581, loss is 0.0037480995524674654\n",
      "epoch: 4 step: 582, loss is 0.03211333975195885\n",
      "epoch: 4 step: 583, loss is 0.0014578187838196754\n",
      "epoch: 4 step: 584, loss is 0.002171215135604143\n",
      "epoch: 4 step: 585, loss is 0.015416695736348629\n",
      "epoch: 4 step: 586, loss is 0.05756712332367897\n",
      "epoch: 4 step: 587, loss is 0.00020388637494761497\n",
      "epoch: 4 step: 588, loss is 0.015500219538807869\n",
      "epoch: 4 step: 589, loss is 0.012109595350921154\n",
      "epoch: 4 step: 590, loss is 0.012594852596521378\n",
      "epoch: 4 step: 591, loss is 0.09699184447526932\n",
      "epoch: 4 step: 592, loss is 0.0003636002657003701\n",
      "epoch: 4 step: 593, loss is 0.035389162600040436\n",
      "epoch: 4 step: 594, loss is 0.00655219703912735\n",
      "epoch: 4 step: 595, loss is 0.12564384937286377\n",
      "epoch: 4 step: 596, loss is 0.18188118934631348\n",
      "epoch: 4 step: 597, loss is 0.03851199522614479\n",
      "epoch: 4 step: 598, loss is 0.011218839325010777\n",
      "epoch: 4 step: 599, loss is 0.002095966599881649\n",
      "epoch: 4 step: 600, loss is 0.0012329366290941834\n",
      "epoch: 4 step: 601, loss is 0.003382423659786582\n",
      "epoch: 4 step: 602, loss is 0.011240732856094837\n",
      "epoch: 4 step: 603, loss is 0.05588448420166969\n",
      "epoch: 4 step: 604, loss is 0.0017206472111865878\n",
      "epoch: 4 step: 605, loss is 0.041739385575056076\n",
      "epoch: 4 step: 606, loss is 0.003077236469835043\n",
      "epoch: 4 step: 607, loss is 0.00985054112970829\n",
      "epoch: 4 step: 608, loss is 0.0016790024237707257\n",
      "epoch: 4 step: 609, loss is 0.002520117675885558\n",
      "epoch: 4 step: 610, loss is 0.004798768553882837\n",
      "epoch: 4 step: 611, loss is 0.02695661038160324\n",
      "epoch: 4 step: 612, loss is 0.08932922780513763\n",
      "epoch: 4 step: 613, loss is 0.0005334505112841725\n",
      "epoch: 4 step: 614, loss is 0.0467277355492115\n",
      "epoch: 4 step: 615, loss is 0.2567537724971771\n",
      "epoch: 4 step: 616, loss is 0.029089784249663353\n",
      "epoch: 4 step: 617, loss is 0.13974641263484955\n",
      "epoch: 4 step: 618, loss is 0.0002857716754078865\n",
      "epoch: 4 step: 619, loss is 0.001668411772698164\n",
      "epoch: 4 step: 620, loss is 0.0760076642036438\n",
      "epoch: 4 step: 621, loss is 0.008676595985889435\n",
      "epoch: 4 step: 622, loss is 0.0007590873283334076\n",
      "epoch: 4 step: 623, loss is 0.14231444895267487\n",
      "epoch: 4 step: 624, loss is 0.00030067047919146717\n",
      "epoch: 4 step: 625, loss is 0.0020338203758001328\n",
      "epoch: 4 step: 626, loss is 0.0009566222433932126\n",
      "epoch: 4 step: 627, loss is 0.08702965080738068\n",
      "epoch: 4 step: 628, loss is 0.21483856439590454\n",
      "epoch: 4 step: 629, loss is 0.03560533747076988\n",
      "epoch: 4 step: 630, loss is 0.003396949265152216\n",
      "epoch: 4 step: 631, loss is 0.002265321323648095\n",
      "epoch: 4 step: 632, loss is 0.005300507415086031\n",
      "epoch: 4 step: 633, loss is 0.09791258722543716\n",
      "epoch: 4 step: 634, loss is 0.29384273290634155\n",
      "epoch: 4 step: 635, loss is 0.006734631489962339\n",
      "epoch: 4 step: 636, loss is 0.011416872963309288\n",
      "epoch: 4 step: 637, loss is 0.012573768384754658\n",
      "epoch: 4 step: 638, loss is 0.002703774254769087\n",
      "epoch: 4 step: 639, loss is 0.03689592331647873\n",
      "epoch: 4 step: 640, loss is 0.0025672689080238342\n",
      "epoch: 4 step: 641, loss is 0.04426095262169838\n",
      "epoch: 4 step: 642, loss is 0.014248429797589779\n",
      "epoch: 4 step: 643, loss is 0.008490042760968208\n",
      "epoch: 4 step: 644, loss is 0.03140628710389137\n",
      "epoch: 4 step: 645, loss is 0.0005848960136063397\n",
      "epoch: 4 step: 646, loss is 0.05535406619310379\n",
      "epoch: 4 step: 647, loss is 0.08000373095273972\n",
      "epoch: 4 step: 648, loss is 0.17752628028392792\n",
      "epoch: 4 step: 649, loss is 0.00597023731097579\n",
      "epoch: 4 step: 650, loss is 0.007723669055849314\n",
      "epoch: 4 step: 651, loss is 0.002857108134776354\n",
      "epoch: 4 step: 652, loss is 0.002681988524273038\n",
      "epoch: 4 step: 653, loss is 0.013242878951132298\n",
      "epoch: 4 step: 654, loss is 0.057462941855192184\n",
      "epoch: 4 step: 655, loss is 0.013025551103055477\n",
      "epoch: 4 step: 656, loss is 0.005799403879791498\n",
      "epoch: 4 step: 657, loss is 0.03914627432823181\n",
      "epoch: 4 step: 658, loss is 0.020962918177247047\n",
      "epoch: 4 step: 659, loss is 0.024925269186496735\n",
      "epoch: 4 step: 660, loss is 0.012513622641563416\n",
      "epoch: 4 step: 661, loss is 0.003345595207065344\n",
      "epoch: 4 step: 662, loss is 0.024776596575975418\n",
      "epoch: 4 step: 663, loss is 0.005667561665177345\n",
      "epoch: 4 step: 664, loss is 0.0004940424114465714\n",
      "epoch: 4 step: 665, loss is 0.04769212752580643\n",
      "epoch: 4 step: 666, loss is 0.005242465529590845\n",
      "epoch: 4 step: 667, loss is 0.06524314731359482\n",
      "epoch: 4 step: 668, loss is 0.009508349001407623\n",
      "epoch: 4 step: 669, loss is 0.00037351902574300766\n",
      "epoch: 4 step: 670, loss is 0.16234111785888672\n",
      "epoch: 4 step: 671, loss is 0.003552026115357876\n",
      "epoch: 4 step: 672, loss is 0.00011877603537868708\n",
      "epoch: 4 step: 673, loss is 0.2975809574127197\n",
      "epoch: 4 step: 674, loss is 0.002508049365133047\n",
      "epoch: 4 step: 675, loss is 0.004079566337168217\n",
      "epoch: 4 step: 676, loss is 0.009217694401741028\n",
      "epoch: 4 step: 677, loss is 0.0013230381300672889\n",
      "epoch: 4 step: 678, loss is 0.00987215619534254\n",
      "epoch: 4 step: 679, loss is 0.004073598887771368\n",
      "epoch: 4 step: 680, loss is 0.032109539955854416\n",
      "epoch: 4 step: 681, loss is 0.026197703555226326\n",
      "epoch: 4 step: 682, loss is 0.004611339420080185\n",
      "epoch: 4 step: 683, loss is 0.06957604736089706\n",
      "epoch: 4 step: 684, loss is 0.014662502333521843\n",
      "epoch: 4 step: 685, loss is 0.04038605839014053\n",
      "epoch: 4 step: 686, loss is 0.003981987480074167\n",
      "epoch: 4 step: 687, loss is 0.011626392602920532\n",
      "epoch: 4 step: 688, loss is 0.0323951318860054\n",
      "epoch: 4 step: 689, loss is 0.008786961436271667\n",
      "epoch: 4 step: 690, loss is 0.02345476672053337\n",
      "epoch: 4 step: 691, loss is 0.07329010963439941\n",
      "epoch: 4 step: 692, loss is 0.11204889416694641\n",
      "epoch: 4 step: 693, loss is 0.0113755464553833\n",
      "epoch: 4 step: 694, loss is 0.021147198975086212\n",
      "epoch: 4 step: 695, loss is 0.0939369723200798\n",
      "epoch: 4 step: 696, loss is 0.0013142033712938428\n",
      "epoch: 4 step: 697, loss is 0.025829147547483444\n",
      "epoch: 4 step: 698, loss is 0.012731097638607025\n",
      "epoch: 4 step: 699, loss is 0.018427303060889244\n",
      "epoch: 4 step: 700, loss is 0.011425522156059742\n",
      "epoch: 4 step: 701, loss is 0.0057453010231256485\n",
      "epoch: 4 step: 702, loss is 0.0015404672594740987\n",
      "epoch: 4 step: 703, loss is 0.0010401858016848564\n",
      "epoch: 4 step: 704, loss is 0.019138867035508156\n",
      "epoch: 4 step: 705, loss is 0.0002566337934695184\n",
      "epoch: 4 step: 706, loss is 0.020518219098448753\n",
      "epoch: 4 step: 707, loss is 0.000660646241158247\n",
      "epoch: 4 step: 708, loss is 0.004666416440159082\n",
      "epoch: 4 step: 709, loss is 0.00282823434099555\n",
      "epoch: 4 step: 710, loss is 0.09638043493032455\n",
      "epoch: 4 step: 711, loss is 0.0016846711514517665\n",
      "epoch: 4 step: 712, loss is 0.22536998987197876\n",
      "epoch: 4 step: 713, loss is 0.0658193975687027\n",
      "epoch: 4 step: 714, loss is 0.0029934823978692293\n",
      "epoch: 4 step: 715, loss is 0.008685576729476452\n",
      "epoch: 4 step: 716, loss is 0.013575518503785133\n",
      "epoch: 4 step: 717, loss is 0.12499557435512543\n",
      "epoch: 4 step: 718, loss is 0.04216761142015457\n",
      "epoch: 4 step: 719, loss is 0.11039682477712631\n",
      "epoch: 4 step: 720, loss is 0.007028796244412661\n",
      "epoch: 4 step: 721, loss is 0.0011591558577492833\n",
      "epoch: 4 step: 722, loss is 0.0011352768633514643\n",
      "epoch: 4 step: 723, loss is 0.005892512388527393\n",
      "epoch: 4 step: 724, loss is 0.003432885045185685\n",
      "epoch: 4 step: 725, loss is 0.048032376915216446\n",
      "epoch: 4 step: 726, loss is 0.008224019780755043\n",
      "epoch: 4 step: 727, loss is 0.0840795710682869\n",
      "epoch: 4 step: 728, loss is 0.003740213345736265\n",
      "epoch: 4 step: 729, loss is 0.010659570805728436\n",
      "epoch: 4 step: 730, loss is 0.00026465117116458714\n",
      "epoch: 4 step: 731, loss is 0.11087485402822495\n",
      "epoch: 4 step: 732, loss is 0.024408195167779922\n",
      "epoch: 4 step: 733, loss is 0.0035221551079303026\n",
      "epoch: 4 step: 734, loss is 0.006889690179377794\n",
      "epoch: 4 step: 735, loss is 0.06826015561819077\n",
      "epoch: 4 step: 736, loss is 0.059689152985811234\n",
      "epoch: 4 step: 737, loss is 0.0021353824995458126\n",
      "epoch: 4 step: 738, loss is 0.0008547320030629635\n",
      "epoch: 4 step: 739, loss is 0.0011988705955445766\n",
      "epoch: 4 step: 740, loss is 0.026904907077550888\n",
      "epoch: 4 step: 741, loss is 0.000967093335930258\n",
      "epoch: 4 step: 742, loss is 0.017309311777353287\n",
      "epoch: 4 step: 743, loss is 0.0056914337910711765\n",
      "epoch: 4 step: 744, loss is 0.000612402509432286\n",
      "epoch: 4 step: 745, loss is 0.26143190264701843\n",
      "epoch: 4 step: 746, loss is 0.011004583910107613\n",
      "epoch: 4 step: 747, loss is 0.001768754213117063\n",
      "epoch: 4 step: 748, loss is 0.0024563618935644627\n",
      "epoch: 4 step: 749, loss is 0.001685788854956627\n",
      "epoch: 4 step: 750, loss is 0.017592208459973335\n",
      "epoch: 4 step: 751, loss is 0.002025081543251872\n",
      "epoch: 4 step: 752, loss is 0.0026303455233573914\n",
      "epoch: 4 step: 753, loss is 0.032545022666454315\n",
      "epoch: 4 step: 754, loss is 0.0001753280230332166\n",
      "epoch: 4 step: 755, loss is 0.001852278015576303\n",
      "epoch: 4 step: 756, loss is 0.12338703125715256\n",
      "epoch: 4 step: 757, loss is 0.004206818528473377\n",
      "epoch: 4 step: 758, loss is 0.007319487631320953\n",
      "epoch: 4 step: 759, loss is 0.006502932403236628\n",
      "epoch: 4 step: 760, loss is 0.0707189068198204\n",
      "epoch: 4 step: 761, loss is 0.007661053910851479\n",
      "epoch: 4 step: 762, loss is 0.3045503497123718\n",
      "epoch: 4 step: 763, loss is 0.00254359794780612\n",
      "epoch: 4 step: 764, loss is 0.0009490541415289044\n",
      "epoch: 4 step: 765, loss is 0.00048793028690852225\n",
      "epoch: 4 step: 766, loss is 0.03562421724200249\n",
      "epoch: 4 step: 767, loss is 0.032530028373003006\n",
      "epoch: 4 step: 768, loss is 0.03306848928332329\n",
      "epoch: 4 step: 769, loss is 0.0012088142102584243\n",
      "epoch: 4 step: 770, loss is 0.0020961768459528685\n",
      "epoch: 4 step: 771, loss is 0.0005945251905359328\n",
      "epoch: 4 step: 772, loss is 0.010963535867631435\n",
      "epoch: 4 step: 773, loss is 0.0024970679078251123\n",
      "epoch: 4 step: 774, loss is 0.025710387155413628\n",
      "epoch: 4 step: 775, loss is 0.06568816304206848\n",
      "epoch: 4 step: 776, loss is 0.009651060216128826\n",
      "epoch: 4 step: 777, loss is 0.003846758045256138\n",
      "epoch: 4 step: 778, loss is 0.0016593113541603088\n",
      "epoch: 4 step: 779, loss is 0.06395925581455231\n",
      "epoch: 4 step: 780, loss is 0.0005546718603000045\n",
      "epoch: 4 step: 781, loss is 0.004246555268764496\n",
      "epoch: 4 step: 782, loss is 0.035747017711400986\n",
      "epoch: 4 step: 783, loss is 0.0027087486814707518\n",
      "epoch: 4 step: 784, loss is 0.06784079968929291\n",
      "epoch: 4 step: 785, loss is 0.10489659011363983\n",
      "epoch: 4 step: 786, loss is 0.008465529419481754\n",
      "epoch: 4 step: 787, loss is 0.12280481308698654\n",
      "epoch: 4 step: 788, loss is 0.002367292996495962\n",
      "epoch: 4 step: 789, loss is 0.012166018597781658\n",
      "epoch: 4 step: 790, loss is 0.007529925089329481\n",
      "epoch: 4 step: 791, loss is 0.007243664935231209\n",
      "epoch: 4 step: 792, loss is 0.006740615237504244\n",
      "epoch: 4 step: 793, loss is 0.05145234614610672\n",
      "epoch: 4 step: 794, loss is 0.004258971195667982\n",
      "epoch: 4 step: 795, loss is 0.01237589679658413\n",
      "epoch: 4 step: 796, loss is 0.01887492649257183\n",
      "epoch: 4 step: 797, loss is 0.0026654864195734262\n",
      "epoch: 4 step: 798, loss is 0.004684665240347385\n",
      "epoch: 4 step: 799, loss is 0.00605233246460557\n",
      "epoch: 4 step: 800, loss is 0.0032053908798843622\n",
      "epoch: 4 step: 801, loss is 0.03715654835104942\n",
      "epoch: 4 step: 802, loss is 0.005350414197891951\n",
      "epoch: 4 step: 803, loss is 0.008216808550059795\n",
      "epoch: 4 step: 804, loss is 0.05537350848317146\n",
      "epoch: 4 step: 805, loss is 0.07330267131328583\n",
      "epoch: 4 step: 806, loss is 0.000715496251359582\n",
      "epoch: 4 step: 807, loss is 0.004597384948283434\n",
      "epoch: 4 step: 808, loss is 0.07687785476446152\n",
      "epoch: 4 step: 809, loss is 0.020664537325501442\n",
      "epoch: 4 step: 810, loss is 0.03871209919452667\n",
      "epoch: 4 step: 811, loss is 0.003883759491145611\n",
      "epoch: 4 step: 812, loss is 0.1119956448674202\n",
      "epoch: 4 step: 813, loss is 0.0013783875619992614\n",
      "epoch: 4 step: 814, loss is 0.008243163116276264\n",
      "epoch: 4 step: 815, loss is 0.010207208804786205\n",
      "epoch: 4 step: 816, loss is 0.06769677996635437\n",
      "epoch: 4 step: 817, loss is 0.04981369152665138\n",
      "epoch: 4 step: 818, loss is 0.005627909209579229\n",
      "epoch: 4 step: 819, loss is 0.002908723894506693\n",
      "epoch: 4 step: 820, loss is 0.0026301834732294083\n",
      "epoch: 4 step: 821, loss is 0.1015014722943306\n",
      "epoch: 4 step: 822, loss is 0.17804230749607086\n",
      "epoch: 4 step: 823, loss is 0.06492572277784348\n",
      "epoch: 4 step: 824, loss is 0.043855540454387665\n",
      "epoch: 4 step: 825, loss is 0.0833844467997551\n",
      "epoch: 4 step: 826, loss is 0.0015902162995189428\n",
      "epoch: 4 step: 827, loss is 0.0004904715460725129\n",
      "epoch: 4 step: 828, loss is 0.0049331397749483585\n",
      "epoch: 4 step: 829, loss is 0.0006665474502369761\n",
      "epoch: 4 step: 830, loss is 0.00452452152967453\n",
      "epoch: 4 step: 831, loss is 0.05973818898200989\n",
      "epoch: 4 step: 832, loss is 0.0013980123912915587\n",
      "epoch: 4 step: 833, loss is 0.017630651593208313\n",
      "epoch: 4 step: 834, loss is 0.13167551159858704\n",
      "epoch: 4 step: 835, loss is 0.030487557873129845\n",
      "epoch: 4 step: 836, loss is 0.0030170916579663754\n",
      "epoch: 4 step: 837, loss is 0.0007048706756904721\n",
      "epoch: 4 step: 838, loss is 0.009039285592734814\n",
      "epoch: 4 step: 839, loss is 0.006708876229822636\n",
      "epoch: 4 step: 840, loss is 0.021871963515877724\n",
      "epoch: 4 step: 841, loss is 0.014669305644929409\n",
      "epoch: 4 step: 842, loss is 0.011170687153935432\n",
      "epoch: 4 step: 843, loss is 0.020274631679058075\n",
      "epoch: 4 step: 844, loss is 0.006444127298891544\n",
      "epoch: 4 step: 845, loss is 0.00807335413992405\n",
      "epoch: 4 step: 846, loss is 0.002073054201900959\n",
      "epoch: 4 step: 847, loss is 0.00633449200540781\n",
      "epoch: 4 step: 848, loss is 0.011430175974965096\n",
      "epoch: 4 step: 849, loss is 0.1893787682056427\n",
      "epoch: 4 step: 850, loss is 0.07120257616043091\n",
      "epoch: 4 step: 851, loss is 0.0009623452206142247\n",
      "epoch: 4 step: 852, loss is 0.004582081455737352\n",
      "epoch: 4 step: 853, loss is 0.011577734723687172\n",
      "epoch: 4 step: 854, loss is 0.0015897846315056086\n",
      "epoch: 4 step: 855, loss is 0.0367402508854866\n",
      "epoch: 4 step: 856, loss is 0.01035845186561346\n",
      "epoch: 4 step: 857, loss is 0.012625274248421192\n",
      "epoch: 4 step: 858, loss is 0.03989692032337189\n",
      "epoch: 4 step: 859, loss is 0.016227789223194122\n",
      "epoch: 4 step: 860, loss is 0.0010839710012078285\n",
      "epoch: 4 step: 861, loss is 0.03379804641008377\n",
      "epoch: 4 step: 862, loss is 0.0007217100355774164\n",
      "epoch: 4 step: 863, loss is 0.1611355096101761\n",
      "epoch: 4 step: 864, loss is 0.001988429808989167\n",
      "epoch: 4 step: 865, loss is 0.010743505321443081\n",
      "epoch: 4 step: 866, loss is 0.001807496533729136\n",
      "epoch: 4 step: 867, loss is 0.001014620647765696\n",
      "epoch: 4 step: 868, loss is 0.0017239770386368036\n",
      "epoch: 4 step: 869, loss is 0.036434393376111984\n",
      "epoch: 4 step: 870, loss is 0.0006697975331917405\n",
      "epoch: 4 step: 871, loss is 0.007707640063017607\n",
      "epoch: 4 step: 872, loss is 0.05948569253087044\n",
      "epoch: 4 step: 873, loss is 0.042286667972803116\n",
      "epoch: 4 step: 874, loss is 0.017140455543994904\n",
      "epoch: 4 step: 875, loss is 0.002793447347357869\n",
      "epoch: 4 step: 876, loss is 0.0004181640688329935\n",
      "epoch: 4 step: 877, loss is 0.00014041934628039598\n",
      "epoch: 4 step: 878, loss is 0.0007415930740535259\n",
      "epoch: 4 step: 879, loss is 0.11124227195978165\n",
      "epoch: 4 step: 880, loss is 0.009566286578774452\n",
      "epoch: 4 step: 881, loss is 0.03796582669019699\n",
      "epoch: 4 step: 882, loss is 0.0026041872333735228\n",
      "epoch: 4 step: 883, loss is 0.03283132240176201\n",
      "epoch: 4 step: 884, loss is 0.00019998221250716597\n",
      "epoch: 4 step: 885, loss is 0.0024388094898313284\n",
      "epoch: 4 step: 886, loss is 0.1346554309129715\n",
      "epoch: 4 step: 887, loss is 0.016257498413324356\n",
      "epoch: 4 step: 888, loss is 0.0006965715438127518\n",
      "epoch: 4 step: 889, loss is 0.0023387412074953318\n",
      "epoch: 4 step: 890, loss is 0.009797937236726284\n",
      "epoch: 4 step: 891, loss is 0.011165352538228035\n",
      "epoch: 4 step: 892, loss is 0.006051059812307358\n",
      "epoch: 4 step: 893, loss is 0.008176666684448719\n",
      "epoch: 4 step: 894, loss is 0.0006979610770940781\n",
      "epoch: 4 step: 895, loss is 0.10765384882688522\n",
      "epoch: 4 step: 896, loss is 0.026619790121912956\n",
      "epoch: 4 step: 897, loss is 0.0004673951189033687\n",
      "epoch: 4 step: 898, loss is 0.0013829611707478762\n",
      "epoch: 4 step: 899, loss is 0.0022023923229426146\n",
      "epoch: 4 step: 900, loss is 0.010162205435335636\n",
      "epoch: 4 step: 901, loss is 0.029055755585432053\n",
      "epoch: 4 step: 902, loss is 0.010445971973240376\n",
      "epoch: 4 step: 903, loss is 0.10660471767187119\n",
      "epoch: 4 step: 904, loss is 0.00031884617055766284\n",
      "epoch: 4 step: 905, loss is 0.0030191424302756786\n",
      "epoch: 4 step: 906, loss is 0.11722061038017273\n",
      "epoch: 4 step: 907, loss is 0.07545524090528488\n",
      "epoch: 4 step: 908, loss is 0.003796276869252324\n",
      "epoch: 4 step: 909, loss is 0.03358864039182663\n",
      "epoch: 4 step: 910, loss is 0.2067437767982483\n",
      "epoch: 4 step: 911, loss is 0.0015380466356873512\n",
      "epoch: 4 step: 912, loss is 0.004773729480803013\n",
      "epoch: 4 step: 913, loss is 0.004854640457779169\n",
      "epoch: 4 step: 914, loss is 0.07154157012701035\n",
      "epoch: 4 step: 915, loss is 0.0009070818778127432\n",
      "epoch: 4 step: 916, loss is 0.0913115069270134\n",
      "epoch: 4 step: 917, loss is 0.006476340349763632\n",
      "epoch: 4 step: 918, loss is 0.00026826237444765866\n",
      "epoch: 4 step: 919, loss is 0.0019435936119407415\n",
      "epoch: 4 step: 920, loss is 0.027122648432850838\n",
      "epoch: 4 step: 921, loss is 0.025522692129015923\n",
      "epoch: 4 step: 922, loss is 0.0017234523547813296\n",
      "epoch: 4 step: 923, loss is 0.01266560424119234\n",
      "epoch: 4 step: 924, loss is 0.021729327738285065\n",
      "epoch: 4 step: 925, loss is 0.09158205986022949\n",
      "epoch: 4 step: 926, loss is 0.01170962955802679\n",
      "epoch: 4 step: 927, loss is 0.037957534193992615\n",
      "epoch: 4 step: 928, loss is 0.0013599327066913247\n",
      "epoch: 4 step: 929, loss is 0.1180420070886612\n",
      "epoch: 4 step: 930, loss is 0.05691914260387421\n",
      "epoch: 4 step: 931, loss is 0.02842593565583229\n",
      "epoch: 4 step: 932, loss is 0.002599780447781086\n",
      "epoch: 4 step: 933, loss is 5.171004886506125e-05\n",
      "epoch: 4 step: 934, loss is 0.006074003875255585\n",
      "epoch: 4 step: 935, loss is 0.004845352377742529\n",
      "epoch: 4 step: 936, loss is 0.004261505324393511\n",
      "epoch: 4 step: 937, loss is 0.0001268299820367247\n",
      "epoch: 4 step: 938, loss is 0.06783339381217957\n",
      "epoch: 4 step: 939, loss is 0.007698698900640011\n",
      "epoch: 4 step: 940, loss is 0.000617383630014956\n",
      "epoch: 4 step: 941, loss is 0.0010604731505736709\n",
      "epoch: 4 step: 942, loss is 0.0004122971440665424\n",
      "epoch: 4 step: 943, loss is 0.0031800083816051483\n",
      "epoch: 4 step: 944, loss is 0.13854213058948517\n",
      "epoch: 4 step: 945, loss is 0.002597385784611106\n",
      "epoch: 4 step: 946, loss is 0.003428810043260455\n",
      "epoch: 4 step: 947, loss is 0.08082305639982224\n",
      "epoch: 4 step: 948, loss is 0.0036785036791116\n",
      "epoch: 4 step: 949, loss is 0.012486530467867851\n",
      "epoch: 4 step: 950, loss is 0.0007385845528915524\n",
      "epoch: 4 step: 951, loss is 0.0015177377499639988\n",
      "epoch: 4 step: 952, loss is 0.0004494095337577164\n",
      "epoch: 4 step: 953, loss is 0.35676339268684387\n",
      "epoch: 4 step: 954, loss is 0.0002709330292418599\n",
      "epoch: 4 step: 955, loss is 0.017626214772462845\n",
      "epoch: 4 step: 956, loss is 0.001723055960610509\n",
      "epoch: 4 step: 957, loss is 0.007741563022136688\n",
      "epoch: 4 step: 958, loss is 0.02911258116364479\n",
      "epoch: 4 step: 959, loss is 0.15460805594921112\n",
      "epoch: 4 step: 960, loss is 0.0042000520043075085\n",
      "epoch: 4 step: 961, loss is 0.005944525357335806\n",
      "epoch: 4 step: 962, loss is 0.00023106923617888242\n",
      "epoch: 4 step: 963, loss is 0.05520997196435928\n",
      "epoch: 4 step: 964, loss is 0.003725344082340598\n",
      "epoch: 4 step: 965, loss is 0.0023908226285129786\n",
      "epoch: 4 step: 966, loss is 0.005693702958524227\n",
      "epoch: 4 step: 967, loss is 0.0003041325253434479\n",
      "epoch: 4 step: 968, loss is 0.00022425854695029557\n",
      "epoch: 4 step: 969, loss is 0.03402421623468399\n",
      "epoch: 4 step: 970, loss is 0.021409926936030388\n",
      "epoch: 4 step: 971, loss is 0.00038181612035259604\n",
      "epoch: 4 step: 972, loss is 0.003414096776396036\n",
      "epoch: 4 step: 973, loss is 0.17179003357887268\n",
      "epoch: 4 step: 974, loss is 0.0233999602496624\n",
      "epoch: 4 step: 975, loss is 0.00016426941147074103\n",
      "epoch: 4 step: 976, loss is 0.0006335403886623681\n",
      "epoch: 4 step: 977, loss is 0.0019194785272702575\n",
      "epoch: 4 step: 978, loss is 0.0010326005285605788\n",
      "epoch: 4 step: 979, loss is 0.000727946637198329\n",
      "epoch: 4 step: 980, loss is 0.24378849565982819\n",
      "epoch: 4 step: 981, loss is 0.061631809920072556\n",
      "epoch: 4 step: 982, loss is 0.00132087676320225\n",
      "epoch: 4 step: 983, loss is 0.08496485650539398\n",
      "epoch: 4 step: 984, loss is 0.0037175300531089306\n",
      "epoch: 4 step: 985, loss is 0.031686995178461075\n",
      "epoch: 4 step: 986, loss is 0.0030610659159719944\n",
      "epoch: 4 step: 987, loss is 0.0013445898657664657\n",
      "epoch: 4 step: 988, loss is 0.011412153020501137\n",
      "epoch: 4 step: 989, loss is 0.005567797925323248\n",
      "epoch: 4 step: 990, loss is 0.07712510228157043\n",
      "epoch: 4 step: 991, loss is 0.036559708416461945\n",
      "epoch: 4 step: 992, loss is 0.12757731974124908\n",
      "epoch: 4 step: 993, loss is 0.11920908093452454\n",
      "epoch: 4 step: 994, loss is 0.044618457555770874\n",
      "epoch: 4 step: 995, loss is 0.0017310017719864845\n",
      "epoch: 4 step: 996, loss is 0.004179047420620918\n",
      "epoch: 4 step: 997, loss is 0.009748000651597977\n",
      "epoch: 4 step: 998, loss is 0.03888629376888275\n",
      "epoch: 4 step: 999, loss is 0.008694091811776161\n",
      "epoch: 4 step: 1000, loss is 0.004836444742977619\n",
      "epoch: 4 step: 1001, loss is 0.16597016155719757\n",
      "epoch: 4 step: 1002, loss is 0.00843034591525793\n",
      "epoch: 4 step: 1003, loss is 0.02522863633930683\n",
      "epoch: 4 step: 1004, loss is 0.018145103007555008\n",
      "epoch: 4 step: 1005, loss is 0.0027680909261107445\n",
      "epoch: 4 step: 1006, loss is 0.0019528649281710386\n",
      "epoch: 4 step: 1007, loss is 0.0805717259645462\n",
      "epoch: 4 step: 1008, loss is 0.023963816463947296\n",
      "epoch: 4 step: 1009, loss is 0.14503587782382965\n",
      "epoch: 4 step: 1010, loss is 0.037854474037885666\n",
      "epoch: 4 step: 1011, loss is 0.007413887418806553\n",
      "epoch: 4 step: 1012, loss is 0.15045583248138428\n",
      "epoch: 4 step: 1013, loss is 0.007556007709354162\n",
      "epoch: 4 step: 1014, loss is 0.007209802512079477\n",
      "epoch: 4 step: 1015, loss is 0.007272837683558464\n",
      "epoch: 4 step: 1016, loss is 0.07027510553598404\n",
      "epoch: 4 step: 1017, loss is 0.011701762676239014\n",
      "epoch: 4 step: 1018, loss is 0.028879236429929733\n",
      "epoch: 4 step: 1019, loss is 0.0007212229538708925\n",
      "epoch: 4 step: 1020, loss is 0.004233023151755333\n",
      "epoch: 4 step: 1021, loss is 0.0025066498201340437\n",
      "epoch: 4 step: 1022, loss is 0.0723285898566246\n",
      "epoch: 4 step: 1023, loss is 0.016206078231334686\n",
      "epoch: 4 step: 1024, loss is 0.015907371416687965\n",
      "epoch: 4 step: 1025, loss is 0.00547162676230073\n",
      "epoch: 4 step: 1026, loss is 0.016989905387163162\n",
      "epoch: 4 step: 1027, loss is 0.005505868699401617\n",
      "epoch: 4 step: 1028, loss is 0.016245616599917412\n",
      "epoch: 4 step: 1029, loss is 0.008433890528976917\n",
      "epoch: 4 step: 1030, loss is 0.01670202985405922\n",
      "epoch: 4 step: 1031, loss is 0.06578007340431213\n",
      "epoch: 4 step: 1032, loss is 0.016384001821279526\n",
      "epoch: 4 step: 1033, loss is 0.0007745636394247413\n",
      "epoch: 4 step: 1034, loss is 0.0014732307754456997\n",
      "epoch: 4 step: 1035, loss is 0.02246015891432762\n",
      "epoch: 4 step: 1036, loss is 0.014263739809393883\n",
      "epoch: 4 step: 1037, loss is 0.012518876232206821\n",
      "epoch: 4 step: 1038, loss is 0.009901173412799835\n",
      "epoch: 4 step: 1039, loss is 0.028251489624381065\n",
      "epoch: 4 step: 1040, loss is 0.06821251660585403\n",
      "epoch: 4 step: 1041, loss is 0.16474772989749908\n",
      "epoch: 4 step: 1042, loss is 0.016152312979102135\n",
      "epoch: 4 step: 1043, loss is 0.048947349190711975\n",
      "epoch: 4 step: 1044, loss is 0.005593071226030588\n",
      "epoch: 4 step: 1045, loss is 0.00032548035960644484\n",
      "epoch: 4 step: 1046, loss is 0.010667056776583195\n",
      "epoch: 4 step: 1047, loss is 0.001835426315665245\n",
      "epoch: 4 step: 1048, loss is 0.0033303159289062023\n",
      "epoch: 4 step: 1049, loss is 0.004775447305291891\n",
      "epoch: 4 step: 1050, loss is 0.008327807299792767\n",
      "epoch: 4 step: 1051, loss is 0.00802267249673605\n",
      "epoch: 4 step: 1052, loss is 0.00530917989090085\n",
      "epoch: 4 step: 1053, loss is 0.006354028359055519\n",
      "epoch: 4 step: 1054, loss is 0.12374080717563629\n",
      "epoch: 4 step: 1055, loss is 0.004035761579871178\n",
      "epoch: 4 step: 1056, loss is 0.02658761292695999\n",
      "epoch: 4 step: 1057, loss is 0.0010279153939336538\n",
      "epoch: 4 step: 1058, loss is 0.02084202878177166\n",
      "epoch: 4 step: 1059, loss is 0.02201642096042633\n",
      "epoch: 4 step: 1060, loss is 0.002387316431850195\n",
      "epoch: 4 step: 1061, loss is 0.0007863459759391844\n",
      "epoch: 4 step: 1062, loss is 0.0005041183903813362\n",
      "epoch: 4 step: 1063, loss is 0.008434424176812172\n",
      "epoch: 4 step: 1064, loss is 0.038070932030677795\n",
      "epoch: 4 step: 1065, loss is 0.0747077539563179\n",
      "epoch: 4 step: 1066, loss is 0.03780725598335266\n",
      "epoch: 4 step: 1067, loss is 0.014070343226194382\n",
      "epoch: 4 step: 1068, loss is 0.017135480418801308\n",
      "epoch: 4 step: 1069, loss is 0.009494039230048656\n",
      "epoch: 4 step: 1070, loss is 0.0033497975673526525\n",
      "epoch: 4 step: 1071, loss is 0.034448329359292984\n",
      "epoch: 4 step: 1072, loss is 0.027375884354114532\n",
      "epoch: 4 step: 1073, loss is 0.007406655699014664\n",
      "epoch: 4 step: 1074, loss is 0.0028914092108607292\n",
      "epoch: 4 step: 1075, loss is 0.0073900423012673855\n",
      "epoch: 4 step: 1076, loss is 0.04033562168478966\n",
      "epoch: 4 step: 1077, loss is 0.17864301800727844\n",
      "epoch: 4 step: 1078, loss is 0.010641279630362988\n",
      "epoch: 4 step: 1079, loss is 0.00013775253319181502\n",
      "epoch: 4 step: 1080, loss is 0.00917043723165989\n",
      "epoch: 4 step: 1081, loss is 0.015408920124173164\n",
      "epoch: 4 step: 1082, loss is 0.016867296770215034\n",
      "epoch: 4 step: 1083, loss is 0.0007458966574631631\n",
      "epoch: 4 step: 1084, loss is 0.06365163624286652\n",
      "epoch: 4 step: 1085, loss is 0.09953849762678146\n",
      "epoch: 4 step: 1086, loss is 0.010633955709636211\n",
      "epoch: 4 step: 1087, loss is 0.0005017929361201823\n",
      "epoch: 4 step: 1088, loss is 0.028984826058149338\n",
      "epoch: 4 step: 1089, loss is 0.13132399320602417\n",
      "epoch: 4 step: 1090, loss is 0.0022659441456198692\n",
      "epoch: 4 step: 1091, loss is 0.004310184624046087\n",
      "epoch: 4 step: 1092, loss is 0.00540913175791502\n",
      "epoch: 4 step: 1093, loss is 0.011172079481184483\n",
      "epoch: 4 step: 1094, loss is 0.009594609029591084\n",
      "epoch: 4 step: 1095, loss is 0.17239192128181458\n",
      "epoch: 4 step: 1096, loss is 0.006576129235327244\n",
      "epoch: 4 step: 1097, loss is 0.15726281702518463\n",
      "epoch: 4 step: 1098, loss is 0.03769483044743538\n",
      "epoch: 4 step: 1099, loss is 0.008403198793530464\n",
      "epoch: 4 step: 1100, loss is 0.01576404459774494\n",
      "epoch: 4 step: 1101, loss is 0.03722470626235008\n",
      "epoch: 4 step: 1102, loss is 0.12425142526626587\n",
      "epoch: 4 step: 1103, loss is 0.005886114202439785\n",
      "epoch: 4 step: 1104, loss is 0.021961266174912453\n",
      "epoch: 4 step: 1105, loss is 0.05256706476211548\n",
      "epoch: 4 step: 1106, loss is 0.012831652536988258\n",
      "epoch: 4 step: 1107, loss is 0.03192110359668732\n",
      "epoch: 4 step: 1108, loss is 0.019259708002209663\n",
      "epoch: 4 step: 1109, loss is 0.14216136932373047\n",
      "epoch: 4 step: 1110, loss is 0.010797993279993534\n",
      "epoch: 4 step: 1111, loss is 0.019901862367987633\n",
      "epoch: 4 step: 1112, loss is 7.46467849239707e-05\n",
      "epoch: 4 step: 1113, loss is 0.07736014574766159\n",
      "epoch: 4 step: 1114, loss is 0.006568672135472298\n",
      "epoch: 4 step: 1115, loss is 0.0019514999585226178\n",
      "epoch: 4 step: 1116, loss is 0.0023517501540482044\n",
      "epoch: 4 step: 1117, loss is 0.01831042394042015\n",
      "epoch: 4 step: 1118, loss is 0.009275508113205433\n",
      "epoch: 4 step: 1119, loss is 0.005349311511963606\n",
      "epoch: 4 step: 1120, loss is 0.008263692259788513\n",
      "epoch: 4 step: 1121, loss is 0.001931870705448091\n",
      "epoch: 4 step: 1122, loss is 0.0033662221394479275\n",
      "epoch: 4 step: 1123, loss is 0.0009149751858785748\n",
      "epoch: 4 step: 1124, loss is 0.08840861916542053\n",
      "epoch: 4 step: 1125, loss is 0.0057526640594005585\n",
      "epoch: 4 step: 1126, loss is 0.004184533376246691\n",
      "epoch: 4 step: 1127, loss is 0.04340186342597008\n",
      "epoch: 4 step: 1128, loss is 0.038168031722307205\n",
      "epoch: 4 step: 1129, loss is 0.0038612147327512503\n",
      "epoch: 4 step: 1130, loss is 0.16872771084308624\n",
      "epoch: 4 step: 1131, loss is 0.000288313691271469\n",
      "epoch: 4 step: 1132, loss is 0.00015168670506682247\n",
      "epoch: 4 step: 1133, loss is 0.00019129147403873503\n",
      "epoch: 4 step: 1134, loss is 0.0005460757529363036\n",
      "epoch: 4 step: 1135, loss is 0.15012907981872559\n",
      "epoch: 4 step: 1136, loss is 0.007567782886326313\n",
      "epoch: 4 step: 1137, loss is 0.01909010112285614\n",
      "epoch: 4 step: 1138, loss is 0.0016786158084869385\n",
      "epoch: 4 step: 1139, loss is 0.00266739958897233\n",
      "epoch: 4 step: 1140, loss is 0.00116917269770056\n",
      "epoch: 4 step: 1141, loss is 0.014878727495670319\n",
      "epoch: 4 step: 1142, loss is 0.005528947338461876\n",
      "epoch: 4 step: 1143, loss is 0.01598556898534298\n",
      "epoch: 4 step: 1144, loss is 0.002078966237604618\n",
      "epoch: 4 step: 1145, loss is 0.004794015083462\n",
      "epoch: 4 step: 1146, loss is 0.00033733039163053036\n",
      "epoch: 4 step: 1147, loss is 0.0007663556607440114\n",
      "epoch: 4 step: 1148, loss is 0.1044209823012352\n",
      "epoch: 4 step: 1149, loss is 0.001089923083782196\n",
      "epoch: 4 step: 1150, loss is 0.0762491300702095\n",
      "epoch: 4 step: 1151, loss is 8.645860361866653e-05\n",
      "epoch: 4 step: 1152, loss is 0.00263049453496933\n",
      "epoch: 4 step: 1153, loss is 0.020998802036046982\n",
      "epoch: 4 step: 1154, loss is 0.0012377979001030326\n",
      "epoch: 4 step: 1155, loss is 0.0011562991421669722\n",
      "epoch: 4 step: 1156, loss is 0.0003560383047442883\n",
      "epoch: 4 step: 1157, loss is 0.0645521804690361\n",
      "epoch: 4 step: 1158, loss is 0.009279279038310051\n",
      "epoch: 4 step: 1159, loss is 0.05834316089749336\n",
      "epoch: 4 step: 1160, loss is 0.02763586863875389\n",
      "epoch: 4 step: 1161, loss is 0.0021010434720665216\n",
      "epoch: 4 step: 1162, loss is 0.006754661910235882\n",
      "epoch: 4 step: 1163, loss is 0.0016703649889677763\n",
      "epoch: 4 step: 1164, loss is 0.0011445777490735054\n",
      "epoch: 4 step: 1165, loss is 0.024120956659317017\n",
      "epoch: 4 step: 1166, loss is 0.045668117702007294\n",
      "epoch: 4 step: 1167, loss is 0.00036729435669258237\n",
      "epoch: 4 step: 1168, loss is 0.0005389960133470595\n",
      "epoch: 4 step: 1169, loss is 0.010175902396440506\n",
      "epoch: 4 step: 1170, loss is 0.0018676476320251822\n",
      "epoch: 4 step: 1171, loss is 0.017440330237150192\n",
      "epoch: 4 step: 1172, loss is 0.49306079745292664\n",
      "epoch: 4 step: 1173, loss is 0.0008606644696556032\n",
      "epoch: 4 step: 1174, loss is 0.0050272527150809765\n",
      "epoch: 4 step: 1175, loss is 0.22143347561359406\n",
      "epoch: 4 step: 1176, loss is 0.0011258840095251799\n",
      "epoch: 4 step: 1177, loss is 0.001785171334631741\n",
      "epoch: 4 step: 1178, loss is 0.00809965468943119\n",
      "epoch: 4 step: 1179, loss is 0.0035832778085023165\n",
      "epoch: 4 step: 1180, loss is 0.007489632815122604\n",
      "epoch: 4 step: 1181, loss is 0.003436775878071785\n",
      "epoch: 4 step: 1182, loss is 0.0002653474803082645\n",
      "epoch: 4 step: 1183, loss is 0.20777718722820282\n",
      "epoch: 4 step: 1184, loss is 0.03695153817534447\n",
      "epoch: 4 step: 1185, loss is 0.0012495602713897824\n",
      "epoch: 4 step: 1186, loss is 0.0187278650701046\n",
      "epoch: 4 step: 1187, loss is 0.09530967473983765\n",
      "epoch: 4 step: 1188, loss is 0.20464357733726501\n",
      "epoch: 4 step: 1189, loss is 0.021197393536567688\n",
      "epoch: 4 step: 1190, loss is 0.010094613768160343\n",
      "epoch: 4 step: 1191, loss is 0.10598151385784149\n",
      "epoch: 4 step: 1192, loss is 0.4713234007358551\n",
      "epoch: 4 step: 1193, loss is 0.09516346454620361\n",
      "epoch: 4 step: 1194, loss is 0.04468025267124176\n",
      "epoch: 4 step: 1195, loss is 0.1133333295583725\n",
      "epoch: 4 step: 1196, loss is 0.08145303279161453\n",
      "epoch: 4 step: 1197, loss is 0.0363960824906826\n",
      "epoch: 4 step: 1198, loss is 0.04099821671843529\n",
      "epoch: 4 step: 1199, loss is 0.006064882501959801\n",
      "epoch: 4 step: 1200, loss is 0.09131333976984024\n",
      "epoch: 4 step: 1201, loss is 0.19982369244098663\n",
      "epoch: 4 step: 1202, loss is 0.1607842743396759\n",
      "epoch: 4 step: 1203, loss is 0.023536575958132744\n",
      "epoch: 4 step: 1204, loss is 0.02066575177013874\n",
      "epoch: 4 step: 1205, loss is 0.004948852583765984\n",
      "epoch: 4 step: 1206, loss is 0.017834695056080818\n",
      "epoch: 4 step: 1207, loss is 0.014345635659992695\n",
      "epoch: 4 step: 1208, loss is 0.10400275141000748\n",
      "epoch: 4 step: 1209, loss is 0.014421339146792889\n",
      "epoch: 4 step: 1210, loss is 0.07831314206123352\n",
      "epoch: 4 step: 1211, loss is 0.11551711708307266\n",
      "epoch: 4 step: 1212, loss is 0.018864106386899948\n",
      "epoch: 4 step: 1213, loss is 0.03445296734571457\n",
      "epoch: 4 step: 1214, loss is 0.031580593436956406\n",
      "epoch: 4 step: 1215, loss is 0.012027901597321033\n",
      "epoch: 4 step: 1216, loss is 0.04626566544175148\n",
      "epoch: 4 step: 1217, loss is 0.14102163910865784\n",
      "epoch: 4 step: 1218, loss is 0.0022155942860990763\n",
      "epoch: 4 step: 1219, loss is 0.017405765131115913\n",
      "epoch: 4 step: 1220, loss is 0.05758383125066757\n",
      "epoch: 4 step: 1221, loss is 0.13955767452716827\n",
      "epoch: 4 step: 1222, loss is 0.022323882207274437\n",
      "epoch: 4 step: 1223, loss is 0.014293385669589043\n",
      "epoch: 4 step: 1224, loss is 0.03062237612903118\n",
      "epoch: 4 step: 1225, loss is 0.010731867514550686\n",
      "epoch: 4 step: 1226, loss is 0.08162336051464081\n",
      "epoch: 4 step: 1227, loss is 0.010948320850729942\n",
      "epoch: 4 step: 1228, loss is 0.03385317698121071\n",
      "epoch: 4 step: 1229, loss is 0.06450311839580536\n",
      "epoch: 4 step: 1230, loss is 0.0930042415857315\n",
      "epoch: 4 step: 1231, loss is 0.039403434842824936\n",
      "epoch: 4 step: 1232, loss is 0.15285445749759674\n",
      "epoch: 4 step: 1233, loss is 0.1900927722454071\n",
      "epoch: 4 step: 1234, loss is 0.17201723158359528\n",
      "epoch: 4 step: 1235, loss is 0.0063485135324299335\n",
      "epoch: 4 step: 1236, loss is 0.04180692881345749\n",
      "epoch: 4 step: 1237, loss is 0.08861326426267624\n",
      "epoch: 4 step: 1238, loss is 0.002444826764985919\n",
      "epoch: 4 step: 1239, loss is 0.08770669251680374\n",
      "epoch: 4 step: 1240, loss is 0.005727274343371391\n",
      "epoch: 4 step: 1241, loss is 0.05696337670087814\n",
      "epoch: 4 step: 1242, loss is 0.003578154370188713\n",
      "epoch: 4 step: 1243, loss is 0.03359832987189293\n",
      "epoch: 4 step: 1244, loss is 0.027396099641919136\n",
      "epoch: 4 step: 1245, loss is 0.007707876153290272\n",
      "epoch: 4 step: 1246, loss is 0.009550143033266068\n",
      "epoch: 4 step: 1247, loss is 0.11740189045667648\n",
      "epoch: 4 step: 1248, loss is 0.038086824119091034\n",
      "epoch: 4 step: 1249, loss is 0.031422317028045654\n",
      "epoch: 4 step: 1250, loss is 0.010824425145983696\n",
      "epoch: 4 step: 1251, loss is 0.011697069741785526\n",
      "epoch: 4 step: 1252, loss is 0.010455586947500706\n",
      "epoch: 4 step: 1253, loss is 0.015685442835092545\n",
      "epoch: 4 step: 1254, loss is 0.005300784483551979\n",
      "epoch: 4 step: 1255, loss is 0.009423361159861088\n",
      "epoch: 4 step: 1256, loss is 0.2755742073059082\n",
      "epoch: 4 step: 1257, loss is 0.008975195698440075\n",
      "epoch: 4 step: 1258, loss is 0.001012394204735756\n",
      "epoch: 4 step: 1259, loss is 0.012416639365255833\n",
      "epoch: 4 step: 1260, loss is 0.004679642617702484\n",
      "epoch: 4 step: 1261, loss is 0.006117247510701418\n",
      "epoch: 4 step: 1262, loss is 0.005169800948351622\n",
      "epoch: 4 step: 1263, loss is 0.002766173565760255\n",
      "epoch: 4 step: 1264, loss is 0.005258762277662754\n",
      "epoch: 4 step: 1265, loss is 0.027568405494093895\n",
      "epoch: 4 step: 1266, loss is 0.04375975579023361\n",
      "epoch: 4 step: 1267, loss is 0.018892955034971237\n",
      "epoch: 4 step: 1268, loss is 0.0018352405168116093\n",
      "epoch: 4 step: 1269, loss is 0.019118178635835648\n",
      "epoch: 4 step: 1270, loss is 0.05130809172987938\n",
      "epoch: 4 step: 1271, loss is 0.0023189641069620848\n",
      "epoch: 4 step: 1272, loss is 0.0120826605707407\n",
      "epoch: 4 step: 1273, loss is 0.0024484407622367144\n",
      "epoch: 4 step: 1274, loss is 0.010244498960673809\n",
      "epoch: 4 step: 1275, loss is 0.03336390107870102\n",
      "epoch: 4 step: 1276, loss is 0.006658576428890228\n",
      "epoch: 4 step: 1277, loss is 0.004155111499130726\n",
      "epoch: 4 step: 1278, loss is 0.0007657342939637601\n",
      "epoch: 4 step: 1279, loss is 0.004012493882328272\n",
      "epoch: 4 step: 1280, loss is 0.0014746636152267456\n",
      "epoch: 4 step: 1281, loss is 0.037366803735494614\n",
      "epoch: 4 step: 1282, loss is 0.002759838942438364\n",
      "epoch: 4 step: 1283, loss is 0.0024339347146451473\n",
      "epoch: 4 step: 1284, loss is 0.0038839990738779306\n",
      "epoch: 4 step: 1285, loss is 0.00419630529358983\n",
      "epoch: 4 step: 1286, loss is 0.03139088675379753\n",
      "epoch: 4 step: 1287, loss is 0.021476348862051964\n",
      "epoch: 4 step: 1288, loss is 0.05117236450314522\n",
      "epoch: 4 step: 1289, loss is 0.001024984521791339\n",
      "epoch: 4 step: 1290, loss is 0.019561832770705223\n",
      "epoch: 4 step: 1291, loss is 0.16941428184509277\n",
      "epoch: 4 step: 1292, loss is 0.06021353229880333\n",
      "epoch: 4 step: 1293, loss is 0.026096003130078316\n",
      "epoch: 4 step: 1294, loss is 0.0527179054915905\n",
      "epoch: 4 step: 1295, loss is 0.002318359212949872\n",
      "epoch: 4 step: 1296, loss is 0.043492641299963\n",
      "epoch: 4 step: 1297, loss is 0.016602711752057076\n",
      "epoch: 4 step: 1298, loss is 0.00966346450150013\n",
      "epoch: 4 step: 1299, loss is 0.0733305811882019\n",
      "epoch: 4 step: 1300, loss is 0.10019741207361221\n",
      "epoch: 4 step: 1301, loss is 0.005798649042844772\n",
      "epoch: 4 step: 1302, loss is 0.010556263849139214\n",
      "epoch: 4 step: 1303, loss is 0.044675927609205246\n",
      "epoch: 4 step: 1304, loss is 0.001224609324708581\n",
      "epoch: 4 step: 1305, loss is 0.007461806759238243\n",
      "epoch: 4 step: 1306, loss is 0.005894913338124752\n",
      "epoch: 4 step: 1307, loss is 0.026246903464198112\n",
      "epoch: 4 step: 1308, loss is 0.0021442179568111897\n",
      "epoch: 4 step: 1309, loss is 0.0010754286777228117\n",
      "epoch: 4 step: 1310, loss is 0.237177312374115\n",
      "epoch: 4 step: 1311, loss is 0.11336855590343475\n",
      "epoch: 4 step: 1312, loss is 0.08338581025600433\n",
      "epoch: 4 step: 1313, loss is 0.002205782802775502\n",
      "epoch: 4 step: 1314, loss is 0.08630045503377914\n",
      "epoch: 4 step: 1315, loss is 0.0006901557208038867\n",
      "epoch: 4 step: 1316, loss is 0.08290908485651016\n",
      "epoch: 4 step: 1317, loss is 0.005477063823491335\n",
      "epoch: 4 step: 1318, loss is 0.1572273224592209\n",
      "epoch: 4 step: 1319, loss is 0.004211627412587404\n",
      "epoch: 4 step: 1320, loss is 0.01937801204621792\n",
      "epoch: 4 step: 1321, loss is 0.06441466510295868\n",
      "epoch: 4 step: 1322, loss is 0.0019319014390930533\n",
      "epoch: 4 step: 1323, loss is 0.0011649879161268473\n",
      "epoch: 4 step: 1324, loss is 0.006854885723441839\n",
      "epoch: 4 step: 1325, loss is 0.007224271073937416\n",
      "epoch: 4 step: 1326, loss is 0.030119825154542923\n",
      "epoch: 4 step: 1327, loss is 0.16030806303024292\n",
      "epoch: 4 step: 1328, loss is 0.000881078012753278\n",
      "epoch: 4 step: 1329, loss is 0.008428545668721199\n",
      "epoch: 4 step: 1330, loss is 0.0012478259159252048\n",
      "epoch: 4 step: 1331, loss is 0.007666787598282099\n",
      "epoch: 4 step: 1332, loss is 0.0007454473525285721\n",
      "epoch: 4 step: 1333, loss is 0.0055124154314398766\n",
      "epoch: 4 step: 1334, loss is 0.2386505901813507\n",
      "epoch: 4 step: 1335, loss is 0.0019187636207789183\n",
      "epoch: 4 step: 1336, loss is 0.005246234592050314\n",
      "epoch: 4 step: 1337, loss is 0.07700670510530472\n",
      "epoch: 4 step: 1338, loss is 0.017590733245015144\n",
      "epoch: 4 step: 1339, loss is 0.013251427561044693\n",
      "epoch: 4 step: 1340, loss is 0.0023397316690534353\n",
      "epoch: 4 step: 1341, loss is 0.08380905538797379\n",
      "epoch: 4 step: 1342, loss is 0.0055503989569842815\n",
      "epoch: 4 step: 1343, loss is 0.008476043120026588\n",
      "epoch: 4 step: 1344, loss is 0.07010388374328613\n",
      "epoch: 4 step: 1345, loss is 0.0009386964957229793\n",
      "epoch: 4 step: 1346, loss is 0.004270289093255997\n",
      "epoch: 4 step: 1347, loss is 0.014686373993754387\n",
      "epoch: 4 step: 1348, loss is 0.0008208249928429723\n",
      "epoch: 4 step: 1349, loss is 0.0011440946254879236\n",
      "epoch: 4 step: 1350, loss is 0.12294870615005493\n",
      "epoch: 4 step: 1351, loss is 0.009756682440638542\n",
      "epoch: 4 step: 1352, loss is 0.0754888579249382\n",
      "epoch: 4 step: 1353, loss is 0.09632664173841476\n",
      "epoch: 4 step: 1354, loss is 0.11852697283029556\n",
      "epoch: 4 step: 1355, loss is 0.2326185703277588\n",
      "epoch: 4 step: 1356, loss is 0.05289085954427719\n",
      "epoch: 4 step: 1357, loss is 0.0031993405427783728\n",
      "epoch: 4 step: 1358, loss is 0.12942096590995789\n",
      "epoch: 4 step: 1359, loss is 0.0005665363860316575\n",
      "epoch: 4 step: 1360, loss is 0.0027273341547697783\n",
      "epoch: 4 step: 1361, loss is 0.00033403042471036315\n",
      "epoch: 4 step: 1362, loss is 0.036098893731832504\n",
      "epoch: 4 step: 1363, loss is 0.012537535279989243\n",
      "epoch: 4 step: 1364, loss is 0.0005431277677416801\n",
      "epoch: 4 step: 1365, loss is 0.0024478158447891474\n",
      "epoch: 4 step: 1366, loss is 0.0016521922079846263\n",
      "epoch: 4 step: 1367, loss is 0.00330550130456686\n",
      "epoch: 4 step: 1368, loss is 0.06650169938802719\n",
      "epoch: 4 step: 1369, loss is 0.1998954862356186\n",
      "epoch: 4 step: 1370, loss is 0.09442099928855896\n",
      "epoch: 4 step: 1371, loss is 0.013214189559221268\n",
      "epoch: 4 step: 1372, loss is 0.013146073557436466\n",
      "epoch: 4 step: 1373, loss is 0.020635511726140976\n",
      "epoch: 4 step: 1374, loss is 0.016066579148173332\n",
      "epoch: 4 step: 1375, loss is 0.23337361216545105\n",
      "epoch: 4 step: 1376, loss is 0.006288514006882906\n",
      "epoch: 4 step: 1377, loss is 0.04122099280357361\n",
      "epoch: 4 step: 1378, loss is 0.06271342933177948\n",
      "epoch: 4 step: 1379, loss is 0.0037424920592457056\n",
      "epoch: 4 step: 1380, loss is 0.11559484899044037\n",
      "epoch: 4 step: 1381, loss is 0.09088459610939026\n",
      "epoch: 4 step: 1382, loss is 0.01825338788330555\n",
      "epoch: 4 step: 1383, loss is 0.030296549201011658\n",
      "epoch: 4 step: 1384, loss is 0.05802929401397705\n",
      "epoch: 4 step: 1385, loss is 0.007360778748989105\n",
      "epoch: 4 step: 1386, loss is 0.03564935177564621\n",
      "epoch: 4 step: 1387, loss is 0.0452192947268486\n",
      "epoch: 4 step: 1388, loss is 0.004457142669707537\n",
      "epoch: 4 step: 1389, loss is 0.023101571947336197\n",
      "epoch: 4 step: 1390, loss is 0.05808475613594055\n",
      "epoch: 4 step: 1391, loss is 0.04861494153738022\n",
      "epoch: 4 step: 1392, loss is 0.09535979479551315\n",
      "epoch: 4 step: 1393, loss is 0.007234959863126278\n",
      "epoch: 4 step: 1394, loss is 0.002002168446779251\n",
      "epoch: 4 step: 1395, loss is 0.0038876004982739687\n",
      "epoch: 4 step: 1396, loss is 0.007458837237209082\n",
      "epoch: 4 step: 1397, loss is 0.22844022512435913\n",
      "epoch: 4 step: 1398, loss is 0.003583823563531041\n",
      "epoch: 4 step: 1399, loss is 0.004314550198614597\n",
      "epoch: 4 step: 1400, loss is 0.0025857952423393726\n",
      "epoch: 4 step: 1401, loss is 0.0031450456008315086\n",
      "epoch: 4 step: 1402, loss is 0.002671079244464636\n",
      "epoch: 4 step: 1403, loss is 0.013330215588212013\n",
      "epoch: 4 step: 1404, loss is 0.005882814526557922\n",
      "epoch: 4 step: 1405, loss is 0.0843605026602745\n",
      "epoch: 4 step: 1406, loss is 0.11646474897861481\n",
      "epoch: 4 step: 1407, loss is 0.22338786721229553\n",
      "epoch: 4 step: 1408, loss is 0.005485248286277056\n",
      "epoch: 4 step: 1409, loss is 0.007428603246808052\n",
      "epoch: 4 step: 1410, loss is 0.040535785257816315\n",
      "epoch: 4 step: 1411, loss is 0.0014446101849898696\n",
      "epoch: 4 step: 1412, loss is 0.009546956978738308\n",
      "epoch: 4 step: 1413, loss is 0.00039484575972892344\n",
      "epoch: 4 step: 1414, loss is 0.008076696656644344\n",
      "epoch: 4 step: 1415, loss is 0.03718841075897217\n",
      "epoch: 4 step: 1416, loss is 0.012288562022149563\n",
      "epoch: 4 step: 1417, loss is 0.006103599444031715\n",
      "epoch: 4 step: 1418, loss is 0.33651694655418396\n",
      "epoch: 4 step: 1419, loss is 0.024295540526509285\n",
      "epoch: 4 step: 1420, loss is 0.01786220259964466\n",
      "epoch: 4 step: 1421, loss is 0.00898930337280035\n",
      "epoch: 4 step: 1422, loss is 0.026562310755252838\n",
      "epoch: 4 step: 1423, loss is 0.02367411181330681\n",
      "epoch: 4 step: 1424, loss is 0.0010346779599785805\n",
      "epoch: 4 step: 1425, loss is 0.02426200732588768\n",
      "epoch: 4 step: 1426, loss is 0.010759929195046425\n",
      "epoch: 4 step: 1427, loss is 0.014191129244863987\n",
      "epoch: 4 step: 1428, loss is 0.024633709341287613\n",
      "epoch: 4 step: 1429, loss is 0.0006482481840066612\n",
      "epoch: 4 step: 1430, loss is 0.005336734000593424\n",
      "epoch: 4 step: 1431, loss is 0.0006364888395182788\n",
      "epoch: 4 step: 1432, loss is 0.009178862906992435\n",
      "epoch: 4 step: 1433, loss is 0.03012504056096077\n",
      "epoch: 4 step: 1434, loss is 0.00039794124313630164\n",
      "epoch: 4 step: 1435, loss is 0.005115302745252848\n",
      "epoch: 4 step: 1436, loss is 0.01116265170276165\n",
      "epoch: 4 step: 1437, loss is 0.030972331762313843\n",
      "epoch: 4 step: 1438, loss is 0.019537730142474174\n",
      "epoch: 4 step: 1439, loss is 0.0018238234333693981\n",
      "epoch: 4 step: 1440, loss is 0.0033670340199023485\n",
      "epoch: 4 step: 1441, loss is 0.0015687390696257353\n",
      "epoch: 4 step: 1442, loss is 0.0008779807249084115\n",
      "epoch: 4 step: 1443, loss is 0.022764846682548523\n",
      "epoch: 4 step: 1444, loss is 0.03765622898936272\n",
      "epoch: 4 step: 1445, loss is 0.04548602178692818\n",
      "epoch: 4 step: 1446, loss is 0.003870977321639657\n",
      "epoch: 4 step: 1447, loss is 0.06464892625808716\n",
      "epoch: 4 step: 1448, loss is 0.0005102920695208013\n",
      "epoch: 4 step: 1449, loss is 0.001671541715040803\n",
      "epoch: 4 step: 1450, loss is 0.2261950522661209\n",
      "epoch: 4 step: 1451, loss is 0.0005858930526301265\n",
      "epoch: 4 step: 1452, loss is 0.06470394879579544\n",
      "epoch: 4 step: 1453, loss is 0.009746384806931019\n",
      "epoch: 4 step: 1454, loss is 0.00234607863239944\n",
      "epoch: 4 step: 1455, loss is 0.022323695942759514\n",
      "epoch: 4 step: 1456, loss is 0.0014880991075187922\n",
      "epoch: 4 step: 1457, loss is 0.01254271063953638\n",
      "epoch: 4 step: 1458, loss is 0.010441239923238754\n",
      "epoch: 4 step: 1459, loss is 0.08095631748437881\n",
      "epoch: 4 step: 1460, loss is 0.0012049054494127631\n",
      "epoch: 4 step: 1461, loss is 0.00045139933354221284\n",
      "epoch: 4 step: 1462, loss is 0.008888778276741505\n",
      "epoch: 4 step: 1463, loss is 0.006227524019777775\n",
      "epoch: 4 step: 1464, loss is 0.0002750040148384869\n",
      "epoch: 4 step: 1465, loss is 0.003091445891186595\n",
      "epoch: 4 step: 1466, loss is 0.028938505798578262\n",
      "epoch: 4 step: 1467, loss is 0.008598847314715385\n",
      "epoch: 4 step: 1468, loss is 0.013115138746798038\n",
      "epoch: 4 step: 1469, loss is 0.007072941400110722\n",
      "epoch: 4 step: 1470, loss is 0.01284179836511612\n",
      "epoch: 4 step: 1471, loss is 0.008594706654548645\n",
      "epoch: 4 step: 1472, loss is 0.09934432804584503\n",
      "epoch: 4 step: 1473, loss is 0.015052366070449352\n",
      "epoch: 4 step: 1474, loss is 0.022694075480103493\n",
      "epoch: 4 step: 1475, loss is 0.00152676017023623\n",
      "epoch: 4 step: 1476, loss is 0.0019689062610268593\n",
      "epoch: 4 step: 1477, loss is 0.24842019379138947\n",
      "epoch: 4 step: 1478, loss is 0.030448783189058304\n",
      "epoch: 4 step: 1479, loss is 0.01556608360260725\n",
      "epoch: 4 step: 1480, loss is 0.1495131552219391\n",
      "epoch: 4 step: 1481, loss is 0.0012157922610640526\n",
      "epoch: 4 step: 1482, loss is 0.0013781297020614147\n",
      "epoch: 4 step: 1483, loss is 0.09480997174978256\n",
      "epoch: 4 step: 1484, loss is 0.0006256919587031007\n",
      "epoch: 4 step: 1485, loss is 0.005527562461793423\n",
      "epoch: 4 step: 1486, loss is 0.14299023151397705\n",
      "epoch: 4 step: 1487, loss is 0.041738323867321014\n",
      "epoch: 4 step: 1488, loss is 0.390225887298584\n",
      "epoch: 4 step: 1489, loss is 0.001999791245907545\n",
      "epoch: 4 step: 1490, loss is 0.016944894567131996\n",
      "epoch: 4 step: 1491, loss is 0.06660464406013489\n",
      "epoch: 4 step: 1492, loss is 0.028180105611681938\n",
      "epoch: 4 step: 1493, loss is 0.00039456793456338346\n",
      "epoch: 4 step: 1494, loss is 0.007833562791347504\n",
      "epoch: 4 step: 1495, loss is 0.21249637007713318\n",
      "epoch: 4 step: 1496, loss is 0.006842863745987415\n",
      "epoch: 4 step: 1497, loss is 0.00412434246391058\n",
      "epoch: 4 step: 1498, loss is 0.0027375228237360716\n",
      "epoch: 4 step: 1499, loss is 0.0011534156510606408\n",
      "epoch: 4 step: 1500, loss is 0.0022543910890817642\n",
      "epoch: 4 step: 1501, loss is 0.0036855917423963547\n",
      "epoch: 4 step: 1502, loss is 0.00437180744484067\n",
      "epoch: 4 step: 1503, loss is 0.10518760979175568\n",
      "epoch: 4 step: 1504, loss is 0.02753932774066925\n",
      "epoch: 4 step: 1505, loss is 0.0013624901184812188\n",
      "epoch: 4 step: 1506, loss is 0.0007604039856232703\n",
      "epoch: 4 step: 1507, loss is 0.0010701015125960112\n",
      "epoch: 4 step: 1508, loss is 0.17413005232810974\n",
      "epoch: 4 step: 1509, loss is 0.1014588326215744\n",
      "epoch: 4 step: 1510, loss is 0.003949329722672701\n",
      "epoch: 4 step: 1511, loss is 0.002106316387653351\n",
      "epoch: 4 step: 1512, loss is 0.14185455441474915\n",
      "epoch: 4 step: 1513, loss is 0.0011253620032221079\n",
      "epoch: 4 step: 1514, loss is 0.006531912367790937\n",
      "epoch: 4 step: 1515, loss is 0.00836179219186306\n",
      "epoch: 4 step: 1516, loss is 0.0014336637686938047\n",
      "epoch: 4 step: 1517, loss is 0.03371173143386841\n",
      "epoch: 4 step: 1518, loss is 0.003758450038731098\n",
      "epoch: 4 step: 1519, loss is 0.002303349319845438\n",
      "epoch: 4 step: 1520, loss is 0.029591236263513565\n",
      "epoch: 4 step: 1521, loss is 0.003150630509480834\n",
      "epoch: 4 step: 1522, loss is 0.0041361344046890736\n",
      "epoch: 4 step: 1523, loss is 0.045244235545396805\n",
      "epoch: 4 step: 1524, loss is 0.04996667429804802\n",
      "epoch: 4 step: 1525, loss is 0.0019310698844492435\n",
      "epoch: 4 step: 1526, loss is 0.03511587530374527\n",
      "epoch: 4 step: 1527, loss is 0.05929907411336899\n",
      "epoch: 4 step: 1528, loss is 0.005978243891149759\n",
      "epoch: 4 step: 1529, loss is 0.15308724343776703\n",
      "epoch: 4 step: 1530, loss is 0.018162909895181656\n",
      "epoch: 4 step: 1531, loss is 0.08214159309864044\n",
      "epoch: 4 step: 1532, loss is 0.0016864680219441652\n",
      "epoch: 4 step: 1533, loss is 0.011856683529913425\n",
      "epoch: 4 step: 1534, loss is 0.002542071742936969\n",
      "epoch: 4 step: 1535, loss is 0.0056944056414067745\n",
      "epoch: 4 step: 1536, loss is 0.0037739952094852924\n",
      "epoch: 4 step: 1537, loss is 0.019326690584421158\n",
      "epoch: 4 step: 1538, loss is 0.0018079980509355664\n",
      "epoch: 4 step: 1539, loss is 0.0006096340948715806\n",
      "epoch: 4 step: 1540, loss is 0.0015573266427963972\n",
      "epoch: 4 step: 1541, loss is 0.04128582775592804\n",
      "epoch: 4 step: 1542, loss is 0.0452565960586071\n",
      "epoch: 4 step: 1543, loss is 0.14262942969799042\n",
      "epoch: 4 step: 1544, loss is 0.10694769769906998\n",
      "epoch: 4 step: 1545, loss is 0.03378210589289665\n",
      "epoch: 4 step: 1546, loss is 0.0206815917044878\n",
      "epoch: 4 step: 1547, loss is 0.0009412447107024491\n",
      "epoch: 4 step: 1548, loss is 0.00021265828399918973\n",
      "epoch: 4 step: 1549, loss is 0.005255152937024832\n",
      "epoch: 4 step: 1550, loss is 0.0023565159644931555\n",
      "epoch: 4 step: 1551, loss is 0.004300608765333891\n",
      "epoch: 4 step: 1552, loss is 0.008515183813869953\n",
      "epoch: 4 step: 1553, loss is 0.07560840994119644\n",
      "epoch: 4 step: 1554, loss is 0.078986257314682\n",
      "epoch: 4 step: 1555, loss is 0.0019411344546824694\n",
      "epoch: 4 step: 1556, loss is 0.1369599550962448\n",
      "epoch: 4 step: 1557, loss is 0.001705209375359118\n",
      "epoch: 4 step: 1558, loss is 0.0016369082732126117\n",
      "epoch: 4 step: 1559, loss is 0.008792571723461151\n",
      "epoch: 4 step: 1560, loss is 0.03916079178452492\n",
      "epoch: 4 step: 1561, loss is 0.0010670516639947891\n",
      "epoch: 4 step: 1562, loss is 0.06076131761074066\n",
      "epoch: 4 step: 1563, loss is 0.00614586565643549\n",
      "epoch: 4 step: 1564, loss is 0.004437012132257223\n",
      "epoch: 4 step: 1565, loss is 0.0010535953333601356\n",
      "epoch: 4 step: 1566, loss is 0.001878230134025216\n",
      "epoch: 4 step: 1567, loss is 0.0014617731794714928\n",
      "epoch: 4 step: 1568, loss is 0.007262547966092825\n",
      "epoch: 4 step: 1569, loss is 0.05857221782207489\n",
      "epoch: 4 step: 1570, loss is 0.056255169212818146\n",
      "epoch: 4 step: 1571, loss is 0.1509702056646347\n",
      "epoch: 4 step: 1572, loss is 0.002106486586853862\n",
      "epoch: 4 step: 1573, loss is 0.005087509751319885\n",
      "epoch: 4 step: 1574, loss is 0.12468384951353073\n",
      "epoch: 4 step: 1575, loss is 0.04115394502878189\n",
      "epoch: 4 step: 1576, loss is 0.02659701369702816\n",
      "epoch: 4 step: 1577, loss is 0.03746270015835762\n",
      "epoch: 4 step: 1578, loss is 0.004254570230841637\n",
      "epoch: 4 step: 1579, loss is 0.029738735407590866\n",
      "epoch: 4 step: 1580, loss is 0.0036904793232679367\n",
      "epoch: 4 step: 1581, loss is 0.08353063464164734\n",
      "epoch: 4 step: 1582, loss is 0.2516070008277893\n",
      "epoch: 4 step: 1583, loss is 0.0011976530076935887\n",
      "epoch: 4 step: 1584, loss is 0.07555022835731506\n",
      "epoch: 4 step: 1585, loss is 0.1515379697084427\n",
      "epoch: 4 step: 1586, loss is 0.020703963935375214\n",
      "epoch: 4 step: 1587, loss is 0.01976555772125721\n",
      "epoch: 4 step: 1588, loss is 0.0035323621705174446\n",
      "epoch: 4 step: 1589, loss is 0.0327121801674366\n",
      "epoch: 4 step: 1590, loss is 0.03876527026295662\n",
      "epoch: 4 step: 1591, loss is 0.06442708522081375\n",
      "epoch: 4 step: 1592, loss is 0.053416308015584946\n",
      "epoch: 4 step: 1593, loss is 0.013201835565268993\n",
      "epoch: 4 step: 1594, loss is 0.0039687189273536205\n",
      "epoch: 4 step: 1595, loss is 0.0014932012418285012\n",
      "epoch: 4 step: 1596, loss is 0.0009732372127473354\n",
      "epoch: 4 step: 1597, loss is 0.03975775092840195\n",
      "epoch: 4 step: 1598, loss is 0.003215756034478545\n",
      "epoch: 4 step: 1599, loss is 0.029506195336580276\n",
      "epoch: 4 step: 1600, loss is 0.0066367932595312595\n",
      "epoch: 4 step: 1601, loss is 0.05871092155575752\n",
      "epoch: 4 step: 1602, loss is 0.0003320658579468727\n",
      "epoch: 4 step: 1603, loss is 0.09511516988277435\n",
      "epoch: 4 step: 1604, loss is 0.011715521104633808\n",
      "epoch: 4 step: 1605, loss is 0.0033596139401197433\n",
      "epoch: 4 step: 1606, loss is 0.0431751050055027\n",
      "epoch: 4 step: 1607, loss is 0.024010853841900826\n",
      "epoch: 4 step: 1608, loss is 0.0029176492244005203\n",
      "epoch: 4 step: 1609, loss is 0.06896741688251495\n",
      "epoch: 4 step: 1610, loss is 0.006798005197197199\n",
      "epoch: 4 step: 1611, loss is 0.0016255597583949566\n",
      "epoch: 4 step: 1612, loss is 0.001753629301674664\n",
      "epoch: 4 step: 1613, loss is 0.001965390983968973\n",
      "epoch: 4 step: 1614, loss is 0.06907255202531815\n",
      "epoch: 4 step: 1615, loss is 0.06342007964849472\n",
      "epoch: 4 step: 1616, loss is 0.028737053275108337\n",
      "epoch: 4 step: 1617, loss is 0.017824264243245125\n",
      "epoch: 4 step: 1618, loss is 0.07602033764123917\n",
      "epoch: 4 step: 1619, loss is 0.015710880979895592\n",
      "epoch: 4 step: 1620, loss is 0.001868535066023469\n",
      "epoch: 4 step: 1621, loss is 0.007020238786935806\n",
      "epoch: 4 step: 1622, loss is 0.0010158001678064466\n",
      "epoch: 4 step: 1623, loss is 0.0026858863420784473\n",
      "epoch: 4 step: 1624, loss is 0.1388588696718216\n",
      "epoch: 4 step: 1625, loss is 0.02523626945912838\n",
      "epoch: 4 step: 1626, loss is 0.003461691550910473\n",
      "epoch: 4 step: 1627, loss is 0.1909489631652832\n",
      "epoch: 4 step: 1628, loss is 0.035422101616859436\n",
      "epoch: 4 step: 1629, loss is 0.04601293429732323\n",
      "epoch: 4 step: 1630, loss is 0.002577562816441059\n",
      "epoch: 4 step: 1631, loss is 0.23727843165397644\n",
      "epoch: 4 step: 1632, loss is 0.0036217779852449894\n",
      "epoch: 4 step: 1633, loss is 0.002560696331784129\n",
      "epoch: 4 step: 1634, loss is 0.018821537494659424\n",
      "epoch: 4 step: 1635, loss is 0.006262721959501505\n",
      "epoch: 4 step: 1636, loss is 0.0015336013166233897\n",
      "epoch: 4 step: 1637, loss is 0.013550389558076859\n",
      "epoch: 4 step: 1638, loss is 0.011499889194965363\n",
      "epoch: 4 step: 1639, loss is 0.002214138861745596\n",
      "epoch: 4 step: 1640, loss is 0.12213147431612015\n",
      "epoch: 4 step: 1641, loss is 0.023605117574334145\n",
      "epoch: 4 step: 1642, loss is 0.058250948786735535\n",
      "epoch: 4 step: 1643, loss is 0.013120654970407486\n",
      "epoch: 4 step: 1644, loss is 0.07698673754930496\n",
      "epoch: 4 step: 1645, loss is 0.002593192970380187\n",
      "epoch: 4 step: 1646, loss is 0.08006489276885986\n",
      "epoch: 4 step: 1647, loss is 0.024396738037467003\n",
      "epoch: 4 step: 1648, loss is 0.021861912682652473\n",
      "epoch: 4 step: 1649, loss is 0.0009274380281567574\n",
      "epoch: 4 step: 1650, loss is 0.012701135128736496\n",
      "epoch: 4 step: 1651, loss is 0.005006167106330395\n",
      "epoch: 4 step: 1652, loss is 0.0022648032754659653\n",
      "epoch: 4 step: 1653, loss is 0.021994348615407944\n",
      "epoch: 4 step: 1654, loss is 0.01343934703618288\n",
      "epoch: 4 step: 1655, loss is 0.0040231505408883095\n",
      "epoch: 4 step: 1656, loss is 0.015869973227381706\n",
      "epoch: 4 step: 1657, loss is 0.03383947163820267\n",
      "epoch: 4 step: 1658, loss is 0.029001399874687195\n",
      "epoch: 4 step: 1659, loss is 0.0573187954723835\n",
      "epoch: 4 step: 1660, loss is 0.008001892827451229\n",
      "epoch: 4 step: 1661, loss is 0.003866895567625761\n",
      "epoch: 4 step: 1662, loss is 0.0038177482783794403\n",
      "epoch: 4 step: 1663, loss is 0.04776904731988907\n",
      "epoch: 4 step: 1664, loss is 0.004182012286037207\n",
      "epoch: 4 step: 1665, loss is 0.01896694116294384\n",
      "epoch: 4 step: 1666, loss is 0.03311673924326897\n",
      "epoch: 4 step: 1667, loss is 0.0005735518643632531\n",
      "epoch: 4 step: 1668, loss is 0.00048762536607682705\n",
      "epoch: 4 step: 1669, loss is 0.09007962048053741\n",
      "epoch: 4 step: 1670, loss is 0.0601770393550396\n",
      "epoch: 4 step: 1671, loss is 0.04628005251288414\n",
      "epoch: 4 step: 1672, loss is 0.007411913946270943\n",
      "epoch: 4 step: 1673, loss is 0.00018711283337324858\n",
      "epoch: 4 step: 1674, loss is 0.1455691009759903\n",
      "epoch: 4 step: 1675, loss is 0.0008563067531213164\n",
      "epoch: 4 step: 1676, loss is 0.013637596741318703\n",
      "epoch: 4 step: 1677, loss is 0.010869006626307964\n",
      "epoch: 4 step: 1678, loss is 0.002808336401358247\n",
      "epoch: 4 step: 1679, loss is 0.0006541500333696604\n",
      "epoch: 4 step: 1680, loss is 0.0020025090780109167\n",
      "epoch: 4 step: 1681, loss is 0.00031693236087448895\n",
      "epoch: 4 step: 1682, loss is 0.005564418155699968\n",
      "epoch: 4 step: 1683, loss is 0.02397940680384636\n",
      "epoch: 4 step: 1684, loss is 0.05554695799946785\n",
      "epoch: 4 step: 1685, loss is 0.03601274639368057\n",
      "epoch: 4 step: 1686, loss is 0.0492565743625164\n",
      "epoch: 4 step: 1687, loss is 0.039457134902477264\n",
      "epoch: 4 step: 1688, loss is 0.003298654453828931\n",
      "epoch: 4 step: 1689, loss is 0.0014510659966617823\n",
      "epoch: 4 step: 1690, loss is 0.0030580898746848106\n",
      "epoch: 4 step: 1691, loss is 0.019645966589450836\n",
      "epoch: 4 step: 1692, loss is 0.0006191860884428024\n",
      "epoch: 4 step: 1693, loss is 0.05393674597144127\n",
      "epoch: 4 step: 1694, loss is 0.012641413137316704\n",
      "epoch: 4 step: 1695, loss is 0.022834282368421555\n",
      "epoch: 4 step: 1696, loss is 0.0006416491814889014\n",
      "epoch: 4 step: 1697, loss is 0.007422644179314375\n",
      "epoch: 4 step: 1698, loss is 0.0018862809520214796\n",
      "epoch: 4 step: 1699, loss is 0.0328812450170517\n",
      "epoch: 4 step: 1700, loss is 0.0015303201507776976\n",
      "epoch: 4 step: 1701, loss is 0.0059072175063192844\n",
      "epoch: 4 step: 1702, loss is 0.0009999769972637296\n",
      "epoch: 4 step: 1703, loss is 0.0054365792311728\n",
      "epoch: 4 step: 1704, loss is 0.0011853267205879092\n",
      "epoch: 4 step: 1705, loss is 0.014856241643428802\n",
      "epoch: 4 step: 1706, loss is 0.010093094781041145\n",
      "epoch: 4 step: 1707, loss is 0.049905989319086075\n",
      "epoch: 4 step: 1708, loss is 0.002192205283790827\n",
      "epoch: 4 step: 1709, loss is 0.0017518504755571485\n",
      "epoch: 4 step: 1710, loss is 0.00041015748865902424\n",
      "epoch: 4 step: 1711, loss is 0.013861767016351223\n",
      "epoch: 4 step: 1712, loss is 0.00010536958870943636\n",
      "epoch: 4 step: 1713, loss is 0.00782514177262783\n",
      "epoch: 4 step: 1714, loss is 0.0009308878215961158\n",
      "epoch: 4 step: 1715, loss is 0.13515445590019226\n",
      "epoch: 4 step: 1716, loss is 0.005121145863085985\n",
      "epoch: 4 step: 1717, loss is 0.0002520968846511096\n",
      "epoch: 4 step: 1718, loss is 0.0031170896254479885\n",
      "epoch: 4 step: 1719, loss is 0.000374720199033618\n",
      "epoch: 4 step: 1720, loss is 0.15886324644088745\n",
      "epoch: 4 step: 1721, loss is 0.0023077114019542933\n",
      "epoch: 4 step: 1722, loss is 0.012944712303578854\n",
      "epoch: 4 step: 1723, loss is 0.004607422277331352\n",
      "epoch: 4 step: 1724, loss is 0.054477035999298096\n",
      "epoch: 4 step: 1725, loss is 0.017746852710843086\n",
      "epoch: 4 step: 1726, loss is 0.00013990340812597424\n",
      "epoch: 4 step: 1727, loss is 0.00016692410281393677\n",
      "epoch: 4 step: 1728, loss is 0.0009651896543800831\n",
      "epoch: 4 step: 1729, loss is 0.12591825425624847\n",
      "epoch: 4 step: 1730, loss is 0.00025303877191618085\n",
      "epoch: 4 step: 1731, loss is 0.0118936225771904\n",
      "epoch: 4 step: 1732, loss is 0.060119062662124634\n",
      "epoch: 4 step: 1733, loss is 0.0017201831797137856\n",
      "epoch: 4 step: 1734, loss is 0.016232594847679138\n",
      "epoch: 4 step: 1735, loss is 0.0008112611249089241\n",
      "epoch: 4 step: 1736, loss is 0.03300190716981888\n",
      "epoch: 4 step: 1737, loss is 0.013073854148387909\n",
      "epoch: 4 step: 1738, loss is 0.002721522469073534\n",
      "epoch: 4 step: 1739, loss is 0.196330264210701\n",
      "epoch: 4 step: 1740, loss is 0.00047340014134533703\n",
      "epoch: 4 step: 1741, loss is 0.003965695388615131\n",
      "epoch: 4 step: 1742, loss is 0.003587173530831933\n",
      "epoch: 4 step: 1743, loss is 0.016400210559368134\n",
      "epoch: 4 step: 1744, loss is 0.0018286488484591246\n",
      "epoch: 4 step: 1745, loss is 0.006887031719088554\n",
      "epoch: 4 step: 1746, loss is 0.0039458381943404675\n",
      "epoch: 4 step: 1747, loss is 0.0011055285576730967\n",
      "epoch: 4 step: 1748, loss is 0.00026063708355650306\n",
      "epoch: 4 step: 1749, loss is 0.15192952752113342\n",
      "epoch: 4 step: 1750, loss is 0.0035907463170588017\n",
      "epoch: 4 step: 1751, loss is 0.25481370091438293\n",
      "epoch: 4 step: 1752, loss is 0.24050970375537872\n",
      "epoch: 4 step: 1753, loss is 0.0019482553470879793\n",
      "epoch: 4 step: 1754, loss is 0.00760516244918108\n",
      "epoch: 4 step: 1755, loss is 0.0006308005540631711\n",
      "epoch: 4 step: 1756, loss is 0.09448471665382385\n",
      "epoch: 4 step: 1757, loss is 0.001783151994459331\n",
      "epoch: 4 step: 1758, loss is 0.11290016770362854\n",
      "epoch: 4 step: 1759, loss is 0.015718499198555946\n",
      "epoch: 4 step: 1760, loss is 0.017741728574037552\n",
      "epoch: 4 step: 1761, loss is 0.017017407342791557\n",
      "epoch: 4 step: 1762, loss is 0.023462533950805664\n",
      "epoch: 4 step: 1763, loss is 0.18104787170886993\n",
      "epoch: 4 step: 1764, loss is 0.038674890995025635\n",
      "epoch: 4 step: 1765, loss is 0.03601204976439476\n",
      "epoch: 4 step: 1766, loss is 0.049697309732437134\n",
      "epoch: 4 step: 1767, loss is 0.031339120119810104\n",
      "epoch: 4 step: 1768, loss is 0.05568219721317291\n",
      "epoch: 4 step: 1769, loss is 0.0015648961998522282\n",
      "epoch: 4 step: 1770, loss is 0.005275221541523933\n",
      "epoch: 4 step: 1771, loss is 0.000511135789565742\n",
      "epoch: 4 step: 1772, loss is 0.2142205536365509\n",
      "epoch: 4 step: 1773, loss is 0.024588361382484436\n",
      "epoch: 4 step: 1774, loss is 0.007122434675693512\n",
      "epoch: 4 step: 1775, loss is 0.0016991999000310898\n",
      "epoch: 4 step: 1776, loss is 0.005080782808363438\n",
      "epoch: 4 step: 1777, loss is 0.007909723557531834\n",
      "epoch: 4 step: 1778, loss is 0.006649942137300968\n",
      "epoch: 4 step: 1779, loss is 0.0084666283801198\n",
      "epoch: 4 step: 1780, loss is 0.0196328517049551\n",
      "epoch: 4 step: 1781, loss is 0.0030640801414847374\n",
      "epoch: 4 step: 1782, loss is 0.011890670284628868\n",
      "epoch: 4 step: 1783, loss is 0.02445150353014469\n",
      "epoch: 4 step: 1784, loss is 0.004378701094537973\n",
      "epoch: 4 step: 1785, loss is 0.0024308059364557266\n",
      "epoch: 4 step: 1786, loss is 0.005549226421862841\n",
      "epoch: 4 step: 1787, loss is 0.04945296421647072\n",
      "epoch: 4 step: 1788, loss is 0.0010916648898273706\n",
      "epoch: 4 step: 1789, loss is 0.003653193125501275\n",
      "epoch: 4 step: 1790, loss is 0.0004819448513444513\n",
      "epoch: 4 step: 1791, loss is 0.0016082272632047534\n",
      "epoch: 4 step: 1792, loss is 0.004637217149138451\n",
      "epoch: 4 step: 1793, loss is 0.0018606504891067743\n",
      "epoch: 4 step: 1794, loss is 0.29519280791282654\n",
      "epoch: 4 step: 1795, loss is 0.0791146457195282\n",
      "epoch: 4 step: 1796, loss is 0.00044618459651246667\n",
      "epoch: 4 step: 1797, loss is 0.0014619572320953012\n",
      "epoch: 4 step: 1798, loss is 0.011218663305044174\n",
      "epoch: 4 step: 1799, loss is 0.04364395514130592\n",
      "epoch: 4 step: 1800, loss is 0.056317493319511414\n",
      "epoch: 4 step: 1801, loss is 0.004666220862418413\n",
      "epoch: 4 step: 1802, loss is 0.07641782611608505\n",
      "epoch: 4 step: 1803, loss is 0.0006138795288279653\n",
      "epoch: 4 step: 1804, loss is 0.01854177750647068\n",
      "epoch: 4 step: 1805, loss is 0.006088418886065483\n",
      "epoch: 4 step: 1806, loss is 0.015312884002923965\n",
      "epoch: 4 step: 1807, loss is 0.0012170239351689816\n",
      "epoch: 4 step: 1808, loss is 0.002120391698554158\n",
      "epoch: 4 step: 1809, loss is 0.017008448019623756\n",
      "epoch: 4 step: 1810, loss is 0.002384754130616784\n",
      "epoch: 4 step: 1811, loss is 0.019065020605921745\n",
      "epoch: 4 step: 1812, loss is 0.0025876793079078197\n",
      "epoch: 4 step: 1813, loss is 0.08860745280981064\n",
      "epoch: 4 step: 1814, loss is 0.0001493148592999205\n",
      "epoch: 4 step: 1815, loss is 0.012239726260304451\n",
      "epoch: 4 step: 1816, loss is 0.007332878187298775\n",
      "epoch: 4 step: 1817, loss is 0.000423756631789729\n",
      "epoch: 4 step: 1818, loss is 0.009350433014333248\n",
      "epoch: 4 step: 1819, loss is 0.005772106349468231\n",
      "epoch: 4 step: 1820, loss is 0.029235713183879852\n",
      "epoch: 4 step: 1821, loss is 0.013235663063824177\n",
      "epoch: 4 step: 1822, loss is 0.02112025022506714\n",
      "epoch: 4 step: 1823, loss is 0.17442947626113892\n",
      "epoch: 4 step: 1824, loss is 0.04191601648926735\n",
      "epoch: 4 step: 1825, loss is 0.012275942601263523\n",
      "epoch: 4 step: 1826, loss is 0.0008625609916634858\n",
      "epoch: 4 step: 1827, loss is 0.0007479837513528764\n",
      "epoch: 4 step: 1828, loss is 0.0002990190696436912\n",
      "epoch: 4 step: 1829, loss is 0.028650235384702682\n",
      "epoch: 4 step: 1830, loss is 0.060028452426195145\n",
      "epoch: 4 step: 1831, loss is 0.0004208711034152657\n",
      "epoch: 4 step: 1832, loss is 0.0025370772927999496\n",
      "epoch: 4 step: 1833, loss is 0.0011848086724057794\n",
      "epoch: 4 step: 1834, loss is 0.001434849575161934\n",
      "epoch: 4 step: 1835, loss is 0.0014949265168979764\n",
      "epoch: 4 step: 1836, loss is 0.0002494273066986352\n",
      "epoch: 4 step: 1837, loss is 0.0012509999796748161\n",
      "epoch: 4 step: 1838, loss is 0.003921974450349808\n",
      "epoch: 4 step: 1839, loss is 0.01736331731081009\n",
      "epoch: 4 step: 1840, loss is 0.2965514063835144\n",
      "epoch: 4 step: 1841, loss is 0.009058262221515179\n",
      "epoch: 4 step: 1842, loss is 0.07505930215120316\n",
      "epoch: 4 step: 1843, loss is 0.0008446431020274758\n",
      "epoch: 4 step: 1844, loss is 0.031514450907707214\n",
      "epoch: 4 step: 1845, loss is 0.0029292944818735123\n",
      "epoch: 4 step: 1846, loss is 0.06571117043495178\n",
      "epoch: 4 step: 1847, loss is 0.01564932055771351\n",
      "epoch: 4 step: 1848, loss is 0.03690952807664871\n",
      "epoch: 4 step: 1849, loss is 0.006542152259498835\n",
      "epoch: 4 step: 1850, loss is 0.007621095050126314\n",
      "epoch: 4 step: 1851, loss is 0.0018513608956709504\n",
      "epoch: 4 step: 1852, loss is 0.08542831987142563\n",
      "epoch: 4 step: 1853, loss is 0.0007842967170290649\n",
      "epoch: 4 step: 1854, loss is 0.058392539620399475\n",
      "epoch: 4 step: 1855, loss is 0.11528477072715759\n",
      "epoch: 4 step: 1856, loss is 0.00919746421277523\n",
      "epoch: 4 step: 1857, loss is 0.0017418673960492015\n",
      "epoch: 4 step: 1858, loss is 0.003239754121750593\n",
      "epoch: 4 step: 1859, loss is 0.033913541585206985\n",
      "epoch: 4 step: 1860, loss is 0.009831706993281841\n",
      "epoch: 4 step: 1861, loss is 0.2958871126174927\n",
      "epoch: 4 step: 1862, loss is 0.01345762424170971\n",
      "epoch: 4 step: 1863, loss is 0.02690611407160759\n",
      "epoch: 4 step: 1864, loss is 0.008145974017679691\n",
      "epoch: 4 step: 1865, loss is 0.045453090220689774\n",
      "epoch: 4 step: 1866, loss is 0.07521442323923111\n",
      "epoch: 4 step: 1867, loss is 0.02308240905404091\n",
      "epoch: 4 step: 1868, loss is 0.055898360908031464\n",
      "epoch: 4 step: 1869, loss is 0.02292334847152233\n",
      "epoch: 4 step: 1870, loss is 0.06910797953605652\n",
      "epoch: 4 step: 1871, loss is 0.17944718897342682\n",
      "epoch: 4 step: 1872, loss is 0.005934407934546471\n",
      "epoch: 4 step: 1873, loss is 0.3468666672706604\n",
      "epoch: 4 step: 1874, loss is 0.007669249549508095\n",
      "epoch: 4 step: 1875, loss is 0.045454636216163635\n",
      "epoch: 5 step: 1, loss is 0.05942411720752716\n",
      "epoch: 5 step: 2, loss is 0.020123980939388275\n",
      "epoch: 5 step: 3, loss is 0.0036901189014315605\n",
      "epoch: 5 step: 4, loss is 0.00523659773170948\n",
      "epoch: 5 step: 5, loss is 0.003780617145821452\n",
      "epoch: 5 step: 6, loss is 0.002553533064201474\n",
      "epoch: 5 step: 7, loss is 0.025591209530830383\n",
      "epoch: 5 step: 8, loss is 0.00809767097234726\n",
      "epoch: 5 step: 9, loss is 0.12231572717428207\n",
      "epoch: 5 step: 10, loss is 0.0017145019955933094\n",
      "epoch: 5 step: 11, loss is 0.0002739938790909946\n",
      "epoch: 5 step: 12, loss is 0.011997788213193417\n",
      "epoch: 5 step: 13, loss is 0.013113446533679962\n",
      "epoch: 5 step: 14, loss is 0.0551283061504364\n",
      "epoch: 5 step: 15, loss is 0.09711840748786926\n",
      "epoch: 5 step: 16, loss is 0.003776035737246275\n",
      "epoch: 5 step: 17, loss is 0.0009749271557666361\n",
      "epoch: 5 step: 18, loss is 0.004326098132878542\n",
      "epoch: 5 step: 19, loss is 0.0006181131466291845\n",
      "epoch: 5 step: 20, loss is 0.17666183412075043\n",
      "epoch: 5 step: 21, loss is 0.001051975297741592\n",
      "epoch: 5 step: 22, loss is 0.003564011538401246\n",
      "epoch: 5 step: 23, loss is 0.03491796553134918\n",
      "epoch: 5 step: 24, loss is 0.0003120604087598622\n",
      "epoch: 5 step: 25, loss is 0.04221709817647934\n",
      "epoch: 5 step: 26, loss is 0.01951439678668976\n",
      "epoch: 5 step: 27, loss is 0.00595784792676568\n",
      "epoch: 5 step: 28, loss is 0.003668326186016202\n",
      "epoch: 5 step: 29, loss is 0.06189888343214989\n",
      "epoch: 5 step: 30, loss is 0.034310225397348404\n",
      "epoch: 5 step: 31, loss is 0.014527929946780205\n",
      "epoch: 5 step: 32, loss is 0.0012844586744904518\n",
      "epoch: 5 step: 33, loss is 0.0001249201741302386\n",
      "epoch: 5 step: 34, loss is 0.00291827623732388\n",
      "epoch: 5 step: 35, loss is 0.0027725701220333576\n",
      "epoch: 5 step: 36, loss is 0.07540041208267212\n",
      "epoch: 5 step: 37, loss is 0.029508788138628006\n",
      "epoch: 5 step: 38, loss is 0.0008110998314805329\n",
      "epoch: 5 step: 39, loss is 0.0015198691980913281\n",
      "epoch: 5 step: 40, loss is 0.10990060120820999\n",
      "epoch: 5 step: 41, loss is 0.24602970480918884\n",
      "epoch: 5 step: 42, loss is 0.001321649644523859\n",
      "epoch: 5 step: 43, loss is 0.04949481040239334\n",
      "epoch: 5 step: 44, loss is 0.000320099905366078\n",
      "epoch: 5 step: 45, loss is 0.10614043474197388\n",
      "epoch: 5 step: 46, loss is 0.020494870841503143\n",
      "epoch: 5 step: 47, loss is 0.22072537243366241\n",
      "epoch: 5 step: 48, loss is 0.016682758927345276\n",
      "epoch: 5 step: 49, loss is 0.009537830017507076\n",
      "epoch: 5 step: 50, loss is 0.005942667834460735\n",
      "epoch: 5 step: 51, loss is 0.020688340067863464\n",
      "epoch: 5 step: 52, loss is 0.10681772977113724\n",
      "epoch: 5 step: 53, loss is 0.0568012110888958\n",
      "epoch: 5 step: 54, loss is 0.0012926472118124366\n",
      "epoch: 5 step: 55, loss is 0.03924962505698204\n",
      "epoch: 5 step: 56, loss is 0.044251300394535065\n",
      "epoch: 5 step: 57, loss is 0.003880737815052271\n",
      "epoch: 5 step: 58, loss is 0.01217624731361866\n",
      "epoch: 5 step: 59, loss is 0.06504614651203156\n",
      "epoch: 5 step: 60, loss is 0.001454641460441053\n",
      "epoch: 5 step: 61, loss is 0.05214730277657509\n",
      "epoch: 5 step: 62, loss is 0.004494842141866684\n",
      "epoch: 5 step: 63, loss is 0.061405014246702194\n",
      "epoch: 5 step: 64, loss is 0.001571526168845594\n",
      "epoch: 5 step: 65, loss is 0.10636809468269348\n",
      "epoch: 5 step: 66, loss is 0.006777090020477772\n",
      "epoch: 5 step: 67, loss is 0.05929059162735939\n",
      "epoch: 5 step: 68, loss is 0.01444548461586237\n",
      "epoch: 5 step: 69, loss is 0.17470219731330872\n",
      "epoch: 5 step: 70, loss is 0.007347025442868471\n",
      "epoch: 5 step: 71, loss is 0.024557234719395638\n",
      "epoch: 5 step: 72, loss is 0.0009210104472003877\n",
      "epoch: 5 step: 73, loss is 0.0010330684017390013\n",
      "epoch: 5 step: 74, loss is 0.0006048739887773991\n",
      "epoch: 5 step: 75, loss is 0.0014774083392694592\n",
      "epoch: 5 step: 76, loss is 0.001482752151787281\n",
      "epoch: 5 step: 77, loss is 0.0026593939401209354\n",
      "epoch: 5 step: 78, loss is 0.007860204204916954\n",
      "epoch: 5 step: 79, loss is 0.004475143272429705\n",
      "epoch: 5 step: 80, loss is 0.10830782353878021\n",
      "epoch: 5 step: 81, loss is 0.0708165243268013\n",
      "epoch: 5 step: 82, loss is 0.005127800162881613\n",
      "epoch: 5 step: 83, loss is 0.0036898311227560043\n",
      "epoch: 5 step: 84, loss is 0.01249619759619236\n",
      "epoch: 5 step: 85, loss is 0.12770120799541473\n",
      "epoch: 5 step: 86, loss is 0.011298375204205513\n",
      "epoch: 5 step: 87, loss is 0.0014976365491747856\n",
      "epoch: 5 step: 88, loss is 0.0026575943920761347\n",
      "epoch: 5 step: 89, loss is 0.03391536325216293\n",
      "epoch: 5 step: 90, loss is 0.00057493697386235\n",
      "epoch: 5 step: 91, loss is 0.013734693638980389\n",
      "epoch: 5 step: 92, loss is 0.04069371521472931\n",
      "epoch: 5 step: 93, loss is 0.09423432499170303\n",
      "epoch: 5 step: 94, loss is 0.13949404656887054\n",
      "epoch: 5 step: 95, loss is 0.026288533583283424\n",
      "epoch: 5 step: 96, loss is 0.005616005975753069\n",
      "epoch: 5 step: 97, loss is 0.0024181732442229986\n",
      "epoch: 5 step: 98, loss is 0.0027072385419160128\n",
      "epoch: 5 step: 99, loss is 0.0008276982698589563\n",
      "epoch: 5 step: 100, loss is 0.002247181721031666\n",
      "epoch: 5 step: 101, loss is 0.0006086964276619256\n",
      "epoch: 5 step: 102, loss is 0.001985318725928664\n",
      "epoch: 5 step: 103, loss is 0.018973007798194885\n",
      "epoch: 5 step: 104, loss is 0.004013456869870424\n",
      "epoch: 5 step: 105, loss is 0.0035912026651203632\n",
      "epoch: 5 step: 106, loss is 0.005601893179118633\n",
      "epoch: 5 step: 107, loss is 0.00039139072760008276\n",
      "epoch: 5 step: 108, loss is 0.008283630013465881\n",
      "epoch: 5 step: 109, loss is 0.003082140814512968\n",
      "epoch: 5 step: 110, loss is 0.014897498302161694\n",
      "epoch: 5 step: 111, loss is 0.02656588889658451\n",
      "epoch: 5 step: 112, loss is 0.0006277330685406923\n",
      "epoch: 5 step: 113, loss is 0.0011862764367833734\n",
      "epoch: 5 step: 114, loss is 0.0008320973720401525\n",
      "epoch: 5 step: 115, loss is 0.0036403273697942495\n",
      "epoch: 5 step: 116, loss is 0.04589591175317764\n",
      "epoch: 5 step: 117, loss is 0.001913942862302065\n",
      "epoch: 5 step: 118, loss is 0.0344461165368557\n",
      "epoch: 5 step: 119, loss is 0.10710573941469193\n",
      "epoch: 5 step: 120, loss is 0.011131570674479008\n",
      "epoch: 5 step: 121, loss is 0.015760526061058044\n",
      "epoch: 5 step: 122, loss is 0.004394453018903732\n",
      "epoch: 5 step: 123, loss is 0.0025061334017664194\n",
      "epoch: 5 step: 124, loss is 0.0011756987078115344\n",
      "epoch: 5 step: 125, loss is 0.0003218157507944852\n",
      "epoch: 5 step: 126, loss is 0.0008213387918658555\n",
      "epoch: 5 step: 127, loss is 0.1496971845626831\n",
      "epoch: 5 step: 128, loss is 0.00022666183940600604\n",
      "epoch: 5 step: 129, loss is 0.16850054264068604\n",
      "epoch: 5 step: 130, loss is 0.06572354584932327\n",
      "epoch: 5 step: 131, loss is 0.10467663407325745\n",
      "epoch: 5 step: 132, loss is 0.0008425322594121099\n",
      "epoch: 5 step: 133, loss is 0.056931886821985245\n",
      "epoch: 5 step: 134, loss is 0.019345784559845924\n",
      "epoch: 5 step: 135, loss is 0.0053364187479019165\n",
      "epoch: 5 step: 136, loss is 0.06360266357660294\n",
      "epoch: 5 step: 137, loss is 0.07448307424783707\n",
      "epoch: 5 step: 138, loss is 0.002226088661700487\n",
      "epoch: 5 step: 139, loss is 0.0030277245678007603\n",
      "epoch: 5 step: 140, loss is 0.0031181364320218563\n",
      "epoch: 5 step: 141, loss is 0.003095892956480384\n",
      "epoch: 5 step: 142, loss is 0.013758198358118534\n",
      "epoch: 5 step: 143, loss is 0.03519175574183464\n",
      "epoch: 5 step: 144, loss is 0.13190396130084991\n",
      "epoch: 5 step: 145, loss is 0.03719218075275421\n",
      "epoch: 5 step: 146, loss is 0.0014975126832723618\n",
      "epoch: 5 step: 147, loss is 0.006994142197072506\n",
      "epoch: 5 step: 148, loss is 0.0030834227800369263\n",
      "epoch: 5 step: 149, loss is 0.0044376179575920105\n",
      "epoch: 5 step: 150, loss is 0.02318471297621727\n",
      "epoch: 5 step: 151, loss is 0.05913056805729866\n",
      "epoch: 5 step: 152, loss is 0.0034127887338399887\n",
      "epoch: 5 step: 153, loss is 0.0004605260328389704\n",
      "epoch: 5 step: 154, loss is 0.036548882722854614\n",
      "epoch: 5 step: 155, loss is 0.013317580334842205\n",
      "epoch: 5 step: 156, loss is 0.05891154333949089\n",
      "epoch: 5 step: 157, loss is 0.10136547684669495\n",
      "epoch: 5 step: 158, loss is 0.001201486331410706\n",
      "epoch: 5 step: 159, loss is 0.018859192728996277\n",
      "epoch: 5 step: 160, loss is 0.005316304974257946\n",
      "epoch: 5 step: 161, loss is 0.000889313465449959\n",
      "epoch: 5 step: 162, loss is 0.0003001266159117222\n",
      "epoch: 5 step: 163, loss is 0.005175037309527397\n",
      "epoch: 5 step: 164, loss is 0.27662715315818787\n",
      "epoch: 5 step: 165, loss is 0.0003959623572882265\n",
      "epoch: 5 step: 166, loss is 0.0062299491837620735\n",
      "epoch: 5 step: 167, loss is 0.024018991738557816\n",
      "epoch: 5 step: 168, loss is 0.0030994454864412546\n",
      "epoch: 5 step: 169, loss is 0.004451905842870474\n",
      "epoch: 5 step: 170, loss is 0.0016605507116764784\n",
      "epoch: 5 step: 171, loss is 0.01590418815612793\n",
      "epoch: 5 step: 172, loss is 0.027977479621767998\n",
      "epoch: 5 step: 173, loss is 0.10541636496782303\n",
      "epoch: 5 step: 174, loss is 0.004026311449706554\n",
      "epoch: 5 step: 175, loss is 0.014204300940036774\n",
      "epoch: 5 step: 176, loss is 0.00029374161385931075\n",
      "epoch: 5 step: 177, loss is 0.0009749144082888961\n",
      "epoch: 5 step: 178, loss is 0.035728588700294495\n",
      "epoch: 5 step: 179, loss is 0.006716335192322731\n",
      "epoch: 5 step: 180, loss is 0.003517519449815154\n",
      "epoch: 5 step: 181, loss is 0.02887970581650734\n",
      "epoch: 5 step: 182, loss is 0.0021454410161823034\n",
      "epoch: 5 step: 183, loss is 0.016855841502547264\n",
      "epoch: 5 step: 184, loss is 0.05222697928547859\n",
      "epoch: 5 step: 185, loss is 0.0007131905294954777\n",
      "epoch: 5 step: 186, loss is 0.001591097447089851\n",
      "epoch: 5 step: 187, loss is 0.0012684734538197517\n",
      "epoch: 5 step: 188, loss is 0.003448957810178399\n",
      "epoch: 5 step: 189, loss is 0.019887708127498627\n",
      "epoch: 5 step: 190, loss is 0.005393059924244881\n",
      "epoch: 5 step: 191, loss is 0.001502534607425332\n",
      "epoch: 5 step: 192, loss is 0.00035629808553494513\n",
      "epoch: 5 step: 193, loss is 0.014922980219125748\n",
      "epoch: 5 step: 194, loss is 0.0014130587223917246\n",
      "epoch: 5 step: 195, loss is 0.00689005758613348\n",
      "epoch: 5 step: 196, loss is 0.012853479944169521\n",
      "epoch: 5 step: 197, loss is 0.01294326689094305\n",
      "epoch: 5 step: 198, loss is 0.00986216776072979\n",
      "epoch: 5 step: 199, loss is 0.0007944860262796283\n",
      "epoch: 5 step: 200, loss is 0.002624486107379198\n",
      "epoch: 5 step: 201, loss is 0.009084823541343212\n",
      "epoch: 5 step: 202, loss is 0.00862203724682331\n",
      "epoch: 5 step: 203, loss is 0.003555245930328965\n",
      "epoch: 5 step: 204, loss is 0.011708528734743595\n",
      "epoch: 5 step: 205, loss is 0.003650263650342822\n",
      "epoch: 5 step: 206, loss is 0.0019611683674156666\n",
      "epoch: 5 step: 207, loss is 0.0019406586652621627\n",
      "epoch: 5 step: 208, loss is 0.00598755432292819\n",
      "epoch: 5 step: 209, loss is 0.009492534212768078\n",
      "epoch: 5 step: 210, loss is 0.0006289980956353247\n",
      "epoch: 5 step: 211, loss is 0.005526137072592974\n",
      "epoch: 5 step: 212, loss is 0.00572202866896987\n",
      "epoch: 5 step: 213, loss is 0.0012543133925646544\n",
      "epoch: 5 step: 214, loss is 0.005821906961500645\n",
      "epoch: 5 step: 215, loss is 0.001636151922866702\n",
      "epoch: 5 step: 216, loss is 0.0012002362636849284\n",
      "epoch: 5 step: 217, loss is 0.0016375206178054214\n",
      "epoch: 5 step: 218, loss is 0.0005830447771586478\n",
      "epoch: 5 step: 219, loss is 0.0010329653741791844\n",
      "epoch: 5 step: 220, loss is 0.16521061956882477\n",
      "epoch: 5 step: 221, loss is 0.0029602854046970606\n",
      "epoch: 5 step: 222, loss is 0.00022635917412117124\n",
      "epoch: 5 step: 223, loss is 0.0006854000384919345\n",
      "epoch: 5 step: 224, loss is 0.005731847137212753\n",
      "epoch: 5 step: 225, loss is 0.02130439504981041\n",
      "epoch: 5 step: 226, loss is 0.0017557002138346434\n",
      "epoch: 5 step: 227, loss is 0.07646095007658005\n",
      "epoch: 5 step: 228, loss is 0.009481365792453289\n",
      "epoch: 5 step: 229, loss is 0.0014914863277226686\n",
      "epoch: 5 step: 230, loss is 0.04366452619433403\n",
      "epoch: 5 step: 231, loss is 0.0003276289498899132\n",
      "epoch: 5 step: 232, loss is 0.009732968173921108\n",
      "epoch: 5 step: 233, loss is 0.005559900309890509\n",
      "epoch: 5 step: 234, loss is 3.561637640814297e-05\n",
      "epoch: 5 step: 235, loss is 0.020534558221697807\n",
      "epoch: 5 step: 236, loss is 0.0003087867225985974\n",
      "epoch: 5 step: 237, loss is 0.08215656876564026\n",
      "epoch: 5 step: 238, loss is 0.0005208761431276798\n",
      "epoch: 5 step: 239, loss is 0.0007967756246216595\n",
      "epoch: 5 step: 240, loss is 0.003858520882204175\n",
      "epoch: 5 step: 241, loss is 0.00019590288866311312\n",
      "epoch: 5 step: 242, loss is 0.000869491312187165\n",
      "epoch: 5 step: 243, loss is 0.00047156159416772425\n",
      "epoch: 5 step: 244, loss is 8.670926763443276e-05\n",
      "epoch: 5 step: 245, loss is 0.0010126408888027072\n",
      "epoch: 5 step: 246, loss is 0.005600204691290855\n",
      "epoch: 5 step: 247, loss is 0.02058076672255993\n",
      "epoch: 5 step: 248, loss is 0.05711827054619789\n",
      "epoch: 5 step: 249, loss is 0.00020768219837918878\n",
      "epoch: 5 step: 250, loss is 0.006362439598888159\n",
      "epoch: 5 step: 251, loss is 0.0075577800162136555\n",
      "epoch: 5 step: 252, loss is 0.0007230736082419753\n",
      "epoch: 5 step: 253, loss is 0.004375986289232969\n",
      "epoch: 5 step: 254, loss is 0.0017868001013994217\n",
      "epoch: 5 step: 255, loss is 0.0004403822822496295\n",
      "epoch: 5 step: 256, loss is 0.06172315776348114\n",
      "epoch: 5 step: 257, loss is 0.0013183034025132656\n",
      "epoch: 5 step: 258, loss is 0.004781157243996859\n",
      "epoch: 5 step: 259, loss is 0.005096370354294777\n",
      "epoch: 5 step: 260, loss is 0.0011063949204981327\n",
      "epoch: 5 step: 261, loss is 0.0008542538271285594\n",
      "epoch: 5 step: 262, loss is 0.012203546240925789\n",
      "epoch: 5 step: 263, loss is 0.006313308607786894\n",
      "epoch: 5 step: 264, loss is 0.1643081158399582\n",
      "epoch: 5 step: 265, loss is 0.007473287172615528\n",
      "epoch: 5 step: 266, loss is 0.031685251742601395\n",
      "epoch: 5 step: 267, loss is 0.018010089173913002\n",
      "epoch: 5 step: 268, loss is 0.014073890633881092\n",
      "epoch: 5 step: 269, loss is 0.09585457295179367\n",
      "epoch: 5 step: 270, loss is 0.0025655298959463835\n",
      "epoch: 5 step: 271, loss is 0.0003360880946274847\n",
      "epoch: 5 step: 272, loss is 0.019010456278920174\n",
      "epoch: 5 step: 273, loss is 0.012657593004405499\n",
      "epoch: 5 step: 274, loss is 0.006271563936024904\n",
      "epoch: 5 step: 275, loss is 0.009198741056025028\n",
      "epoch: 5 step: 276, loss is 0.002694820985198021\n",
      "epoch: 5 step: 277, loss is 0.02882419154047966\n",
      "epoch: 5 step: 278, loss is 0.03856148570775986\n",
      "epoch: 5 step: 279, loss is 0.021493136882781982\n",
      "epoch: 5 step: 280, loss is 0.008490419015288353\n",
      "epoch: 5 step: 281, loss is 0.003739047097042203\n",
      "epoch: 5 step: 282, loss is 0.03243987262248993\n",
      "epoch: 5 step: 283, loss is 0.02839387021958828\n",
      "epoch: 5 step: 284, loss is 0.08694223314523697\n",
      "epoch: 5 step: 285, loss is 0.02522624097764492\n",
      "epoch: 5 step: 286, loss is 0.01363581046462059\n",
      "epoch: 5 step: 287, loss is 0.0005643933545798063\n",
      "epoch: 5 step: 288, loss is 0.0002653414267115295\n",
      "epoch: 5 step: 289, loss is 0.0028932238928973675\n",
      "epoch: 5 step: 290, loss is 0.00016451426199637353\n",
      "epoch: 5 step: 291, loss is 0.059057675302028656\n",
      "epoch: 5 step: 292, loss is 0.042991023510694504\n",
      "epoch: 5 step: 293, loss is 0.009359759278595448\n",
      "epoch: 5 step: 294, loss is 0.009935108944773674\n",
      "epoch: 5 step: 295, loss is 0.22573815286159515\n",
      "epoch: 5 step: 296, loss is 0.0014743716455996037\n",
      "epoch: 5 step: 297, loss is 0.041207749396562576\n",
      "epoch: 5 step: 298, loss is 0.0030986566562205553\n",
      "epoch: 5 step: 299, loss is 0.0017995075322687626\n",
      "epoch: 5 step: 300, loss is 0.002277188701555133\n",
      "epoch: 5 step: 301, loss is 0.0007584238774143159\n",
      "epoch: 5 step: 302, loss is 0.0008596334373578429\n",
      "epoch: 5 step: 303, loss is 0.03541449084877968\n",
      "epoch: 5 step: 304, loss is 0.0017851416487246752\n",
      "epoch: 5 step: 305, loss is 0.0029431860893964767\n",
      "epoch: 5 step: 306, loss is 0.0003021563170477748\n",
      "epoch: 5 step: 307, loss is 0.006444268859922886\n",
      "epoch: 5 step: 308, loss is 0.23308958113193512\n",
      "epoch: 5 step: 309, loss is 0.15209825336933136\n",
      "epoch: 5 step: 310, loss is 0.10181461274623871\n",
      "epoch: 5 step: 311, loss is 0.007936293259263039\n",
      "epoch: 5 step: 312, loss is 0.0017205720068886876\n",
      "epoch: 5 step: 313, loss is 0.04814765974879265\n",
      "epoch: 5 step: 314, loss is 0.024895796552300453\n",
      "epoch: 5 step: 315, loss is 0.009094818495213985\n",
      "epoch: 5 step: 316, loss is 0.0026558334939181805\n",
      "epoch: 5 step: 317, loss is 0.005304333753883839\n",
      "epoch: 5 step: 318, loss is 0.0011222675675526261\n",
      "epoch: 5 step: 319, loss is 0.002029082737863064\n",
      "epoch: 5 step: 320, loss is 0.06768882274627686\n",
      "epoch: 5 step: 321, loss is 0.0030345767736434937\n",
      "epoch: 5 step: 322, loss is 0.0022990559227764606\n",
      "epoch: 5 step: 323, loss is 0.0018464643508195877\n",
      "epoch: 5 step: 324, loss is 0.0012711883755400777\n",
      "epoch: 5 step: 325, loss is 0.016828158870339394\n",
      "epoch: 5 step: 326, loss is 0.039501309394836426\n",
      "epoch: 5 step: 327, loss is 0.0010185721330344677\n",
      "epoch: 5 step: 328, loss is 0.045283179730176926\n",
      "epoch: 5 step: 329, loss is 0.0016739508137106895\n",
      "epoch: 5 step: 330, loss is 0.04168742150068283\n",
      "epoch: 5 step: 331, loss is 0.05376880243420601\n",
      "epoch: 5 step: 332, loss is 0.001326675759628415\n",
      "epoch: 5 step: 333, loss is 0.0033077995758503675\n",
      "epoch: 5 step: 334, loss is 0.004789725411683321\n",
      "epoch: 5 step: 335, loss is 0.0014277072623372078\n",
      "epoch: 5 step: 336, loss is 0.0054903519339859486\n",
      "epoch: 5 step: 337, loss is 0.001854497822932899\n",
      "epoch: 5 step: 338, loss is 0.0007764704059809446\n",
      "epoch: 5 step: 339, loss is 0.002569808391854167\n",
      "epoch: 5 step: 340, loss is 0.0021758642978966236\n",
      "epoch: 5 step: 341, loss is 0.0006368847098201513\n",
      "epoch: 5 step: 342, loss is 0.18225902318954468\n",
      "epoch: 5 step: 343, loss is 0.0032663699239492416\n",
      "epoch: 5 step: 344, loss is 0.034450870007276535\n",
      "epoch: 5 step: 345, loss is 0.0018252574373036623\n",
      "epoch: 5 step: 346, loss is 0.0017774506704881787\n",
      "epoch: 5 step: 347, loss is 0.008925984613597393\n",
      "epoch: 5 step: 348, loss is 0.0016401383327320218\n",
      "epoch: 5 step: 349, loss is 0.06050518900156021\n",
      "epoch: 5 step: 350, loss is 0.033983420580625534\n",
      "epoch: 5 step: 351, loss is 0.02739621326327324\n",
      "epoch: 5 step: 352, loss is 0.0633431002497673\n",
      "epoch: 5 step: 353, loss is 0.01903895102441311\n",
      "epoch: 5 step: 354, loss is 0.0005835833144374192\n",
      "epoch: 5 step: 355, loss is 0.015493550337851048\n",
      "epoch: 5 step: 356, loss is 0.0003129713877569884\n",
      "epoch: 5 step: 357, loss is 0.004381377249956131\n",
      "epoch: 5 step: 358, loss is 0.006848068442195654\n",
      "epoch: 5 step: 359, loss is 0.06800267845392227\n",
      "epoch: 5 step: 360, loss is 0.0003699658263940364\n",
      "epoch: 5 step: 361, loss is 0.004702161066234112\n",
      "epoch: 5 step: 362, loss is 0.0005006901919841766\n",
      "epoch: 5 step: 363, loss is 0.013987217098474503\n",
      "epoch: 5 step: 364, loss is 0.0009933755500242114\n",
      "epoch: 5 step: 365, loss is 0.007109389640390873\n",
      "epoch: 5 step: 366, loss is 0.0009072211105376482\n",
      "epoch: 5 step: 367, loss is 0.007098293863236904\n",
      "epoch: 5 step: 368, loss is 0.007317663636058569\n",
      "epoch: 5 step: 369, loss is 0.00022826201166026294\n",
      "epoch: 5 step: 370, loss is 0.0033529275096952915\n",
      "epoch: 5 step: 371, loss is 0.03670164570212364\n",
      "epoch: 5 step: 372, loss is 0.0019996087066829205\n",
      "epoch: 5 step: 373, loss is 0.0023629856295883656\n",
      "epoch: 5 step: 374, loss is 0.08038502931594849\n",
      "epoch: 5 step: 375, loss is 0.0017830021679401398\n",
      "epoch: 5 step: 376, loss is 0.04185932129621506\n",
      "epoch: 5 step: 377, loss is 0.29941850900650024\n",
      "epoch: 5 step: 378, loss is 0.036934494972229004\n",
      "epoch: 5 step: 379, loss is 0.021763622760772705\n",
      "epoch: 5 step: 380, loss is 0.010096157900989056\n",
      "epoch: 5 step: 381, loss is 0.014082638546824455\n",
      "epoch: 5 step: 382, loss is 0.00021221544011496007\n",
      "epoch: 5 step: 383, loss is 0.02398146130144596\n",
      "epoch: 5 step: 384, loss is 0.041408028453588486\n",
      "epoch: 5 step: 385, loss is 0.1563018560409546\n",
      "epoch: 5 step: 386, loss is 0.004598966334015131\n",
      "epoch: 5 step: 387, loss is 0.07460550218820572\n",
      "epoch: 5 step: 388, loss is 0.00083269237075001\n",
      "epoch: 5 step: 389, loss is 0.05572235584259033\n",
      "epoch: 5 step: 390, loss is 5.589896682067774e-05\n",
      "epoch: 5 step: 391, loss is 0.0027204854413866997\n",
      "epoch: 5 step: 392, loss is 0.006012749392539263\n",
      "epoch: 5 step: 393, loss is 0.0018755747005343437\n",
      "epoch: 5 step: 394, loss is 0.015750307589769363\n",
      "epoch: 5 step: 395, loss is 0.08900810033082962\n",
      "epoch: 5 step: 396, loss is 0.11637956649065018\n",
      "epoch: 5 step: 397, loss is 0.05756295472383499\n",
      "epoch: 5 step: 398, loss is 0.003652918851003051\n",
      "epoch: 5 step: 399, loss is 0.0025358758866786957\n",
      "epoch: 5 step: 400, loss is 0.020110566169023514\n",
      "epoch: 5 step: 401, loss is 0.3736001253128052\n",
      "epoch: 5 step: 402, loss is 0.000978602096438408\n",
      "epoch: 5 step: 403, loss is 0.0072564114816486835\n",
      "epoch: 5 step: 404, loss is 0.005945608019828796\n",
      "epoch: 5 step: 405, loss is 0.16446928679943085\n",
      "epoch: 5 step: 406, loss is 0.041716933250427246\n",
      "epoch: 5 step: 407, loss is 0.018702009692788124\n",
      "epoch: 5 step: 408, loss is 0.0011615832336246967\n",
      "epoch: 5 step: 409, loss is 0.0007593176560476422\n",
      "epoch: 5 step: 410, loss is 0.010240906849503517\n",
      "epoch: 5 step: 411, loss is 0.0020968401804566383\n",
      "epoch: 5 step: 412, loss is 0.011177949607372284\n",
      "epoch: 5 step: 413, loss is 0.05044557526707649\n",
      "epoch: 5 step: 414, loss is 0.007356896530836821\n",
      "epoch: 5 step: 415, loss is 0.005376437678933144\n",
      "epoch: 5 step: 416, loss is 0.0026204450987279415\n",
      "epoch: 5 step: 417, loss is 0.016155589371919632\n",
      "epoch: 5 step: 418, loss is 0.03508083522319794\n",
      "epoch: 5 step: 419, loss is 0.014471214264631271\n",
      "epoch: 5 step: 420, loss is 0.06191185489296913\n",
      "epoch: 5 step: 421, loss is 0.3529875576496124\n",
      "epoch: 5 step: 422, loss is 0.00967318844050169\n",
      "epoch: 5 step: 423, loss is 0.03314937651157379\n",
      "epoch: 5 step: 424, loss is 0.003040730021893978\n",
      "epoch: 5 step: 425, loss is 0.0005204091430641711\n",
      "epoch: 5 step: 426, loss is 0.011227607727050781\n",
      "epoch: 5 step: 427, loss is 0.0008680481114424765\n",
      "epoch: 5 step: 428, loss is 0.004080701153725386\n",
      "epoch: 5 step: 429, loss is 0.000522912829183042\n",
      "epoch: 5 step: 430, loss is 0.01468425802886486\n",
      "epoch: 5 step: 431, loss is 0.019882621243596077\n",
      "epoch: 5 step: 432, loss is 0.003547980450093746\n",
      "epoch: 5 step: 433, loss is 0.00787636823952198\n",
      "epoch: 5 step: 434, loss is 0.17486104369163513\n",
      "epoch: 5 step: 435, loss is 0.0015727834543213248\n",
      "epoch: 5 step: 436, loss is 0.0025837549474090338\n",
      "epoch: 5 step: 437, loss is 0.0017751699779182673\n",
      "epoch: 5 step: 438, loss is 0.01307370513677597\n",
      "epoch: 5 step: 439, loss is 0.05549520626664162\n",
      "epoch: 5 step: 440, loss is 0.027355268597602844\n",
      "epoch: 5 step: 441, loss is 0.001073218765668571\n",
      "epoch: 5 step: 442, loss is 0.0015348353190347552\n",
      "epoch: 5 step: 443, loss is 0.04665018245577812\n",
      "epoch: 5 step: 444, loss is 0.014894284307956696\n",
      "epoch: 5 step: 445, loss is 0.0008402985986322165\n",
      "epoch: 5 step: 446, loss is 0.027555350214242935\n",
      "epoch: 5 step: 447, loss is 0.0009051481611095369\n",
      "epoch: 5 step: 448, loss is 0.0010209721513092518\n",
      "epoch: 5 step: 449, loss is 0.0026824292726814747\n",
      "epoch: 5 step: 450, loss is 0.02579306624829769\n",
      "epoch: 5 step: 451, loss is 0.0012057024287059903\n",
      "epoch: 5 step: 452, loss is 0.0011719585163518786\n",
      "epoch: 5 step: 453, loss is 0.0014854661421850324\n",
      "epoch: 5 step: 454, loss is 0.0017019896768033504\n",
      "epoch: 5 step: 455, loss is 0.0037392256781458855\n",
      "epoch: 5 step: 456, loss is 0.0013586730929091573\n",
      "epoch: 5 step: 457, loss is 0.0006603498477488756\n",
      "epoch: 5 step: 458, loss is 0.003928627353161573\n",
      "epoch: 5 step: 459, loss is 0.016226395964622498\n",
      "epoch: 5 step: 460, loss is 0.021874714642763138\n",
      "epoch: 5 step: 461, loss is 0.016765965148806572\n",
      "epoch: 5 step: 462, loss is 0.0009442888549529016\n",
      "epoch: 5 step: 463, loss is 0.020633026957511902\n",
      "epoch: 5 step: 464, loss is 0.002272364217787981\n",
      "epoch: 5 step: 465, loss is 0.0057499101385474205\n",
      "epoch: 5 step: 466, loss is 0.00037363028968684375\n",
      "epoch: 5 step: 467, loss is 9.486944327363744e-05\n",
      "epoch: 5 step: 468, loss is 0.0677071064710617\n",
      "epoch: 5 step: 469, loss is 0.048722535371780396\n",
      "epoch: 5 step: 470, loss is 0.005683280993252993\n",
      "epoch: 5 step: 471, loss is 0.00023542993585579097\n",
      "epoch: 5 step: 472, loss is 0.06661194562911987\n",
      "epoch: 5 step: 473, loss is 0.016060449182987213\n",
      "epoch: 5 step: 474, loss is 0.04296344146132469\n",
      "epoch: 5 step: 475, loss is 0.001157547696493566\n",
      "epoch: 5 step: 476, loss is 0.007377054076641798\n",
      "epoch: 5 step: 477, loss is 0.012042115442454815\n",
      "epoch: 5 step: 478, loss is 0.005040955729782581\n",
      "epoch: 5 step: 479, loss is 0.11546256393194199\n",
      "epoch: 5 step: 480, loss is 0.013136922381818295\n",
      "epoch: 5 step: 481, loss is 0.040978558361530304\n",
      "epoch: 5 step: 482, loss is 0.07351289689540863\n",
      "epoch: 5 step: 483, loss is 0.260911226272583\n",
      "epoch: 5 step: 484, loss is 0.00018925659242086112\n",
      "epoch: 5 step: 485, loss is 0.0006305712158791721\n",
      "epoch: 5 step: 486, loss is 0.00022897234885022044\n",
      "epoch: 5 step: 487, loss is 0.003526168642565608\n",
      "epoch: 5 step: 488, loss is 9.172534919343889e-05\n",
      "epoch: 5 step: 489, loss is 0.0027641926426440477\n",
      "epoch: 5 step: 490, loss is 0.0011259166058152914\n",
      "epoch: 5 step: 491, loss is 0.01739136502146721\n",
      "epoch: 5 step: 492, loss is 0.009911811910569668\n",
      "epoch: 5 step: 493, loss is 0.003195498837158084\n",
      "epoch: 5 step: 494, loss is 0.0006173342699185014\n",
      "epoch: 5 step: 495, loss is 0.02572016790509224\n",
      "epoch: 5 step: 496, loss is 0.022007882595062256\n",
      "epoch: 5 step: 497, loss is 0.0016580477822571993\n",
      "epoch: 5 step: 498, loss is 0.0065967971459031105\n",
      "epoch: 5 step: 499, loss is 0.0020234023686498404\n",
      "epoch: 5 step: 500, loss is 0.0006701627862639725\n",
      "epoch: 5 step: 501, loss is 0.0007363088661804795\n",
      "epoch: 5 step: 502, loss is 0.06636776030063629\n",
      "epoch: 5 step: 503, loss is 0.0037078482564538717\n",
      "epoch: 5 step: 504, loss is 0.08524829894304276\n",
      "epoch: 5 step: 505, loss is 0.02233443409204483\n",
      "epoch: 5 step: 506, loss is 0.14003469049930573\n",
      "epoch: 5 step: 507, loss is 0.0008432434988208115\n",
      "epoch: 5 step: 508, loss is 0.00023540118127129972\n",
      "epoch: 5 step: 509, loss is 0.009339244104921818\n",
      "epoch: 5 step: 510, loss is 0.0009827445028349757\n",
      "epoch: 5 step: 511, loss is 0.0013644518330693245\n",
      "epoch: 5 step: 512, loss is 0.0010737462434917688\n",
      "epoch: 5 step: 513, loss is 0.03388141095638275\n",
      "epoch: 5 step: 514, loss is 0.0021529574878513813\n",
      "epoch: 5 step: 515, loss is 0.008262062445282936\n",
      "epoch: 5 step: 516, loss is 0.020315950736403465\n",
      "epoch: 5 step: 517, loss is 0.0031780393328517675\n",
      "epoch: 5 step: 518, loss is 0.000991716398857534\n",
      "epoch: 5 step: 519, loss is 0.06760919094085693\n",
      "epoch: 5 step: 520, loss is 0.002131618792191148\n",
      "epoch: 5 step: 521, loss is 0.05017654597759247\n",
      "epoch: 5 step: 522, loss is 0.007684554439038038\n",
      "epoch: 5 step: 523, loss is 0.002143810736015439\n",
      "epoch: 5 step: 524, loss is 0.002701664110645652\n",
      "epoch: 5 step: 525, loss is 0.006831103935837746\n",
      "epoch: 5 step: 526, loss is 0.0002126802282873541\n",
      "epoch: 5 step: 527, loss is 5.353613232728094e-05\n",
      "epoch: 5 step: 528, loss is 0.008816416375339031\n",
      "epoch: 5 step: 529, loss is 0.07866764068603516\n",
      "epoch: 5 step: 530, loss is 0.00046428744099102914\n",
      "epoch: 5 step: 531, loss is 0.023561853915452957\n",
      "epoch: 5 step: 532, loss is 0.002649538451805711\n",
      "epoch: 5 step: 533, loss is 0.0010662126587703824\n",
      "epoch: 5 step: 534, loss is 0.011794826947152615\n",
      "epoch: 5 step: 535, loss is 0.00038336493889801204\n",
      "epoch: 5 step: 536, loss is 0.06675783544778824\n",
      "epoch: 5 step: 537, loss is 0.0010872241109609604\n",
      "epoch: 5 step: 538, loss is 0.002627602079883218\n",
      "epoch: 5 step: 539, loss is 0.002829601988196373\n",
      "epoch: 5 step: 540, loss is 0.006597209721803665\n",
      "epoch: 5 step: 541, loss is 0.015736544504761696\n",
      "epoch: 5 step: 542, loss is 0.0009591035777702928\n",
      "epoch: 5 step: 543, loss is 0.01134732086211443\n",
      "epoch: 5 step: 544, loss is 0.0022081811912357807\n",
      "epoch: 5 step: 545, loss is 0.002663566265255213\n",
      "epoch: 5 step: 546, loss is 0.031430233269929886\n",
      "epoch: 5 step: 547, loss is 0.011710545048117638\n",
      "epoch: 5 step: 548, loss is 0.000983197707682848\n",
      "epoch: 5 step: 549, loss is 0.0020573160145431757\n",
      "epoch: 5 step: 550, loss is 0.011020266450941563\n",
      "epoch: 5 step: 551, loss is 0.0013571522431448102\n",
      "epoch: 5 step: 552, loss is 0.00046933701378293335\n",
      "epoch: 5 step: 553, loss is 0.14191246032714844\n",
      "epoch: 5 step: 554, loss is 0.03594706580042839\n",
      "epoch: 5 step: 555, loss is 0.001048336736857891\n",
      "epoch: 5 step: 556, loss is 0.0095903892070055\n",
      "epoch: 5 step: 557, loss is 0.0017705076606944203\n",
      "epoch: 5 step: 558, loss is 0.04439809173345566\n",
      "epoch: 5 step: 559, loss is 0.003280216595157981\n",
      "epoch: 5 step: 560, loss is 0.003886867081746459\n",
      "epoch: 5 step: 561, loss is 0.0008895251085050404\n",
      "epoch: 5 step: 562, loss is 0.1111162006855011\n",
      "epoch: 5 step: 563, loss is 0.009311232715845108\n",
      "epoch: 5 step: 564, loss is 0.005508426111191511\n",
      "epoch: 5 step: 565, loss is 0.0002641014871187508\n",
      "epoch: 5 step: 566, loss is 0.0008858192013576627\n",
      "epoch: 5 step: 567, loss is 0.0002680281177163124\n",
      "epoch: 5 step: 568, loss is 0.0314960703253746\n",
      "epoch: 5 step: 569, loss is 0.006680030841380358\n",
      "epoch: 5 step: 570, loss is 0.021176688373088837\n",
      "epoch: 5 step: 571, loss is 0.0002993457019329071\n",
      "epoch: 5 step: 572, loss is 0.0009958654409274459\n",
      "epoch: 5 step: 573, loss is 0.17012569308280945\n",
      "epoch: 5 step: 574, loss is 0.0067397491075098515\n",
      "epoch: 5 step: 575, loss is 0.005980420857667923\n",
      "epoch: 5 step: 576, loss is 0.018918655812740326\n",
      "epoch: 5 step: 577, loss is 0.009296380914747715\n",
      "epoch: 5 step: 578, loss is 0.006068069953471422\n",
      "epoch: 5 step: 579, loss is 0.03918010741472244\n",
      "epoch: 5 step: 580, loss is 0.007323432248085737\n",
      "epoch: 5 step: 581, loss is 0.1935678869485855\n",
      "epoch: 5 step: 582, loss is 0.013081327080726624\n",
      "epoch: 5 step: 583, loss is 0.0059033893048763275\n",
      "epoch: 5 step: 584, loss is 0.00997356977313757\n",
      "epoch: 5 step: 585, loss is 0.00039675194420851767\n",
      "epoch: 5 step: 586, loss is 0.05121607705950737\n",
      "epoch: 5 step: 587, loss is 0.0025296553503721952\n",
      "epoch: 5 step: 588, loss is 0.042281847447156906\n",
      "epoch: 5 step: 589, loss is 0.003725900314748287\n",
      "epoch: 5 step: 590, loss is 0.01164817065000534\n",
      "epoch: 5 step: 591, loss is 0.0006771511980332434\n",
      "epoch: 5 step: 592, loss is 0.013707934878766537\n",
      "epoch: 5 step: 593, loss is 0.005414152052253485\n",
      "epoch: 5 step: 594, loss is 0.0009585473453626037\n",
      "epoch: 5 step: 595, loss is 0.004100099205970764\n",
      "epoch: 5 step: 596, loss is 0.0006007302436046302\n",
      "epoch: 5 step: 597, loss is 0.0011997803812846541\n",
      "epoch: 5 step: 598, loss is 0.030229415744543076\n",
      "epoch: 5 step: 599, loss is 0.00046891887905076146\n",
      "epoch: 5 step: 600, loss is 0.0016663960414007306\n",
      "epoch: 5 step: 601, loss is 0.15339136123657227\n",
      "epoch: 5 step: 602, loss is 0.008917932398617268\n",
      "epoch: 5 step: 603, loss is 0.000366322259651497\n",
      "epoch: 5 step: 604, loss is 0.0033861196134239435\n",
      "epoch: 5 step: 605, loss is 0.07290488481521606\n",
      "epoch: 5 step: 606, loss is 0.0013174371561035514\n",
      "epoch: 5 step: 607, loss is 0.007999739609658718\n",
      "epoch: 5 step: 608, loss is 0.0019746257457882166\n",
      "epoch: 5 step: 609, loss is 0.014104663394391537\n",
      "epoch: 5 step: 610, loss is 0.00025304502923972905\n",
      "epoch: 5 step: 611, loss is 0.014666076749563217\n",
      "epoch: 5 step: 612, loss is 0.00028235005447641015\n",
      "epoch: 5 step: 613, loss is 0.01130212377756834\n",
      "epoch: 5 step: 614, loss is 0.02552253194153309\n",
      "epoch: 5 step: 615, loss is 0.0011227381182834506\n",
      "epoch: 5 step: 616, loss is 0.0019203141564503312\n",
      "epoch: 5 step: 617, loss is 0.00590759702026844\n",
      "epoch: 5 step: 618, loss is 0.013324259780347347\n",
      "epoch: 5 step: 619, loss is 0.012423415668308735\n",
      "epoch: 5 step: 620, loss is 0.0034062571357935667\n",
      "epoch: 5 step: 621, loss is 0.0017999698175117373\n",
      "epoch: 5 step: 622, loss is 0.0018549631349742413\n",
      "epoch: 5 step: 623, loss is 0.0005100129055790603\n",
      "epoch: 5 step: 624, loss is 0.012782995589077473\n",
      "epoch: 5 step: 625, loss is 5.539299309020862e-05\n",
      "epoch: 5 step: 626, loss is 0.0009323516860604286\n",
      "epoch: 5 step: 627, loss is 0.006557460408657789\n",
      "epoch: 5 step: 628, loss is 0.0005759706255048513\n",
      "epoch: 5 step: 629, loss is 0.005921072326600552\n",
      "epoch: 5 step: 630, loss is 0.000894768163561821\n",
      "epoch: 5 step: 631, loss is 0.005283819045871496\n",
      "epoch: 5 step: 632, loss is 0.014904665760695934\n",
      "epoch: 5 step: 633, loss is 0.03403177112340927\n",
      "epoch: 5 step: 634, loss is 0.004184105433523655\n",
      "epoch: 5 step: 635, loss is 0.003335926216095686\n",
      "epoch: 5 step: 636, loss is 0.02197185717523098\n",
      "epoch: 5 step: 637, loss is 0.0008607881609350443\n",
      "epoch: 5 step: 638, loss is 0.000182678661076352\n",
      "epoch: 5 step: 639, loss is 0.0012248740531504154\n",
      "epoch: 5 step: 640, loss is 0.0027460427954792976\n",
      "epoch: 5 step: 641, loss is 0.001140042906627059\n",
      "epoch: 5 step: 642, loss is 0.0003091235994361341\n",
      "epoch: 5 step: 643, loss is 0.003439379623159766\n",
      "epoch: 5 step: 644, loss is 0.0007632292690686882\n",
      "epoch: 5 step: 645, loss is 0.0009455375839024782\n",
      "epoch: 5 step: 646, loss is 0.00013306172331795096\n",
      "epoch: 5 step: 647, loss is 0.0017505129799246788\n",
      "epoch: 5 step: 648, loss is 0.0002761388896033168\n",
      "epoch: 5 step: 649, loss is 0.0006336073856800795\n",
      "epoch: 5 step: 650, loss is 0.00013301045692060143\n",
      "epoch: 5 step: 651, loss is 0.0005953379441052675\n",
      "epoch: 5 step: 652, loss is 0.02287871018052101\n",
      "epoch: 5 step: 653, loss is 0.0006404376472346485\n",
      "epoch: 5 step: 654, loss is 0.000993231893517077\n",
      "epoch: 5 step: 655, loss is 0.009306650608778\n",
      "epoch: 5 step: 656, loss is 0.0033843775745481253\n",
      "epoch: 5 step: 657, loss is 0.0015188425313681364\n",
      "epoch: 5 step: 658, loss is 0.00041307666106149554\n",
      "epoch: 5 step: 659, loss is 0.00019314934615977108\n",
      "epoch: 5 step: 660, loss is 0.0014500117395073175\n",
      "epoch: 5 step: 661, loss is 0.030266057699918747\n",
      "epoch: 5 step: 662, loss is 0.0031594503670930862\n",
      "epoch: 5 step: 663, loss is 0.00027230510022491217\n",
      "epoch: 5 step: 664, loss is 0.0006372321513481438\n",
      "epoch: 5 step: 665, loss is 0.008090999908745289\n",
      "epoch: 5 step: 666, loss is 0.0011521611595526338\n",
      "epoch: 5 step: 667, loss is 0.0024367186706513166\n",
      "epoch: 5 step: 668, loss is 0.003331209998577833\n",
      "epoch: 5 step: 669, loss is 0.10178767889738083\n",
      "epoch: 5 step: 670, loss is 9.145247167907655e-05\n",
      "epoch: 5 step: 671, loss is 0.22813208401203156\n",
      "epoch: 5 step: 672, loss is 3.1320698326453567e-05\n",
      "epoch: 5 step: 673, loss is 3.927836223738268e-05\n",
      "epoch: 5 step: 674, loss is 0.0002453725610394031\n",
      "epoch: 5 step: 675, loss is 0.00032758916495367885\n",
      "epoch: 5 step: 676, loss is 0.08995569497346878\n",
      "epoch: 5 step: 677, loss is 0.016736898571252823\n",
      "epoch: 5 step: 678, loss is 0.001669040648266673\n",
      "epoch: 5 step: 679, loss is 0.0003791044873651117\n",
      "epoch: 5 step: 680, loss is 0.005843869876116514\n",
      "epoch: 5 step: 681, loss is 0.032460134476423264\n",
      "epoch: 5 step: 682, loss is 0.014794304966926575\n",
      "epoch: 5 step: 683, loss is 0.0003965144569519907\n",
      "epoch: 5 step: 684, loss is 0.1545068770647049\n",
      "epoch: 5 step: 685, loss is 0.13728806376457214\n",
      "epoch: 5 step: 686, loss is 0.0363355353474617\n",
      "epoch: 5 step: 687, loss is 0.0037908428348600864\n",
      "epoch: 5 step: 688, loss is 0.00621738750487566\n",
      "epoch: 5 step: 689, loss is 0.11414989084005356\n",
      "epoch: 5 step: 690, loss is 0.0034074641298502684\n",
      "epoch: 5 step: 691, loss is 0.007707471027970314\n",
      "epoch: 5 step: 692, loss is 0.000492248625960201\n",
      "epoch: 5 step: 693, loss is 0.0009623476071283221\n",
      "epoch: 5 step: 694, loss is 0.000613708863966167\n",
      "epoch: 5 step: 695, loss is 0.004471897147595882\n",
      "epoch: 5 step: 696, loss is 0.19688335061073303\n",
      "epoch: 5 step: 697, loss is 0.000465494318632409\n",
      "epoch: 5 step: 698, loss is 0.019373644143342972\n",
      "epoch: 5 step: 699, loss is 0.0003574387519620359\n",
      "epoch: 5 step: 700, loss is 0.003082551993429661\n",
      "epoch: 5 step: 701, loss is 0.08005110919475555\n",
      "epoch: 5 step: 702, loss is 0.004642202518880367\n",
      "epoch: 5 step: 703, loss is 0.05852843075990677\n",
      "epoch: 5 step: 704, loss is 0.01393737643957138\n",
      "epoch: 5 step: 705, loss is 0.002974199131131172\n",
      "epoch: 5 step: 706, loss is 0.005317597649991512\n",
      "epoch: 5 step: 707, loss is 0.053307048976421356\n",
      "epoch: 5 step: 708, loss is 0.0046949307434260845\n",
      "epoch: 5 step: 709, loss is 0.0001615919463802129\n",
      "epoch: 5 step: 710, loss is 0.0037841848097741604\n",
      "epoch: 5 step: 711, loss is 0.27228567004203796\n",
      "epoch: 5 step: 712, loss is 0.0010310567449778318\n",
      "epoch: 5 step: 713, loss is 0.022099805995821953\n",
      "epoch: 5 step: 714, loss is 0.0278997290879488\n",
      "epoch: 5 step: 715, loss is 0.09542979300022125\n",
      "epoch: 5 step: 716, loss is 0.0010809666709974408\n",
      "epoch: 5 step: 717, loss is 0.000500716210808605\n",
      "epoch: 5 step: 718, loss is 0.07708714157342911\n",
      "epoch: 5 step: 719, loss is 0.08531645685434341\n",
      "epoch: 5 step: 720, loss is 0.001323986449278891\n",
      "epoch: 5 step: 721, loss is 0.0047375913709402084\n",
      "epoch: 5 step: 722, loss is 0.007556893862783909\n",
      "epoch: 5 step: 723, loss is 0.012316495180130005\n",
      "epoch: 5 step: 724, loss is 0.0424051359295845\n",
      "epoch: 5 step: 725, loss is 0.009887528605759144\n",
      "epoch: 5 step: 726, loss is 0.007860113866627216\n",
      "epoch: 5 step: 727, loss is 0.005377492401748896\n",
      "epoch: 5 step: 728, loss is 0.0009283866384066641\n",
      "epoch: 5 step: 729, loss is 0.01505318284034729\n",
      "epoch: 5 step: 730, loss is 0.03563380986452103\n",
      "epoch: 5 step: 731, loss is 0.013432404957711697\n",
      "epoch: 5 step: 732, loss is 0.004787940066307783\n",
      "epoch: 5 step: 733, loss is 0.011765288189053535\n",
      "epoch: 5 step: 734, loss is 0.025544283911585808\n",
      "epoch: 5 step: 735, loss is 0.08329632878303528\n",
      "epoch: 5 step: 736, loss is 0.010545585304498672\n",
      "epoch: 5 step: 737, loss is 0.2979593575000763\n",
      "epoch: 5 step: 738, loss is 0.09469885379076004\n",
      "epoch: 5 step: 739, loss is 0.006854668725281954\n",
      "epoch: 5 step: 740, loss is 0.0008811448933556676\n",
      "epoch: 5 step: 741, loss is 0.004134522285312414\n",
      "epoch: 5 step: 742, loss is 0.032672565430402756\n",
      "epoch: 5 step: 743, loss is 0.0026173817459493876\n",
      "epoch: 5 step: 744, loss is 0.027786333113908768\n",
      "epoch: 5 step: 745, loss is 0.0005626842030324042\n",
      "epoch: 5 step: 746, loss is 0.0005504547734744847\n",
      "epoch: 5 step: 747, loss is 0.0017930017784237862\n",
      "epoch: 5 step: 748, loss is 0.0010619084350764751\n",
      "epoch: 5 step: 749, loss is 0.01621062122285366\n",
      "epoch: 5 step: 750, loss is 0.148962140083313\n",
      "epoch: 5 step: 751, loss is 0.0017882587853819132\n",
      "epoch: 5 step: 752, loss is 0.0040970612317323685\n",
      "epoch: 5 step: 753, loss is 0.001054116291925311\n",
      "epoch: 5 step: 754, loss is 0.001682040630839765\n",
      "epoch: 5 step: 755, loss is 0.4372958242893219\n",
      "epoch: 5 step: 756, loss is 0.0012464819010347128\n",
      "epoch: 5 step: 757, loss is 0.008332211524248123\n",
      "epoch: 5 step: 758, loss is 0.016509419307112694\n",
      "epoch: 5 step: 759, loss is 0.003598603652790189\n",
      "epoch: 5 step: 760, loss is 0.0013276275712996721\n",
      "epoch: 5 step: 761, loss is 0.07881681621074677\n",
      "epoch: 5 step: 762, loss is 0.05973072722554207\n",
      "epoch: 5 step: 763, loss is 0.00017191431834362447\n",
      "epoch: 5 step: 764, loss is 0.010351463221013546\n",
      "epoch: 5 step: 765, loss is 0.01333045493811369\n",
      "epoch: 5 step: 766, loss is 0.012955348007380962\n",
      "epoch: 5 step: 767, loss is 0.00015243273810483515\n",
      "epoch: 5 step: 768, loss is 0.0004913967568427324\n",
      "epoch: 5 step: 769, loss is 0.00468397093936801\n",
      "epoch: 5 step: 770, loss is 0.02716214768588543\n",
      "epoch: 5 step: 771, loss is 0.04862240329384804\n",
      "epoch: 5 step: 772, loss is 0.00011742798233171925\n",
      "epoch: 5 step: 773, loss is 0.013244222849607468\n",
      "epoch: 5 step: 774, loss is 0.0011400649091228843\n",
      "epoch: 5 step: 775, loss is 0.03730015456676483\n",
      "epoch: 5 step: 776, loss is 0.009614554233849049\n",
      "epoch: 5 step: 777, loss is 0.0021145474165678024\n",
      "epoch: 5 step: 778, loss is 0.10768229514360428\n",
      "epoch: 5 step: 779, loss is 0.0024899591226130724\n",
      "epoch: 5 step: 780, loss is 0.001759790349751711\n",
      "epoch: 5 step: 781, loss is 0.05899294093251228\n",
      "epoch: 5 step: 782, loss is 0.14523637294769287\n",
      "epoch: 5 step: 783, loss is 0.010771459899842739\n",
      "epoch: 5 step: 784, loss is 0.0027328929863870144\n",
      "epoch: 5 step: 785, loss is 0.009304334409534931\n",
      "epoch: 5 step: 786, loss is 0.0022319178096950054\n",
      "epoch: 5 step: 787, loss is 0.00017547924653626978\n",
      "epoch: 5 step: 788, loss is 0.0001610390900168568\n",
      "epoch: 5 step: 789, loss is 0.013047927990555763\n",
      "epoch: 5 step: 790, loss is 0.0015272634336724877\n",
      "epoch: 5 step: 791, loss is 0.0009269596193917096\n",
      "epoch: 5 step: 792, loss is 0.026548273861408234\n",
      "epoch: 5 step: 793, loss is 0.005871329922229052\n",
      "epoch: 5 step: 794, loss is 0.0029337615706026554\n",
      "epoch: 5 step: 795, loss is 0.015184739604592323\n",
      "epoch: 5 step: 796, loss is 0.0010018477914854884\n",
      "epoch: 5 step: 797, loss is 0.025329548865556717\n",
      "epoch: 5 step: 798, loss is 0.0059274947270751\n",
      "epoch: 5 step: 799, loss is 0.013875970616936684\n",
      "epoch: 5 step: 800, loss is 0.07602015882730484\n",
      "epoch: 5 step: 801, loss is 0.0010292439255863428\n",
      "epoch: 5 step: 802, loss is 0.009972861036658287\n",
      "epoch: 5 step: 803, loss is 0.07262454181909561\n",
      "epoch: 5 step: 804, loss is 0.008013933897018433\n",
      "epoch: 5 step: 805, loss is 0.03849094361066818\n",
      "epoch: 5 step: 806, loss is 0.0004912951262667775\n",
      "epoch: 5 step: 807, loss is 0.06538330763578415\n",
      "epoch: 5 step: 808, loss is 0.1361619234085083\n",
      "epoch: 5 step: 809, loss is 0.0016928561963140965\n",
      "epoch: 5 step: 810, loss is 0.00019442761549726129\n",
      "epoch: 5 step: 811, loss is 0.0021760440431535244\n",
      "epoch: 5 step: 812, loss is 0.0009038601419888437\n",
      "epoch: 5 step: 813, loss is 0.0023274263367056847\n",
      "epoch: 5 step: 814, loss is 0.0006596358143724501\n",
      "epoch: 5 step: 815, loss is 0.03226479887962341\n",
      "epoch: 5 step: 816, loss is 0.01455188449472189\n",
      "epoch: 5 step: 817, loss is 0.02903221920132637\n",
      "epoch: 5 step: 818, loss is 0.000477924186270684\n",
      "epoch: 5 step: 819, loss is 0.0008984066662378609\n",
      "epoch: 5 step: 820, loss is 0.028475893661379814\n",
      "epoch: 5 step: 821, loss is 0.000477441877592355\n",
      "epoch: 5 step: 822, loss is 0.0008396586636081338\n",
      "epoch: 5 step: 823, loss is 0.00011192391684744507\n",
      "epoch: 5 step: 824, loss is 0.0002940820704679936\n",
      "epoch: 5 step: 825, loss is 0.00989664439111948\n",
      "epoch: 5 step: 826, loss is 0.045079201459884644\n",
      "epoch: 5 step: 827, loss is 0.007295540999621153\n",
      "epoch: 5 step: 828, loss is 0.00022337936388794333\n",
      "epoch: 5 step: 829, loss is 0.0014218322467058897\n",
      "epoch: 5 step: 830, loss is 0.03175036609172821\n",
      "epoch: 5 step: 831, loss is 0.00022716427338309586\n",
      "epoch: 5 step: 832, loss is 0.02417081594467163\n",
      "epoch: 5 step: 833, loss is 0.08411303907632828\n",
      "epoch: 5 step: 834, loss is 0.004876977298408747\n",
      "epoch: 5 step: 835, loss is 0.0008907296578399837\n",
      "epoch: 5 step: 836, loss is 0.02077840454876423\n",
      "epoch: 5 step: 837, loss is 0.0014809833373874426\n",
      "epoch: 5 step: 838, loss is 0.04073748737573624\n",
      "epoch: 5 step: 839, loss is 0.11315327882766724\n",
      "epoch: 5 step: 840, loss is 0.0009111915715038776\n",
      "epoch: 5 step: 841, loss is 0.01480405405163765\n",
      "epoch: 5 step: 842, loss is 0.0250169076025486\n",
      "epoch: 5 step: 843, loss is 0.0010602514958009124\n",
      "epoch: 5 step: 844, loss is 0.0004331407544668764\n",
      "epoch: 5 step: 845, loss is 0.026469003409147263\n",
      "epoch: 5 step: 846, loss is 0.015260990709066391\n",
      "epoch: 5 step: 847, loss is 0.03526056557893753\n",
      "epoch: 5 step: 848, loss is 0.01195590477436781\n",
      "epoch: 5 step: 849, loss is 0.30363014340400696\n",
      "epoch: 5 step: 850, loss is 0.0841120034456253\n",
      "epoch: 5 step: 851, loss is 0.0011074816575273871\n",
      "epoch: 5 step: 852, loss is 0.0001509168796474114\n",
      "epoch: 5 step: 853, loss is 0.00035111545003019273\n",
      "epoch: 5 step: 854, loss is 0.0005491874180734158\n",
      "epoch: 5 step: 855, loss is 0.030133657157421112\n",
      "epoch: 5 step: 856, loss is 0.0006685249391011894\n",
      "epoch: 5 step: 857, loss is 0.01012840960174799\n",
      "epoch: 5 step: 858, loss is 0.13968046009540558\n",
      "epoch: 5 step: 859, loss is 0.0001156887665274553\n",
      "epoch: 5 step: 860, loss is 0.01956860162317753\n",
      "epoch: 5 step: 861, loss is 0.006107673980295658\n",
      "epoch: 5 step: 862, loss is 0.0015320259844884276\n",
      "epoch: 5 step: 863, loss is 0.004315034486353397\n",
      "epoch: 5 step: 864, loss is 0.0026855994947254658\n",
      "epoch: 5 step: 865, loss is 0.003343600779771805\n",
      "epoch: 5 step: 866, loss is 0.0025755269452929497\n",
      "epoch: 5 step: 867, loss is 0.003512368071824312\n",
      "epoch: 5 step: 868, loss is 0.11506451666355133\n",
      "epoch: 5 step: 869, loss is 0.002426085527986288\n",
      "epoch: 5 step: 870, loss is 0.0006573241553269327\n",
      "epoch: 5 step: 871, loss is 0.0004922680673189461\n",
      "epoch: 5 step: 872, loss is 0.0017603422747924924\n",
      "epoch: 5 step: 873, loss is 0.005346743855625391\n",
      "epoch: 5 step: 874, loss is 0.012780992314219475\n",
      "epoch: 5 step: 875, loss is 0.03685625642538071\n",
      "epoch: 5 step: 876, loss is 0.015462187118828297\n",
      "epoch: 5 step: 877, loss is 0.00017614391981624067\n",
      "epoch: 5 step: 878, loss is 0.0001460729690734297\n",
      "epoch: 5 step: 879, loss is 0.0015957336872816086\n",
      "epoch: 5 step: 880, loss is 0.04030647501349449\n",
      "epoch: 5 step: 881, loss is 0.04813743755221367\n",
      "epoch: 5 step: 882, loss is 0.0004931768635287881\n",
      "epoch: 5 step: 883, loss is 0.009983319789171219\n",
      "epoch: 5 step: 884, loss is 0.0069016520865261555\n",
      "epoch: 5 step: 885, loss is 0.010613874532282352\n",
      "epoch: 5 step: 886, loss is 0.10869308561086655\n",
      "epoch: 5 step: 887, loss is 0.004527228884398937\n",
      "epoch: 5 step: 888, loss is 0.0002229863457614556\n",
      "epoch: 5 step: 889, loss is 0.023291543126106262\n",
      "epoch: 5 step: 890, loss is 0.0034400897566229105\n",
      "epoch: 5 step: 891, loss is 0.0026842374354600906\n",
      "epoch: 5 step: 892, loss is 0.00016275144298560917\n",
      "epoch: 5 step: 893, loss is 0.007295230403542519\n",
      "epoch: 5 step: 894, loss is 0.002595374593511224\n",
      "epoch: 5 step: 895, loss is 0.004445408470928669\n",
      "epoch: 5 step: 896, loss is 0.002058044308796525\n",
      "epoch: 5 step: 897, loss is 0.0005124331219121814\n",
      "epoch: 5 step: 898, loss is 0.011179814115166664\n",
      "epoch: 5 step: 899, loss is 0.009485279209911823\n",
      "epoch: 5 step: 900, loss is 0.03716569393873215\n",
      "epoch: 5 step: 901, loss is 0.03882218524813652\n",
      "epoch: 5 step: 902, loss is 0.006174460053443909\n",
      "epoch: 5 step: 903, loss is 0.0034627162385731936\n",
      "epoch: 5 step: 904, loss is 0.0024227488320320845\n",
      "epoch: 5 step: 905, loss is 0.009810755960643291\n",
      "epoch: 5 step: 906, loss is 0.008316573686897755\n",
      "epoch: 5 step: 907, loss is 0.000358809920726344\n",
      "epoch: 5 step: 908, loss is 0.0013752583181485534\n",
      "epoch: 5 step: 909, loss is 0.008785635232925415\n",
      "epoch: 5 step: 910, loss is 0.01207698229700327\n",
      "epoch: 5 step: 911, loss is 0.006778811104595661\n",
      "epoch: 5 step: 912, loss is 0.1623605638742447\n",
      "epoch: 5 step: 913, loss is 0.001709392643533647\n",
      "epoch: 5 step: 914, loss is 0.006898088380694389\n",
      "epoch: 5 step: 915, loss is 0.10827969759702682\n",
      "epoch: 5 step: 916, loss is 0.12596119940280914\n",
      "epoch: 5 step: 917, loss is 0.00838868785649538\n",
      "epoch: 5 step: 918, loss is 0.05470578745007515\n",
      "epoch: 5 step: 919, loss is 0.0032378840260207653\n",
      "epoch: 5 step: 920, loss is 6.787604070268571e-05\n",
      "epoch: 5 step: 921, loss is 0.024957837536931038\n",
      "epoch: 5 step: 922, loss is 0.004709519445896149\n",
      "epoch: 5 step: 923, loss is 0.00014977209502831101\n",
      "epoch: 5 step: 924, loss is 0.0007855395087972283\n",
      "epoch: 5 step: 925, loss is 0.06744963675737381\n",
      "epoch: 5 step: 926, loss is 0.006464249454438686\n",
      "epoch: 5 step: 927, loss is 0.14455732703208923\n",
      "epoch: 5 step: 928, loss is 0.12259683758020401\n",
      "epoch: 5 step: 929, loss is 0.00043690315214917064\n",
      "epoch: 5 step: 930, loss is 0.062200259417295456\n",
      "epoch: 5 step: 931, loss is 0.002868626732379198\n",
      "epoch: 5 step: 932, loss is 0.032314151525497437\n",
      "epoch: 5 step: 933, loss is 0.002768212929368019\n",
      "epoch: 5 step: 934, loss is 0.0016600030940026045\n",
      "epoch: 5 step: 935, loss is 0.008331280201673508\n",
      "epoch: 5 step: 936, loss is 0.11643294990062714\n",
      "epoch: 5 step: 937, loss is 0.002137724542990327\n",
      "epoch: 5 step: 938, loss is 0.015920154750347137\n",
      "epoch: 5 step: 939, loss is 0.09494131058454514\n",
      "epoch: 5 step: 940, loss is 0.048068854957818985\n",
      "epoch: 5 step: 941, loss is 0.013571668416261673\n",
      "epoch: 5 step: 942, loss is 0.3632522225379944\n",
      "epoch: 5 step: 943, loss is 0.0005401777452789247\n",
      "epoch: 5 step: 944, loss is 0.0006058475119061768\n",
      "epoch: 5 step: 945, loss is 0.0483480729162693\n",
      "epoch: 5 step: 946, loss is 0.2197597324848175\n",
      "epoch: 5 step: 947, loss is 0.006608897354453802\n",
      "epoch: 5 step: 948, loss is 0.02482571452856064\n",
      "epoch: 5 step: 949, loss is 0.110901840031147\n",
      "epoch: 5 step: 950, loss is 0.007348630111664534\n",
      "epoch: 5 step: 951, loss is 0.141121968626976\n",
      "epoch: 5 step: 952, loss is 0.03819181025028229\n",
      "epoch: 5 step: 953, loss is 0.0010263273725286126\n",
      "epoch: 5 step: 954, loss is 0.0011503235436975956\n",
      "epoch: 5 step: 955, loss is 0.013477787375450134\n",
      "epoch: 5 step: 956, loss is 0.010957835242152214\n",
      "epoch: 5 step: 957, loss is 0.00018082695896737278\n",
      "epoch: 5 step: 958, loss is 0.009082690812647343\n",
      "epoch: 5 step: 959, loss is 0.0003000773140229285\n",
      "epoch: 5 step: 960, loss is 0.004396004602313042\n",
      "epoch: 5 step: 961, loss is 0.0018587391823530197\n",
      "epoch: 5 step: 962, loss is 0.001472159055992961\n",
      "epoch: 5 step: 963, loss is 0.05431438237428665\n",
      "epoch: 5 step: 964, loss is 0.008000126108527184\n",
      "epoch: 5 step: 965, loss is 0.09042143821716309\n",
      "epoch: 5 step: 966, loss is 0.006848442833870649\n",
      "epoch: 5 step: 967, loss is 0.0034306631423532963\n",
      "epoch: 5 step: 968, loss is 0.029805747792124748\n",
      "epoch: 5 step: 969, loss is 0.004559319466352463\n",
      "epoch: 5 step: 970, loss is 0.0013527171686291695\n",
      "epoch: 5 step: 971, loss is 0.06312049925327301\n",
      "epoch: 5 step: 972, loss is 0.0021208683028817177\n",
      "epoch: 5 step: 973, loss is 0.1141468957066536\n",
      "epoch: 5 step: 974, loss is 0.000624811218585819\n",
      "epoch: 5 step: 975, loss is 0.0004932848387397826\n",
      "epoch: 5 step: 976, loss is 0.0955066978931427\n",
      "epoch: 5 step: 977, loss is 0.061912890523672104\n",
      "epoch: 5 step: 978, loss is 0.1549897938966751\n",
      "epoch: 5 step: 979, loss is 0.00039896636735647917\n",
      "epoch: 5 step: 980, loss is 0.06586094945669174\n",
      "epoch: 5 step: 981, loss is 0.047040898352861404\n",
      "epoch: 5 step: 982, loss is 0.0517105758190155\n",
      "epoch: 5 step: 983, loss is 0.17867480218410492\n",
      "epoch: 5 step: 984, loss is 0.004091877955943346\n",
      "epoch: 5 step: 985, loss is 0.03610154986381531\n",
      "epoch: 5 step: 986, loss is 0.07405507564544678\n",
      "epoch: 5 step: 987, loss is 0.03581004589796066\n",
      "epoch: 5 step: 988, loss is 0.0007819642778486013\n",
      "epoch: 5 step: 989, loss is 0.0620354488492012\n",
      "epoch: 5 step: 990, loss is 0.0008036490762606263\n",
      "epoch: 5 step: 991, loss is 0.027099449187517166\n",
      "epoch: 5 step: 992, loss is 0.02715998888015747\n",
      "epoch: 5 step: 993, loss is 0.0012718336656689644\n",
      "epoch: 5 step: 994, loss is 0.001269012806005776\n",
      "epoch: 5 step: 995, loss is 0.003977229353040457\n",
      "epoch: 5 step: 996, loss is 0.0006149596883915365\n",
      "epoch: 5 step: 997, loss is 0.022915543988347054\n",
      "epoch: 5 step: 998, loss is 0.0036707662511616945\n",
      "epoch: 5 step: 999, loss is 0.001406845636665821\n",
      "epoch: 5 step: 1000, loss is 0.07131557911634445\n",
      "epoch: 5 step: 1001, loss is 0.1049499586224556\n",
      "epoch: 5 step: 1002, loss is 0.034225381910800934\n",
      "epoch: 5 step: 1003, loss is 0.02091694436967373\n",
      "epoch: 5 step: 1004, loss is 0.05374383181333542\n",
      "epoch: 5 step: 1005, loss is 0.022860093042254448\n",
      "epoch: 5 step: 1006, loss is 0.007594269700348377\n",
      "epoch: 5 step: 1007, loss is 0.016012152656912804\n",
      "epoch: 5 step: 1008, loss is 0.0005749373813159764\n",
      "epoch: 5 step: 1009, loss is 0.045075345784425735\n",
      "epoch: 5 step: 1010, loss is 0.0006521727191284299\n",
      "epoch: 5 step: 1011, loss is 0.008913397789001465\n",
      "epoch: 5 step: 1012, loss is 0.12477117776870728\n",
      "epoch: 5 step: 1013, loss is 0.18793895840644836\n",
      "epoch: 5 step: 1014, loss is 0.00027493410743772984\n",
      "epoch: 5 step: 1015, loss is 0.030904831364750862\n",
      "epoch: 5 step: 1016, loss is 0.005131889134645462\n",
      "epoch: 5 step: 1017, loss is 0.0007058426272124052\n",
      "epoch: 5 step: 1018, loss is 0.16548040509223938\n",
      "epoch: 5 step: 1019, loss is 0.0011947077000513673\n",
      "epoch: 5 step: 1020, loss is 0.00671527162194252\n",
      "epoch: 5 step: 1021, loss is 0.0014018458314239979\n",
      "epoch: 5 step: 1022, loss is 0.0033812157344073057\n",
      "epoch: 5 step: 1023, loss is 0.00837030354887247\n",
      "epoch: 5 step: 1024, loss is 0.004983970895409584\n",
      "epoch: 5 step: 1025, loss is 0.0007410967955365777\n",
      "epoch: 5 step: 1026, loss is 0.0040825363248586655\n",
      "epoch: 5 step: 1027, loss is 0.014242136850953102\n",
      "epoch: 5 step: 1028, loss is 0.04814031347632408\n",
      "epoch: 5 step: 1029, loss is 0.004403602797538042\n",
      "epoch: 5 step: 1030, loss is 0.0017781970091164112\n",
      "epoch: 5 step: 1031, loss is 0.059762079268693924\n",
      "epoch: 5 step: 1032, loss is 0.015589197166264057\n",
      "epoch: 5 step: 1033, loss is 0.004901576321572065\n",
      "epoch: 5 step: 1034, loss is 0.008478093892335892\n",
      "epoch: 5 step: 1035, loss is 0.02738480269908905\n",
      "epoch: 5 step: 1036, loss is 0.0011123318690806627\n",
      "epoch: 5 step: 1037, loss is 0.13141106069087982\n",
      "epoch: 5 step: 1038, loss is 0.022378239780664444\n",
      "epoch: 5 step: 1039, loss is 0.1027081236243248\n",
      "epoch: 5 step: 1040, loss is 0.16968020796775818\n",
      "epoch: 5 step: 1041, loss is 0.02290140651166439\n",
      "epoch: 5 step: 1042, loss is 0.014408625662326813\n",
      "epoch: 5 step: 1043, loss is 0.0007213777862489223\n",
      "epoch: 5 step: 1044, loss is 0.011586565524339676\n",
      "epoch: 5 step: 1045, loss is 0.0038566854782402515\n",
      "epoch: 5 step: 1046, loss is 0.0655769407749176\n",
      "epoch: 5 step: 1047, loss is 0.002000602660700679\n",
      "epoch: 5 step: 1048, loss is 0.0010025127558037639\n",
      "epoch: 5 step: 1049, loss is 0.0009959457674995065\n",
      "epoch: 5 step: 1050, loss is 0.013555703684687614\n",
      "epoch: 5 step: 1051, loss is 0.002007860690355301\n",
      "epoch: 5 step: 1052, loss is 0.002752294298261404\n",
      "epoch: 5 step: 1053, loss is 0.012351092882454395\n",
      "epoch: 5 step: 1054, loss is 0.07002091407775879\n",
      "epoch: 5 step: 1055, loss is 0.05649980902671814\n",
      "epoch: 5 step: 1056, loss is 0.0011767923133447766\n",
      "epoch: 5 step: 1057, loss is 0.2223978191614151\n",
      "epoch: 5 step: 1058, loss is 0.008125657215714455\n",
      "epoch: 5 step: 1059, loss is 0.0008605666225776076\n",
      "epoch: 5 step: 1060, loss is 0.0028229502495378256\n",
      "epoch: 5 step: 1061, loss is 0.0003708393778651953\n",
      "epoch: 5 step: 1062, loss is 0.01997908018529415\n",
      "epoch: 5 step: 1063, loss is 0.0004239559930283576\n",
      "epoch: 5 step: 1064, loss is 0.07600105553865433\n",
      "epoch: 5 step: 1065, loss is 0.009900136850774288\n",
      "epoch: 5 step: 1066, loss is 0.004441431723535061\n",
      "epoch: 5 step: 1067, loss is 0.002172711305320263\n",
      "epoch: 5 step: 1068, loss is 0.002407216466963291\n",
      "epoch: 5 step: 1069, loss is 0.00046346639283001423\n",
      "epoch: 5 step: 1070, loss is 0.007892598398029804\n",
      "epoch: 5 step: 1071, loss is 0.009506982751190662\n",
      "epoch: 5 step: 1072, loss is 0.0006664468091912568\n",
      "epoch: 5 step: 1073, loss is 0.0921827033162117\n",
      "epoch: 5 step: 1074, loss is 0.006425900384783745\n",
      "epoch: 5 step: 1075, loss is 0.008152823895215988\n",
      "epoch: 5 step: 1076, loss is 0.0017458359943702817\n",
      "epoch: 5 step: 1077, loss is 0.007686342112720013\n",
      "epoch: 5 step: 1078, loss is 0.004284271504729986\n",
      "epoch: 5 step: 1079, loss is 0.0010114723118022084\n",
      "epoch: 5 step: 1080, loss is 0.0027559949085116386\n",
      "epoch: 5 step: 1081, loss is 0.07561108469963074\n",
      "epoch: 5 step: 1082, loss is 0.011042041704058647\n",
      "epoch: 5 step: 1083, loss is 0.02430727146565914\n",
      "epoch: 5 step: 1084, loss is 0.08114577829837799\n",
      "epoch: 5 step: 1085, loss is 0.016316164284944534\n",
      "epoch: 5 step: 1086, loss is 0.005744151305407286\n",
      "epoch: 5 step: 1087, loss is 0.00024130509700626135\n",
      "epoch: 5 step: 1088, loss is 0.020701857283711433\n",
      "epoch: 5 step: 1089, loss is 0.014093431644141674\n",
      "epoch: 5 step: 1090, loss is 0.0036829831078648567\n",
      "epoch: 5 step: 1091, loss is 0.16614072024822235\n",
      "epoch: 5 step: 1092, loss is 0.005130134057253599\n",
      "epoch: 5 step: 1093, loss is 0.007060653995722532\n",
      "epoch: 5 step: 1094, loss is 0.014398147352039814\n",
      "epoch: 5 step: 1095, loss is 0.0001043070005835034\n",
      "epoch: 5 step: 1096, loss is 0.0004920539213344455\n",
      "epoch: 5 step: 1097, loss is 0.006579929497092962\n",
      "epoch: 5 step: 1098, loss is 0.03570546209812164\n",
      "epoch: 5 step: 1099, loss is 0.07306009531021118\n",
      "epoch: 5 step: 1100, loss is 0.00041228579357266426\n",
      "epoch: 5 step: 1101, loss is 0.004931651055812836\n",
      "epoch: 5 step: 1102, loss is 0.00013116805348545313\n",
      "epoch: 5 step: 1103, loss is 0.0598442405462265\n",
      "epoch: 5 step: 1104, loss is 0.002043657237663865\n",
      "epoch: 5 step: 1105, loss is 0.0003279575612396002\n",
      "epoch: 5 step: 1106, loss is 0.02196098119020462\n",
      "epoch: 5 step: 1107, loss is 0.00371919060125947\n",
      "epoch: 5 step: 1108, loss is 0.11183779686689377\n",
      "epoch: 5 step: 1109, loss is 0.0002338403428439051\n",
      "epoch: 5 step: 1110, loss is 0.03415357321500778\n",
      "epoch: 5 step: 1111, loss is 9.04975167941302e-05\n",
      "epoch: 5 step: 1112, loss is 0.004593363031744957\n",
      "epoch: 5 step: 1113, loss is 0.000377726013539359\n",
      "epoch: 5 step: 1114, loss is 0.01270117238163948\n",
      "epoch: 5 step: 1115, loss is 0.09000010788440704\n",
      "epoch: 5 step: 1116, loss is 0.19976770877838135\n",
      "epoch: 5 step: 1117, loss is 0.04218880459666252\n",
      "epoch: 5 step: 1118, loss is 0.009490186348557472\n",
      "epoch: 5 step: 1119, loss is 0.024711253121495247\n",
      "epoch: 5 step: 1120, loss is 0.033133767545223236\n",
      "epoch: 5 step: 1121, loss is 0.04003656283020973\n",
      "epoch: 5 step: 1122, loss is 0.0037373469676822424\n",
      "epoch: 5 step: 1123, loss is 0.029466910287737846\n",
      "epoch: 5 step: 1124, loss is 0.0005551152862608433\n",
      "epoch: 5 step: 1125, loss is 9.656787005951628e-05\n",
      "epoch: 5 step: 1126, loss is 0.00235011987388134\n",
      "epoch: 5 step: 1127, loss is 0.00396651541814208\n",
      "epoch: 5 step: 1128, loss is 0.01704292930662632\n",
      "epoch: 5 step: 1129, loss is 0.0007490666466765106\n",
      "epoch: 5 step: 1130, loss is 0.006944493856281042\n",
      "epoch: 5 step: 1131, loss is 0.017323728650808334\n",
      "epoch: 5 step: 1132, loss is 0.0018089764053002\n",
      "epoch: 5 step: 1133, loss is 0.1744057685136795\n",
      "epoch: 5 step: 1134, loss is 0.005017354618757963\n",
      "epoch: 5 step: 1135, loss is 0.0012418875703588128\n",
      "epoch: 5 step: 1136, loss is 0.05100786313414574\n",
      "epoch: 5 step: 1137, loss is 0.006700982339680195\n",
      "epoch: 5 step: 1138, loss is 0.03767828270792961\n",
      "epoch: 5 step: 1139, loss is 0.0027668150141835213\n",
      "epoch: 5 step: 1140, loss is 0.06530195474624634\n",
      "epoch: 5 step: 1141, loss is 0.0798063799738884\n",
      "epoch: 5 step: 1142, loss is 0.002936944365501404\n",
      "epoch: 5 step: 1143, loss is 0.003775525838136673\n",
      "epoch: 5 step: 1144, loss is 0.0013118558563292027\n",
      "epoch: 5 step: 1145, loss is 0.00811058934777975\n",
      "epoch: 5 step: 1146, loss is 0.01371182780712843\n",
      "epoch: 5 step: 1147, loss is 0.04874732717871666\n",
      "epoch: 5 step: 1148, loss is 0.00014333805302157998\n",
      "epoch: 5 step: 1149, loss is 0.06778261065483093\n",
      "epoch: 5 step: 1150, loss is 0.07156667858362198\n",
      "epoch: 5 step: 1151, loss is 0.0018675887258723378\n",
      "epoch: 5 step: 1152, loss is 0.00022593201720155776\n",
      "epoch: 5 step: 1153, loss is 0.0018285936675965786\n",
      "epoch: 5 step: 1154, loss is 0.004429470747709274\n",
      "epoch: 5 step: 1155, loss is 0.0005569910281337798\n",
      "epoch: 5 step: 1156, loss is 0.08625289052724838\n",
      "epoch: 5 step: 1157, loss is 0.00042557925917208195\n",
      "epoch: 5 step: 1158, loss is 0.0004499608767218888\n",
      "epoch: 5 step: 1159, loss is 0.04530378058552742\n",
      "epoch: 5 step: 1160, loss is 0.013991889543831348\n",
      "epoch: 5 step: 1161, loss is 0.21308661997318268\n",
      "epoch: 5 step: 1162, loss is 0.029992280527949333\n",
      "epoch: 5 step: 1163, loss is 0.021166501566767693\n",
      "epoch: 5 step: 1164, loss is 0.0005342907388694584\n",
      "epoch: 5 step: 1165, loss is 0.0030503019224852324\n",
      "epoch: 5 step: 1166, loss is 0.00964003149420023\n",
      "epoch: 5 step: 1167, loss is 0.026593873277306557\n",
      "epoch: 5 step: 1168, loss is 0.05251683294773102\n",
      "epoch: 5 step: 1169, loss is 0.00922352820634842\n",
      "epoch: 5 step: 1170, loss is 0.11366777122020721\n",
      "epoch: 5 step: 1171, loss is 0.0010058442130684853\n",
      "epoch: 5 step: 1172, loss is 0.00889298040419817\n",
      "epoch: 5 step: 1173, loss is 0.005003983620554209\n",
      "epoch: 5 step: 1174, loss is 0.0017196886474266648\n",
      "epoch: 5 step: 1175, loss is 0.004236280918121338\n",
      "epoch: 5 step: 1176, loss is 0.014460345730185509\n",
      "epoch: 5 step: 1177, loss is 0.16066430509090424\n",
      "epoch: 5 step: 1178, loss is 0.07993269711732864\n",
      "epoch: 5 step: 1179, loss is 0.12145791202783585\n",
      "epoch: 5 step: 1180, loss is 0.0006278435466811061\n",
      "epoch: 5 step: 1181, loss is 0.0012187953107059002\n",
      "epoch: 5 step: 1182, loss is 0.004326959140598774\n",
      "epoch: 5 step: 1183, loss is 0.001072704792022705\n",
      "epoch: 5 step: 1184, loss is 0.003603678662329912\n",
      "epoch: 5 step: 1185, loss is 0.17518235743045807\n",
      "epoch: 5 step: 1186, loss is 0.1672660857439041\n",
      "epoch: 5 step: 1187, loss is 0.0010502367513254285\n",
      "epoch: 5 step: 1188, loss is 0.002768858801573515\n",
      "epoch: 5 step: 1189, loss is 0.0435965433716774\n",
      "epoch: 5 step: 1190, loss is 0.005187505390495062\n",
      "epoch: 5 step: 1191, loss is 0.002356395125389099\n",
      "epoch: 5 step: 1192, loss is 0.05233636125922203\n",
      "epoch: 5 step: 1193, loss is 0.0034328128676861525\n",
      "epoch: 5 step: 1194, loss is 0.046482350677251816\n",
      "epoch: 5 step: 1195, loss is 0.0069567980244755745\n",
      "epoch: 5 step: 1196, loss is 0.0041513144969940186\n",
      "epoch: 5 step: 1197, loss is 0.053346920758485794\n",
      "epoch: 5 step: 1198, loss is 0.011183254420757294\n",
      "epoch: 5 step: 1199, loss is 0.006052582059055567\n",
      "epoch: 5 step: 1200, loss is 0.005886271595954895\n",
      "epoch: 5 step: 1201, loss is 0.045432038605213165\n",
      "epoch: 5 step: 1202, loss is 0.0014512932393699884\n",
      "epoch: 5 step: 1203, loss is 0.0020990048069506884\n",
      "epoch: 5 step: 1204, loss is 0.002488688100129366\n",
      "epoch: 5 step: 1205, loss is 0.0015031484654173255\n",
      "epoch: 5 step: 1206, loss is 0.14772121608257294\n",
      "epoch: 5 step: 1207, loss is 0.0061432067304849625\n",
      "epoch: 5 step: 1208, loss is 0.014951387420296669\n",
      "epoch: 5 step: 1209, loss is 0.001531708985567093\n",
      "epoch: 5 step: 1210, loss is 0.003264862112700939\n",
      "epoch: 5 step: 1211, loss is 0.004010448232293129\n",
      "epoch: 5 step: 1212, loss is 0.05003245919942856\n",
      "epoch: 5 step: 1213, loss is 0.1555556058883667\n",
      "epoch: 5 step: 1214, loss is 0.009157214313745499\n",
      "epoch: 5 step: 1215, loss is 0.014031489379703999\n",
      "epoch: 5 step: 1216, loss is 0.001969721866771579\n",
      "epoch: 5 step: 1217, loss is 0.0010916285682469606\n",
      "epoch: 5 step: 1218, loss is 0.02166229858994484\n",
      "epoch: 5 step: 1219, loss is 0.008636601269245148\n",
      "epoch: 5 step: 1220, loss is 0.1819884330034256\n",
      "epoch: 5 step: 1221, loss is 0.0006626390386372805\n",
      "epoch: 5 step: 1222, loss is 0.00825082790106535\n",
      "epoch: 5 step: 1223, loss is 0.0004949317662976682\n",
      "epoch: 5 step: 1224, loss is 0.014806423336267471\n",
      "epoch: 5 step: 1225, loss is 0.0025040714535862207\n",
      "epoch: 5 step: 1226, loss is 0.004356685094535351\n",
      "epoch: 5 step: 1227, loss is 0.08581680059432983\n",
      "epoch: 5 step: 1228, loss is 0.07842770963907242\n",
      "epoch: 5 step: 1229, loss is 0.02389313466846943\n",
      "epoch: 5 step: 1230, loss is 0.003787827678024769\n",
      "epoch: 5 step: 1231, loss is 0.0013846714282408357\n",
      "epoch: 5 step: 1232, loss is 0.04738836735486984\n",
      "epoch: 5 step: 1233, loss is 0.0030416001100093126\n",
      "epoch: 5 step: 1234, loss is 0.011366195976734161\n",
      "epoch: 5 step: 1235, loss is 0.0024197944439947605\n",
      "epoch: 5 step: 1236, loss is 0.08465468883514404\n",
      "epoch: 5 step: 1237, loss is 0.0023141265846788883\n",
      "epoch: 5 step: 1238, loss is 0.0024885411839932203\n",
      "epoch: 5 step: 1239, loss is 0.10522179305553436\n",
      "epoch: 5 step: 1240, loss is 0.013143496587872505\n",
      "epoch: 5 step: 1241, loss is 0.00916880089789629\n",
      "epoch: 5 step: 1242, loss is 0.005728837102651596\n",
      "epoch: 5 step: 1243, loss is 0.0021431921049952507\n",
      "epoch: 5 step: 1244, loss is 0.0008755138260312378\n",
      "epoch: 5 step: 1245, loss is 0.00034300776314921677\n",
      "epoch: 5 step: 1246, loss is 0.0013679297408089042\n",
      "epoch: 5 step: 1247, loss is 0.2665955424308777\n",
      "epoch: 5 step: 1248, loss is 0.0024914781097322702\n",
      "epoch: 5 step: 1249, loss is 0.05852871015667915\n",
      "epoch: 5 step: 1250, loss is 0.06423316150903702\n",
      "epoch: 5 step: 1251, loss is 0.0003679126384668052\n",
      "epoch: 5 step: 1252, loss is 0.0005682302871719003\n",
      "epoch: 5 step: 1253, loss is 0.050423551350831985\n",
      "epoch: 5 step: 1254, loss is 0.0017315874574705958\n",
      "epoch: 5 step: 1255, loss is 0.02703550085425377\n",
      "epoch: 5 step: 1256, loss is 0.00792967900633812\n",
      "epoch: 5 step: 1257, loss is 0.006083081476390362\n",
      "epoch: 5 step: 1258, loss is 0.0015422722790390253\n",
      "epoch: 5 step: 1259, loss is 0.0034140164498239756\n",
      "epoch: 5 step: 1260, loss is 0.058482587337493896\n",
      "epoch: 5 step: 1261, loss is 0.0015451278304681182\n",
      "epoch: 5 step: 1262, loss is 0.005950895603746176\n",
      "epoch: 5 step: 1263, loss is 0.0033353553153574467\n",
      "epoch: 5 step: 1264, loss is 0.0012196055613458157\n",
      "epoch: 5 step: 1265, loss is 7.363707118202001e-05\n",
      "epoch: 5 step: 1266, loss is 0.005014075897634029\n",
      "epoch: 5 step: 1267, loss is 0.0029960693791508675\n",
      "epoch: 5 step: 1268, loss is 0.00014469402958638966\n",
      "epoch: 5 step: 1269, loss is 0.034055937081575394\n",
      "epoch: 5 step: 1270, loss is 0.007621433585882187\n",
      "epoch: 5 step: 1271, loss is 0.0031397377606481314\n",
      "epoch: 5 step: 1272, loss is 0.0006943790940567851\n",
      "epoch: 5 step: 1273, loss is 0.003581590950489044\n",
      "epoch: 5 step: 1274, loss is 0.0010312189115211368\n",
      "epoch: 5 step: 1275, loss is 0.004724242724478245\n",
      "epoch: 5 step: 1276, loss is 0.00035665996256284416\n",
      "epoch: 5 step: 1277, loss is 0.0006344769499264657\n",
      "epoch: 5 step: 1278, loss is 0.22894731163978577\n",
      "epoch: 5 step: 1279, loss is 0.0171198807656765\n",
      "epoch: 5 step: 1280, loss is 0.006200488191097975\n",
      "epoch: 5 step: 1281, loss is 0.0027333428151905537\n",
      "epoch: 5 step: 1282, loss is 0.0009013620438054204\n",
      "epoch: 5 step: 1283, loss is 0.0035571029875427485\n",
      "epoch: 5 step: 1284, loss is 0.00781251396983862\n",
      "epoch: 5 step: 1285, loss is 0.0708431601524353\n",
      "epoch: 5 step: 1286, loss is 0.1317567527294159\n",
      "epoch: 5 step: 1287, loss is 0.00023591617355123162\n",
      "epoch: 5 step: 1288, loss is 0.013948574662208557\n",
      "epoch: 5 step: 1289, loss is 0.05286000669002533\n",
      "epoch: 5 step: 1290, loss is 0.12674686312675476\n",
      "epoch: 5 step: 1291, loss is 0.0033282602671533823\n",
      "epoch: 5 step: 1292, loss is 0.0008637890568934381\n",
      "epoch: 5 step: 1293, loss is 0.0006860615103505552\n",
      "epoch: 5 step: 1294, loss is 0.0005768825649283826\n",
      "epoch: 5 step: 1295, loss is 0.05376100540161133\n",
      "epoch: 5 step: 1296, loss is 0.021300507709383965\n",
      "epoch: 5 step: 1297, loss is 0.011381255462765694\n",
      "epoch: 5 step: 1298, loss is 0.05439964309334755\n",
      "epoch: 5 step: 1299, loss is 0.021883249282836914\n",
      "epoch: 5 step: 1300, loss is 0.0367777980864048\n",
      "epoch: 5 step: 1301, loss is 0.039592478424310684\n",
      "epoch: 5 step: 1302, loss is 0.006313326768577099\n",
      "epoch: 5 step: 1303, loss is 0.011570687405765057\n",
      "epoch: 5 step: 1304, loss is 0.0012969074305146933\n",
      "epoch: 5 step: 1305, loss is 0.020689811557531357\n",
      "epoch: 5 step: 1306, loss is 0.0018193612340837717\n",
      "epoch: 5 step: 1307, loss is 0.19498589634895325\n",
      "epoch: 5 step: 1308, loss is 0.052678998559713364\n",
      "epoch: 5 step: 1309, loss is 0.02413857728242874\n",
      "epoch: 5 step: 1310, loss is 0.000546452181879431\n",
      "epoch: 5 step: 1311, loss is 0.01322136726230383\n",
      "epoch: 5 step: 1312, loss is 0.004224209114909172\n",
      "epoch: 5 step: 1313, loss is 0.0030320854857563972\n",
      "epoch: 5 step: 1314, loss is 0.05908900126814842\n",
      "epoch: 5 step: 1315, loss is 0.0066637517884373665\n",
      "epoch: 5 step: 1316, loss is 0.011934099718928337\n",
      "epoch: 5 step: 1317, loss is 0.06687972694635391\n",
      "epoch: 5 step: 1318, loss is 0.0007071549189276993\n",
      "epoch: 5 step: 1319, loss is 0.029881224036216736\n",
      "epoch: 5 step: 1320, loss is 0.015534129925072193\n",
      "epoch: 5 step: 1321, loss is 0.04751946032047272\n",
      "epoch: 5 step: 1322, loss is 0.009444027207791805\n",
      "epoch: 5 step: 1323, loss is 0.038865625858306885\n",
      "epoch: 5 step: 1324, loss is 0.03846414387226105\n",
      "epoch: 5 step: 1325, loss is 0.007045970298349857\n",
      "epoch: 5 step: 1326, loss is 0.003331048646941781\n",
      "epoch: 5 step: 1327, loss is 0.13361403346061707\n",
      "epoch: 5 step: 1328, loss is 0.032476846128702164\n",
      "epoch: 5 step: 1329, loss is 0.0017671332461759448\n",
      "epoch: 5 step: 1330, loss is 0.009471137076616287\n",
      "epoch: 5 step: 1331, loss is 0.06629882007837296\n",
      "epoch: 5 step: 1332, loss is 0.005521110258996487\n",
      "epoch: 5 step: 1333, loss is 0.02640053816139698\n",
      "epoch: 5 step: 1334, loss is 0.0006492245011031628\n",
      "epoch: 5 step: 1335, loss is 0.0007133965264074504\n",
      "epoch: 5 step: 1336, loss is 0.05535248667001724\n",
      "epoch: 5 step: 1337, loss is 0.001963777467608452\n",
      "epoch: 5 step: 1338, loss is 0.004009916912764311\n",
      "epoch: 5 step: 1339, loss is 0.00041058287024497986\n",
      "epoch: 5 step: 1340, loss is 0.00018834912043530494\n",
      "epoch: 5 step: 1341, loss is 0.04808274284005165\n",
      "epoch: 5 step: 1342, loss is 0.0318610742688179\n",
      "epoch: 5 step: 1343, loss is 0.00143486971501261\n",
      "epoch: 5 step: 1344, loss is 0.018033016473054886\n",
      "epoch: 5 step: 1345, loss is 0.0071776811964809895\n",
      "epoch: 5 step: 1346, loss is 0.026151329278945923\n",
      "epoch: 5 step: 1347, loss is 0.07279863953590393\n",
      "epoch: 5 step: 1348, loss is 0.003300704061985016\n",
      "epoch: 5 step: 1349, loss is 0.013086434453725815\n",
      "epoch: 5 step: 1350, loss is 0.0038282338064163923\n",
      "epoch: 5 step: 1351, loss is 0.00022914358123671263\n",
      "epoch: 5 step: 1352, loss is 0.0012257039779797196\n",
      "epoch: 5 step: 1353, loss is 0.000890627212356776\n",
      "epoch: 5 step: 1354, loss is 0.007756833452731371\n",
      "epoch: 5 step: 1355, loss is 0.019753195345401764\n",
      "epoch: 5 step: 1356, loss is 0.007556756492704153\n",
      "epoch: 5 step: 1357, loss is 0.06763097643852234\n",
      "epoch: 5 step: 1358, loss is 0.009358626790344715\n",
      "epoch: 5 step: 1359, loss is 0.006798375863581896\n",
      "epoch: 5 step: 1360, loss is 0.034071654081344604\n",
      "epoch: 5 step: 1361, loss is 0.004904283210635185\n",
      "epoch: 5 step: 1362, loss is 0.005121742375195026\n",
      "epoch: 5 step: 1363, loss is 0.003794444026425481\n",
      "epoch: 5 step: 1364, loss is 0.007593067362904549\n",
      "epoch: 5 step: 1365, loss is 0.049893852323293686\n",
      "epoch: 5 step: 1366, loss is 0.002509426325559616\n",
      "epoch: 5 step: 1367, loss is 0.0001818148884922266\n",
      "epoch: 5 step: 1368, loss is 0.06257115304470062\n",
      "epoch: 5 step: 1369, loss is 0.1255168318748474\n",
      "epoch: 5 step: 1370, loss is 0.002043486339971423\n",
      "epoch: 5 step: 1371, loss is 0.0005091571947559714\n",
      "epoch: 5 step: 1372, loss is 0.0008739662007428706\n",
      "epoch: 5 step: 1373, loss is 0.21214106678962708\n",
      "epoch: 5 step: 1374, loss is 0.12915745377540588\n",
      "epoch: 5 step: 1375, loss is 0.006888903211802244\n",
      "epoch: 5 step: 1376, loss is 0.011490130797028542\n",
      "epoch: 5 step: 1377, loss is 0.03799119219183922\n",
      "epoch: 5 step: 1378, loss is 0.000522946531418711\n",
      "epoch: 5 step: 1379, loss is 0.007802389562129974\n",
      "epoch: 5 step: 1380, loss is 0.019619585946202278\n",
      "epoch: 5 step: 1381, loss is 0.0005807342822663486\n",
      "epoch: 5 step: 1382, loss is 0.0006666880799457431\n",
      "epoch: 5 step: 1383, loss is 0.014659607782959938\n",
      "epoch: 5 step: 1384, loss is 0.004257092718034983\n",
      "epoch: 5 step: 1385, loss is 0.0383111871778965\n",
      "epoch: 5 step: 1386, loss is 0.022655682638287544\n",
      "epoch: 5 step: 1387, loss is 0.020729640498757362\n",
      "epoch: 5 step: 1388, loss is 0.000165976831340231\n",
      "epoch: 5 step: 1389, loss is 0.0025379983708262444\n",
      "epoch: 5 step: 1390, loss is 0.010164324194192886\n",
      "epoch: 5 step: 1391, loss is 0.008088196627795696\n",
      "epoch: 5 step: 1392, loss is 0.010021750815212727\n",
      "epoch: 5 step: 1393, loss is 0.061032749712467194\n",
      "epoch: 5 step: 1394, loss is 0.0025387920904904604\n",
      "epoch: 5 step: 1395, loss is 0.00021542012109421194\n",
      "epoch: 5 step: 1396, loss is 0.0006168727995827794\n",
      "epoch: 5 step: 1397, loss is 0.0010033275466412306\n",
      "epoch: 5 step: 1398, loss is 0.03606807813048363\n",
      "epoch: 5 step: 1399, loss is 0.06783686578273773\n",
      "epoch: 5 step: 1400, loss is 0.005085384007543325\n",
      "epoch: 5 step: 1401, loss is 0.00038076849887147546\n",
      "epoch: 5 step: 1402, loss is 0.0014485114952549338\n",
      "epoch: 5 step: 1403, loss is 0.062029480934143066\n",
      "epoch: 5 step: 1404, loss is 0.004425158258527517\n",
      "epoch: 5 step: 1405, loss is 0.13061246275901794\n",
      "epoch: 5 step: 1406, loss is 0.0007977689383551478\n",
      "epoch: 5 step: 1407, loss is 0.03626277670264244\n",
      "epoch: 5 step: 1408, loss is 0.0001546492421766743\n",
      "epoch: 5 step: 1409, loss is 0.016479700803756714\n",
      "epoch: 5 step: 1410, loss is 0.006108864210546017\n",
      "epoch: 5 step: 1411, loss is 0.0026062368415296078\n",
      "epoch: 5 step: 1412, loss is 0.2256849855184555\n",
      "epoch: 5 step: 1413, loss is 0.004070783033967018\n",
      "epoch: 5 step: 1414, loss is 0.0993761196732521\n",
      "epoch: 5 step: 1415, loss is 0.10689493268728256\n",
      "epoch: 5 step: 1416, loss is 0.007846668362617493\n",
      "epoch: 5 step: 1417, loss is 0.017149174585938454\n",
      "epoch: 5 step: 1418, loss is 0.012431023642420769\n",
      "epoch: 5 step: 1419, loss is 0.0020075382199138403\n",
      "epoch: 5 step: 1420, loss is 0.02434692345559597\n",
      "epoch: 5 step: 1421, loss is 0.0022520634811371565\n",
      "epoch: 5 step: 1422, loss is 0.009018604643642902\n",
      "epoch: 5 step: 1423, loss is 0.1411711722612381\n",
      "epoch: 5 step: 1424, loss is 0.017786798998713493\n",
      "epoch: 5 step: 1425, loss is 0.004042401444166899\n",
      "epoch: 5 step: 1426, loss is 0.01946365460753441\n",
      "epoch: 5 step: 1427, loss is 0.013938128016889095\n",
      "epoch: 5 step: 1428, loss is 0.013250756077468395\n",
      "epoch: 5 step: 1429, loss is 0.002067066263407469\n",
      "epoch: 5 step: 1430, loss is 0.04242177680134773\n",
      "epoch: 5 step: 1431, loss is 0.0037769805639982224\n",
      "epoch: 5 step: 1432, loss is 0.022095467895269394\n",
      "epoch: 5 step: 1433, loss is 0.0817219465970993\n",
      "epoch: 5 step: 1434, loss is 0.00228464906103909\n",
      "epoch: 5 step: 1435, loss is 0.0011924224672839046\n",
      "epoch: 5 step: 1436, loss is 0.06798835843801498\n",
      "epoch: 5 step: 1437, loss is 0.29999327659606934\n",
      "epoch: 5 step: 1438, loss is 0.012274475768208504\n",
      "epoch: 5 step: 1439, loss is 0.0005335165187716484\n",
      "epoch: 5 step: 1440, loss is 0.0017677054274827242\n",
      "epoch: 5 step: 1441, loss is 0.017746511846780777\n",
      "epoch: 5 step: 1442, loss is 0.23437659442424774\n",
      "epoch: 5 step: 1443, loss is 0.017634185031056404\n",
      "epoch: 5 step: 1444, loss is 0.018510358408093452\n",
      "epoch: 5 step: 1445, loss is 0.0988767147064209\n",
      "epoch: 5 step: 1446, loss is 0.05148569121956825\n",
      "epoch: 5 step: 1447, loss is 0.00284414179623127\n",
      "epoch: 5 step: 1448, loss is 0.000928916153497994\n",
      "epoch: 5 step: 1449, loss is 0.0422465056180954\n",
      "epoch: 5 step: 1450, loss is 0.007781138643622398\n",
      "epoch: 5 step: 1451, loss is 0.036930475383996964\n",
      "epoch: 5 step: 1452, loss is 0.024486612528562546\n",
      "epoch: 5 step: 1453, loss is 0.0003680700610857457\n",
      "epoch: 5 step: 1454, loss is 0.030938293784856796\n",
      "epoch: 5 step: 1455, loss is 0.08725259453058243\n",
      "epoch: 5 step: 1456, loss is 0.0013455887092277408\n",
      "epoch: 5 step: 1457, loss is 0.3709222376346588\n",
      "epoch: 5 step: 1458, loss is 0.004136722069233656\n",
      "epoch: 5 step: 1459, loss is 0.008801373653113842\n",
      "epoch: 5 step: 1460, loss is 0.0005078596295788884\n",
      "epoch: 5 step: 1461, loss is 0.0024315621703863144\n",
      "epoch: 5 step: 1462, loss is 0.0007026695529930294\n",
      "epoch: 5 step: 1463, loss is 0.128198504447937\n",
      "epoch: 5 step: 1464, loss is 0.0036112971138209105\n",
      "epoch: 5 step: 1465, loss is 0.08524361252784729\n",
      "epoch: 5 step: 1466, loss is 0.03443262353539467\n",
      "epoch: 5 step: 1467, loss is 0.0036925780586898327\n",
      "epoch: 5 step: 1468, loss is 0.09965507686138153\n",
      "epoch: 5 step: 1469, loss is 0.039123695343732834\n",
      "epoch: 5 step: 1470, loss is 0.0007361772586591542\n",
      "epoch: 5 step: 1471, loss is 0.0004762726020999253\n",
      "epoch: 5 step: 1472, loss is 0.005639970302581787\n",
      "epoch: 5 step: 1473, loss is 0.003501532133668661\n",
      "epoch: 5 step: 1474, loss is 0.002843304071575403\n",
      "epoch: 5 step: 1475, loss is 0.0029931822791695595\n",
      "epoch: 5 step: 1476, loss is 0.05416558310389519\n",
      "epoch: 5 step: 1477, loss is 0.0012275996850803494\n",
      "epoch: 5 step: 1478, loss is 0.11144281923770905\n",
      "epoch: 5 step: 1479, loss is 0.007847122848033905\n",
      "epoch: 5 step: 1480, loss is 0.27848631143569946\n",
      "epoch: 5 step: 1481, loss is 0.06567727774381638\n",
      "epoch: 5 step: 1482, loss is 0.002400466473773122\n",
      "epoch: 5 step: 1483, loss is 0.47679153084754944\n",
      "epoch: 5 step: 1484, loss is 0.0037907555233687162\n",
      "epoch: 5 step: 1485, loss is 0.0014844411052763462\n",
      "epoch: 5 step: 1486, loss is 0.0052382079884409904\n",
      "epoch: 5 step: 1487, loss is 0.0023763151839375496\n",
      "epoch: 5 step: 1488, loss is 0.27156710624694824\n",
      "epoch: 5 step: 1489, loss is 0.0025642947293817997\n",
      "epoch: 5 step: 1490, loss is 0.15641359984874725\n",
      "epoch: 5 step: 1491, loss is 0.000607712019700557\n",
      "epoch: 5 step: 1492, loss is 0.03764285519719124\n",
      "epoch: 5 step: 1493, loss is 0.16695891320705414\n",
      "epoch: 5 step: 1494, loss is 0.007970835082232952\n",
      "epoch: 5 step: 1495, loss is 0.009455260820686817\n",
      "epoch: 5 step: 1496, loss is 0.002579575404524803\n",
      "epoch: 5 step: 1497, loss is 0.02289748564362526\n",
      "epoch: 5 step: 1498, loss is 0.03872320055961609\n",
      "epoch: 5 step: 1499, loss is 0.012674810364842415\n",
      "epoch: 5 step: 1500, loss is 0.0064991554245352745\n",
      "epoch: 5 step: 1501, loss is 0.07924861460924149\n",
      "epoch: 5 step: 1502, loss is 0.097293920814991\n",
      "epoch: 5 step: 1503, loss is 0.018303288146853447\n",
      "epoch: 5 step: 1504, loss is 0.014581124298274517\n",
      "epoch: 5 step: 1505, loss is 0.0017405557446181774\n",
      "epoch: 5 step: 1506, loss is 0.044642768800258636\n",
      "epoch: 5 step: 1507, loss is 0.028902877122163773\n",
      "epoch: 5 step: 1508, loss is 0.0020983414724469185\n",
      "epoch: 5 step: 1509, loss is 0.10472004860639572\n",
      "epoch: 5 step: 1510, loss is 0.033610083162784576\n",
      "epoch: 5 step: 1511, loss is 0.01082658302038908\n",
      "epoch: 5 step: 1512, loss is 0.0010782161261886358\n",
      "epoch: 5 step: 1513, loss is 0.053889207541942596\n",
      "epoch: 5 step: 1514, loss is 0.0013027802342548966\n",
      "epoch: 5 step: 1515, loss is 0.0015136353904381394\n",
      "epoch: 5 step: 1516, loss is 0.006018802057951689\n",
      "epoch: 5 step: 1517, loss is 0.0004279797722119838\n",
      "epoch: 5 step: 1518, loss is 0.0031000066082924604\n",
      "epoch: 5 step: 1519, loss is 0.004544433671981096\n",
      "epoch: 5 step: 1520, loss is 0.024521997198462486\n",
      "epoch: 5 step: 1521, loss is 0.04351077973842621\n",
      "epoch: 5 step: 1522, loss is 0.022633347660303116\n",
      "epoch: 5 step: 1523, loss is 0.0009034837130457163\n",
      "epoch: 5 step: 1524, loss is 0.002013800898566842\n",
      "epoch: 5 step: 1525, loss is 0.00040191461448557675\n",
      "epoch: 5 step: 1526, loss is 0.011937483213841915\n",
      "epoch: 5 step: 1527, loss is 0.0013886825181543827\n",
      "epoch: 5 step: 1528, loss is 0.010092027485370636\n",
      "epoch: 5 step: 1529, loss is 0.035916998982429504\n",
      "epoch: 5 step: 1530, loss is 0.0005271531408652663\n",
      "epoch: 5 step: 1531, loss is 0.10432839393615723\n",
      "epoch: 5 step: 1532, loss is 0.05576959252357483\n",
      "epoch: 5 step: 1533, loss is 0.0010263302829116583\n",
      "epoch: 5 step: 1534, loss is 0.0017772158607840538\n",
      "epoch: 5 step: 1535, loss is 0.0001333679392701015\n",
      "epoch: 5 step: 1536, loss is 0.0040706489235162735\n",
      "epoch: 5 step: 1537, loss is 0.08440834283828735\n",
      "epoch: 5 step: 1538, loss is 0.0014417143538594246\n",
      "epoch: 5 step: 1539, loss is 0.01001201756298542\n",
      "epoch: 5 step: 1540, loss is 0.0058478740975260735\n",
      "epoch: 5 step: 1541, loss is 0.26066216826438904\n",
      "epoch: 5 step: 1542, loss is 0.006186125334352255\n",
      "epoch: 5 step: 1543, loss is 0.015120700933039188\n",
      "epoch: 5 step: 1544, loss is 0.0005858591757714748\n",
      "epoch: 5 step: 1545, loss is 0.0002335730823688209\n",
      "epoch: 5 step: 1546, loss is 0.012607027776539326\n",
      "epoch: 5 step: 1547, loss is 0.0032309498637914658\n",
      "epoch: 5 step: 1548, loss is 0.011919832788407803\n",
      "epoch: 5 step: 1549, loss is 0.15753602981567383\n",
      "epoch: 5 step: 1550, loss is 0.2727271616458893\n",
      "epoch: 5 step: 1551, loss is 0.016871608793735504\n",
      "epoch: 5 step: 1552, loss is 0.014323404990136623\n",
      "epoch: 5 step: 1553, loss is 0.00037223243271000683\n",
      "epoch: 5 step: 1554, loss is 0.0007755683618597686\n",
      "epoch: 5 step: 1555, loss is 0.0022900125477463007\n",
      "epoch: 5 step: 1556, loss is 0.011127522215247154\n",
      "epoch: 5 step: 1557, loss is 0.19121675193309784\n",
      "epoch: 5 step: 1558, loss is 0.0008368677226826549\n",
      "epoch: 5 step: 1559, loss is 0.0009215068421326578\n",
      "epoch: 5 step: 1560, loss is 0.02082662098109722\n",
      "epoch: 5 step: 1561, loss is 0.001701836590655148\n",
      "epoch: 5 step: 1562, loss is 0.01095527783036232\n",
      "epoch: 5 step: 1563, loss is 0.00011665046622511\n",
      "epoch: 5 step: 1564, loss is 0.08817068487405777\n",
      "epoch: 5 step: 1565, loss is 0.011701631359755993\n",
      "epoch: 5 step: 1566, loss is 0.024692701175808907\n",
      "epoch: 5 step: 1567, loss is 0.002273213816806674\n",
      "epoch: 5 step: 1568, loss is 0.00530541455373168\n",
      "epoch: 5 step: 1569, loss is 0.0009463525493629277\n",
      "epoch: 5 step: 1570, loss is 0.0036540457513183355\n",
      "epoch: 5 step: 1571, loss is 0.07424003630876541\n",
      "epoch: 5 step: 1572, loss is 0.0033814087510108948\n",
      "epoch: 5 step: 1573, loss is 0.13716182112693787\n",
      "epoch: 5 step: 1574, loss is 0.03879746422171593\n",
      "epoch: 5 step: 1575, loss is 0.0023136374074965715\n",
      "epoch: 5 step: 1576, loss is 0.003467354690656066\n",
      "epoch: 5 step: 1577, loss is 0.00013074022717773914\n",
      "epoch: 5 step: 1578, loss is 0.03140726685523987\n",
      "epoch: 5 step: 1579, loss is 0.01628091000020504\n",
      "epoch: 5 step: 1580, loss is 0.0008701167535036802\n",
      "epoch: 5 step: 1581, loss is 0.00925684068351984\n",
      "epoch: 5 step: 1582, loss is 0.0027803881093859673\n",
      "epoch: 5 step: 1583, loss is 0.019119523465633392\n",
      "epoch: 5 step: 1584, loss is 0.007612124551087618\n",
      "epoch: 5 step: 1585, loss is 0.002869155490770936\n",
      "epoch: 5 step: 1586, loss is 0.017033783718943596\n",
      "epoch: 5 step: 1587, loss is 0.00027442866121418774\n",
      "epoch: 5 step: 1588, loss is 0.004151583649218082\n",
      "epoch: 5 step: 1589, loss is 0.004599741194397211\n",
      "epoch: 5 step: 1590, loss is 0.0005735938320867717\n",
      "epoch: 5 step: 1591, loss is 0.03968416526913643\n",
      "epoch: 5 step: 1592, loss is 0.0006859691930003464\n",
      "epoch: 5 step: 1593, loss is 0.002192145213484764\n",
      "epoch: 5 step: 1594, loss is 0.002427448518574238\n",
      "epoch: 5 step: 1595, loss is 0.001129661570303142\n",
      "epoch: 5 step: 1596, loss is 0.0007883486687205732\n",
      "epoch: 5 step: 1597, loss is 0.035819120705127716\n",
      "epoch: 5 step: 1598, loss is 0.03278922662138939\n",
      "epoch: 5 step: 1599, loss is 0.09223747998476028\n",
      "epoch: 5 step: 1600, loss is 0.0032906560227274895\n",
      "epoch: 5 step: 1601, loss is 0.008441637270152569\n",
      "epoch: 5 step: 1602, loss is 0.005596785806119442\n",
      "epoch: 5 step: 1603, loss is 0.001941365422680974\n",
      "epoch: 5 step: 1604, loss is 0.0011250171810388565\n",
      "epoch: 5 step: 1605, loss is 0.016925334930419922\n",
      "epoch: 5 step: 1606, loss is 0.016834653913974762\n",
      "epoch: 5 step: 1607, loss is 0.03091481328010559\n",
      "epoch: 5 step: 1608, loss is 0.0009730005403980613\n",
      "epoch: 5 step: 1609, loss is 0.08783898502588272\n",
      "epoch: 5 step: 1610, loss is 0.0027583271730691195\n",
      "epoch: 5 step: 1611, loss is 0.22634069621562958\n",
      "epoch: 5 step: 1612, loss is 0.032995037734508514\n",
      "epoch: 5 step: 1613, loss is 0.0231130700558424\n",
      "epoch: 5 step: 1614, loss is 0.07441891729831696\n",
      "epoch: 5 step: 1615, loss is 0.06345406174659729\n",
      "epoch: 5 step: 1616, loss is 0.005048248451203108\n",
      "epoch: 5 step: 1617, loss is 0.01315537840127945\n",
      "epoch: 5 step: 1618, loss is 0.3970763087272644\n",
      "epoch: 5 step: 1619, loss is 0.006111365742981434\n",
      "epoch: 5 step: 1620, loss is 0.002207974437624216\n",
      "epoch: 5 step: 1621, loss is 0.0007864325889386237\n",
      "epoch: 5 step: 1622, loss is 0.0011252700351178646\n",
      "epoch: 5 step: 1623, loss is 0.017027772963047028\n",
      "epoch: 5 step: 1624, loss is 0.00743475928902626\n",
      "epoch: 5 step: 1625, loss is 0.000326995097566396\n",
      "epoch: 5 step: 1626, loss is 0.005799977108836174\n",
      "epoch: 5 step: 1627, loss is 0.024535875767469406\n",
      "epoch: 5 step: 1628, loss is 0.00783234741538763\n",
      "epoch: 5 step: 1629, loss is 0.013561168685555458\n",
      "epoch: 5 step: 1630, loss is 0.003069032449275255\n",
      "epoch: 5 step: 1631, loss is 0.003596640657633543\n",
      "epoch: 5 step: 1632, loss is 0.0011859424412250519\n",
      "epoch: 5 step: 1633, loss is 0.051432330161333084\n",
      "epoch: 5 step: 1634, loss is 0.011902478523552418\n",
      "epoch: 5 step: 1635, loss is 0.0022609822917729616\n",
      "epoch: 5 step: 1636, loss is 0.004983444698154926\n",
      "epoch: 5 step: 1637, loss is 0.01752932369709015\n",
      "epoch: 5 step: 1638, loss is 0.0005217127618379891\n",
      "epoch: 5 step: 1639, loss is 0.011091851629316807\n",
      "epoch: 5 step: 1640, loss is 0.0013329781359061599\n",
      "epoch: 5 step: 1641, loss is 0.014849470928311348\n",
      "epoch: 5 step: 1642, loss is 0.0022175859194248915\n",
      "epoch: 5 step: 1643, loss is 0.010493995621800423\n",
      "epoch: 5 step: 1644, loss is 0.004091939888894558\n",
      "epoch: 5 step: 1645, loss is 0.0015225601382553577\n",
      "epoch: 5 step: 1646, loss is 0.05323091521859169\n",
      "epoch: 5 step: 1647, loss is 0.08177537471055984\n",
      "epoch: 5 step: 1648, loss is 0.02493794821202755\n",
      "epoch: 5 step: 1649, loss is 0.015880286693572998\n",
      "epoch: 5 step: 1650, loss is 0.002763941651210189\n",
      "epoch: 5 step: 1651, loss is 0.00766346650198102\n",
      "epoch: 5 step: 1652, loss is 0.002168289152905345\n",
      "epoch: 5 step: 1653, loss is 0.017812248319387436\n",
      "epoch: 5 step: 1654, loss is 0.00034442864125594497\n",
      "epoch: 5 step: 1655, loss is 0.007172607351094484\n",
      "epoch: 5 step: 1656, loss is 0.021729983389377594\n",
      "epoch: 5 step: 1657, loss is 0.20038804411888123\n",
      "epoch: 5 step: 1658, loss is 0.0009122827905230224\n",
      "epoch: 5 step: 1659, loss is 0.04009070619940758\n",
      "epoch: 5 step: 1660, loss is 0.0038108304142951965\n",
      "epoch: 5 step: 1661, loss is 0.03394106775522232\n",
      "epoch: 5 step: 1662, loss is 0.005614895839244127\n",
      "epoch: 5 step: 1663, loss is 0.00027902773581445217\n",
      "epoch: 5 step: 1664, loss is 0.0026633290108293295\n",
      "epoch: 5 step: 1665, loss is 0.0028800570871680975\n",
      "epoch: 5 step: 1666, loss is 0.0007740188157185912\n",
      "epoch: 5 step: 1667, loss is 0.030312739312648773\n",
      "epoch: 5 step: 1668, loss is 0.0007058486226014793\n",
      "epoch: 5 step: 1669, loss is 0.029651103541254997\n",
      "epoch: 5 step: 1670, loss is 0.001193129806779325\n",
      "epoch: 5 step: 1671, loss is 0.0847027450799942\n",
      "epoch: 5 step: 1672, loss is 0.06407278776168823\n",
      "epoch: 5 step: 1673, loss is 0.004535830579698086\n",
      "epoch: 5 step: 1674, loss is 0.0006002396112307906\n",
      "epoch: 5 step: 1675, loss is 0.03177066519856453\n",
      "epoch: 5 step: 1676, loss is 0.037802983075380325\n",
      "epoch: 5 step: 1677, loss is 0.019714726135134697\n",
      "epoch: 5 step: 1678, loss is 0.007710948120802641\n",
      "epoch: 5 step: 1679, loss is 0.02019156515598297\n",
      "epoch: 5 step: 1680, loss is 0.0268695168197155\n",
      "epoch: 5 step: 1681, loss is 0.03869503736495972\n",
      "epoch: 5 step: 1682, loss is 0.001024593715555966\n",
      "epoch: 5 step: 1683, loss is 0.04740738123655319\n",
      "epoch: 5 step: 1684, loss is 0.005862101912498474\n",
      "epoch: 5 step: 1685, loss is 0.0600682832300663\n",
      "epoch: 5 step: 1686, loss is 0.000611413677688688\n",
      "epoch: 5 step: 1687, loss is 0.004546440672129393\n",
      "epoch: 5 step: 1688, loss is 0.0010784962214529514\n",
      "epoch: 5 step: 1689, loss is 0.018112191930413246\n",
      "epoch: 5 step: 1690, loss is 0.008066052570939064\n",
      "epoch: 5 step: 1691, loss is 0.005372692830860615\n",
      "epoch: 5 step: 1692, loss is 0.0004934541648253798\n",
      "epoch: 5 step: 1693, loss is 0.024860890582203865\n",
      "epoch: 5 step: 1694, loss is 0.007496213540434837\n",
      "epoch: 5 step: 1695, loss is 0.007518727332353592\n",
      "epoch: 5 step: 1696, loss is 0.004782855045050383\n",
      "epoch: 5 step: 1697, loss is 0.0002447803854010999\n",
      "epoch: 5 step: 1698, loss is 0.011113612912595272\n",
      "epoch: 5 step: 1699, loss is 0.0042258696630597115\n",
      "epoch: 5 step: 1700, loss is 0.07550029456615448\n",
      "epoch: 5 step: 1701, loss is 0.0007899408810772002\n",
      "epoch: 5 step: 1702, loss is 0.08207390457391739\n",
      "epoch: 5 step: 1703, loss is 0.07684268802404404\n",
      "epoch: 5 step: 1704, loss is 0.0026528476737439632\n",
      "epoch: 5 step: 1705, loss is 0.0050291321240365505\n",
      "epoch: 5 step: 1706, loss is 0.0017128672916442156\n",
      "epoch: 5 step: 1707, loss is 0.0037285692524164915\n",
      "epoch: 5 step: 1708, loss is 0.000451703614089638\n",
      "epoch: 5 step: 1709, loss is 0.004961194470524788\n",
      "epoch: 5 step: 1710, loss is 0.027125179767608643\n",
      "epoch: 5 step: 1711, loss is 0.0022950805723667145\n",
      "epoch: 5 step: 1712, loss is 0.0003695352061185986\n",
      "epoch: 5 step: 1713, loss is 0.005055451299995184\n",
      "epoch: 5 step: 1714, loss is 0.014930560253560543\n",
      "epoch: 5 step: 1715, loss is 0.0016353544779121876\n",
      "epoch: 5 step: 1716, loss is 0.12984557449817657\n",
      "epoch: 5 step: 1717, loss is 0.0035565467551350594\n",
      "epoch: 5 step: 1718, loss is 0.017808157950639725\n",
      "epoch: 5 step: 1719, loss is 0.004330505151301622\n",
      "epoch: 5 step: 1720, loss is 0.0013977212365716696\n",
      "epoch: 5 step: 1721, loss is 0.00022118110791780055\n",
      "epoch: 5 step: 1722, loss is 0.12621237337589264\n",
      "epoch: 5 step: 1723, loss is 0.0004525346157606691\n",
      "epoch: 5 step: 1724, loss is 0.0006041701417416334\n",
      "epoch: 5 step: 1725, loss is 0.003705427050590515\n",
      "epoch: 5 step: 1726, loss is 0.00015388114843517542\n",
      "epoch: 5 step: 1727, loss is 0.0133501086384058\n",
      "epoch: 5 step: 1728, loss is 0.07466090470552444\n",
      "epoch: 5 step: 1729, loss is 0.014198997989296913\n",
      "epoch: 5 step: 1730, loss is 0.004996647126972675\n",
      "epoch: 5 step: 1731, loss is 0.0005850863526575267\n",
      "epoch: 5 step: 1732, loss is 0.0017256299033761024\n",
      "epoch: 5 step: 1733, loss is 0.0003092408296652138\n",
      "epoch: 5 step: 1734, loss is 0.10486219078302383\n",
      "epoch: 5 step: 1735, loss is 0.0006348253227770329\n",
      "epoch: 5 step: 1736, loss is 0.007499197032302618\n",
      "epoch: 5 step: 1737, loss is 0.09203005582094193\n",
      "epoch: 5 step: 1738, loss is 0.0003212751180399209\n",
      "epoch: 5 step: 1739, loss is 0.01680973544716835\n",
      "epoch: 5 step: 1740, loss is 0.08961044996976852\n",
      "epoch: 5 step: 1741, loss is 0.044689178466796875\n",
      "epoch: 5 step: 1742, loss is 0.03887835890054703\n",
      "epoch: 5 step: 1743, loss is 0.0024560177698731422\n",
      "epoch: 5 step: 1744, loss is 0.00059746322222054\n",
      "epoch: 5 step: 1745, loss is 0.06405841559171677\n",
      "epoch: 5 step: 1746, loss is 0.0007515285979025066\n",
      "epoch: 5 step: 1747, loss is 0.00043116736924275756\n",
      "epoch: 5 step: 1748, loss is 0.0008721980848349631\n",
      "epoch: 5 step: 1749, loss is 0.0060712858103215694\n",
      "epoch: 5 step: 1750, loss is 0.0022210392635315657\n",
      "epoch: 5 step: 1751, loss is 0.00021448946790769696\n",
      "epoch: 5 step: 1752, loss is 0.00012343913840595633\n",
      "epoch: 5 step: 1753, loss is 0.00019130425062030554\n",
      "epoch: 5 step: 1754, loss is 0.014930788427591324\n",
      "epoch: 5 step: 1755, loss is 0.0011867652647197247\n",
      "epoch: 5 step: 1756, loss is 0.3608994781970978\n",
      "epoch: 5 step: 1757, loss is 0.02677636407315731\n",
      "epoch: 5 step: 1758, loss is 0.0005627886857837439\n",
      "epoch: 5 step: 1759, loss is 0.0006133211427368224\n",
      "epoch: 5 step: 1760, loss is 0.001130110351368785\n",
      "epoch: 5 step: 1761, loss is 0.04048360884189606\n",
      "epoch: 5 step: 1762, loss is 0.014727799221873283\n",
      "epoch: 5 step: 1763, loss is 0.053207382559776306\n",
      "epoch: 5 step: 1764, loss is 0.23990124464035034\n",
      "epoch: 5 step: 1765, loss is 0.09807905554771423\n",
      "epoch: 5 step: 1766, loss is 0.05075060576200485\n",
      "epoch: 5 step: 1767, loss is 0.24359211325645447\n",
      "epoch: 5 step: 1768, loss is 0.057909343391656876\n",
      "epoch: 5 step: 1769, loss is 0.09620944410562515\n",
      "epoch: 5 step: 1770, loss is 0.004434540867805481\n",
      "epoch: 5 step: 1771, loss is 0.04421793296933174\n",
      "epoch: 5 step: 1772, loss is 0.059739455580711365\n",
      "epoch: 5 step: 1773, loss is 0.0029216818511486053\n",
      "epoch: 5 step: 1774, loss is 0.0017051819013431668\n",
      "epoch: 5 step: 1775, loss is 0.0026028617285192013\n",
      "epoch: 5 step: 1776, loss is 0.003708271775394678\n",
      "epoch: 5 step: 1777, loss is 0.0007964144460856915\n",
      "epoch: 5 step: 1778, loss is 0.040126003324985504\n",
      "epoch: 5 step: 1779, loss is 0.01774778962135315\n",
      "epoch: 5 step: 1780, loss is 0.004671111237257719\n",
      "epoch: 5 step: 1781, loss is 0.0006842564325779676\n",
      "epoch: 5 step: 1782, loss is 0.0029208941850811243\n",
      "epoch: 5 step: 1783, loss is 0.14823086559772491\n",
      "epoch: 5 step: 1784, loss is 0.0012660212814807892\n",
      "epoch: 5 step: 1785, loss is 0.10654684901237488\n",
      "epoch: 5 step: 1786, loss is 0.18121662735939026\n",
      "epoch: 5 step: 1787, loss is 0.0008351404685527086\n",
      "epoch: 5 step: 1788, loss is 0.01813817024230957\n",
      "epoch: 5 step: 1789, loss is 0.00857981014996767\n",
      "epoch: 5 step: 1790, loss is 0.13841375708580017\n",
      "epoch: 5 step: 1791, loss is 0.030330486595630646\n",
      "epoch: 5 step: 1792, loss is 0.054710060358047485\n",
      "epoch: 5 step: 1793, loss is 0.005151271820068359\n",
      "epoch: 5 step: 1794, loss is 0.0007761062006466091\n",
      "epoch: 5 step: 1795, loss is 0.05880271643400192\n",
      "epoch: 5 step: 1796, loss is 0.0017726334044709802\n",
      "epoch: 5 step: 1797, loss is 0.026254164054989815\n",
      "epoch: 5 step: 1798, loss is 0.003753285389393568\n",
      "epoch: 5 step: 1799, loss is 0.007541054394096136\n",
      "epoch: 5 step: 1800, loss is 0.09617520868778229\n",
      "epoch: 5 step: 1801, loss is 0.004263737704604864\n",
      "epoch: 5 step: 1802, loss is 0.21398282051086426\n",
      "epoch: 5 step: 1803, loss is 0.010350153781473637\n",
      "epoch: 5 step: 1804, loss is 0.03970615938305855\n",
      "epoch: 5 step: 1805, loss is 0.06217782944440842\n",
      "epoch: 5 step: 1806, loss is 0.006803485099226236\n",
      "epoch: 5 step: 1807, loss is 0.016325293108820915\n",
      "epoch: 5 step: 1808, loss is 0.012223598547279835\n",
      "epoch: 5 step: 1809, loss is 0.1079115942120552\n",
      "epoch: 5 step: 1810, loss is 0.0008090722840279341\n",
      "epoch: 5 step: 1811, loss is 0.0044396184384822845\n",
      "epoch: 5 step: 1812, loss is 0.006340565159916878\n",
      "epoch: 5 step: 1813, loss is 0.02205643244087696\n",
      "epoch: 5 step: 1814, loss is 0.0022183023393154144\n",
      "epoch: 5 step: 1815, loss is 0.002727988176047802\n",
      "epoch: 5 step: 1816, loss is 0.029018929228186607\n",
      "epoch: 5 step: 1817, loss is 0.026837924495339394\n",
      "epoch: 5 step: 1818, loss is 0.02861309051513672\n",
      "epoch: 5 step: 1819, loss is 0.020033275708556175\n",
      "epoch: 5 step: 1820, loss is 0.026622610166668892\n",
      "epoch: 5 step: 1821, loss is 0.0011630428489297628\n",
      "epoch: 5 step: 1822, loss is 0.1481347382068634\n",
      "epoch: 5 step: 1823, loss is 0.0007910278509370983\n",
      "epoch: 5 step: 1824, loss is 0.04337015375494957\n",
      "epoch: 5 step: 1825, loss is 0.08845718204975128\n",
      "epoch: 5 step: 1826, loss is 0.005102565977722406\n",
      "epoch: 5 step: 1827, loss is 0.1353507936000824\n",
      "epoch: 5 step: 1828, loss is 0.007909273728728294\n",
      "epoch: 5 step: 1829, loss is 0.030344758182764053\n",
      "epoch: 5 step: 1830, loss is 0.01932148076593876\n",
      "epoch: 5 step: 1831, loss is 0.0035041850060224533\n",
      "epoch: 5 step: 1832, loss is 0.009952562861144543\n",
      "epoch: 5 step: 1833, loss is 0.004554265178740025\n",
      "epoch: 5 step: 1834, loss is 0.003192901611328125\n",
      "epoch: 5 step: 1835, loss is 0.02256009727716446\n",
      "epoch: 5 step: 1836, loss is 0.02451992779970169\n",
      "epoch: 5 step: 1837, loss is 0.004342119675129652\n",
      "epoch: 5 step: 1838, loss is 0.03795067220926285\n",
      "epoch: 5 step: 1839, loss is 0.04207564517855644\n",
      "epoch: 5 step: 1840, loss is 0.0017708227969706059\n",
      "epoch: 5 step: 1841, loss is 0.0016785472398623824\n",
      "epoch: 5 step: 1842, loss is 0.1197437047958374\n",
      "epoch: 5 step: 1843, loss is 0.04084290191531181\n",
      "epoch: 5 step: 1844, loss is 0.0009125550859607756\n",
      "epoch: 5 step: 1845, loss is 0.1627189815044403\n",
      "epoch: 5 step: 1846, loss is 0.0035731543321162462\n",
      "epoch: 5 step: 1847, loss is 0.022561773657798767\n",
      "epoch: 5 step: 1848, loss is 0.002769683487713337\n",
      "epoch: 5 step: 1849, loss is 0.0055884867906570435\n",
      "epoch: 5 step: 1850, loss is 0.03523383289575577\n",
      "epoch: 5 step: 1851, loss is 0.008535263128578663\n",
      "epoch: 5 step: 1852, loss is 0.004598044790327549\n",
      "epoch: 5 step: 1853, loss is 0.06287668645381927\n",
      "epoch: 5 step: 1854, loss is 0.00544300302863121\n",
      "epoch: 5 step: 1855, loss is 0.00421174569055438\n",
      "epoch: 5 step: 1856, loss is 0.007763595320284367\n",
      "epoch: 5 step: 1857, loss is 0.007265124469995499\n",
      "epoch: 5 step: 1858, loss is 0.00860612839460373\n",
      "epoch: 5 step: 1859, loss is 0.0004027006507385522\n",
      "epoch: 5 step: 1860, loss is 0.004886607639491558\n",
      "epoch: 5 step: 1861, loss is 0.039244361221790314\n",
      "epoch: 5 step: 1862, loss is 0.0063042668625712395\n",
      "epoch: 5 step: 1863, loss is 0.0028566750697791576\n",
      "epoch: 5 step: 1864, loss is 0.039841726422309875\n",
      "epoch: 5 step: 1865, loss is 0.003076434601098299\n",
      "epoch: 5 step: 1866, loss is 0.007215843535959721\n",
      "epoch: 5 step: 1867, loss is 9.61877522058785e-05\n",
      "epoch: 5 step: 1868, loss is 0.030253320932388306\n",
      "epoch: 5 step: 1869, loss is 0.0046162959188222885\n",
      "epoch: 5 step: 1870, loss is 0.0032371883280575275\n",
      "epoch: 5 step: 1871, loss is 0.0031824305187910795\n",
      "epoch: 5 step: 1872, loss is 0.0006290096207521856\n",
      "epoch: 5 step: 1873, loss is 0.0009843932930380106\n",
      "epoch: 5 step: 1874, loss is 0.0009682598756626248\n",
      "epoch: 5 step: 1875, loss is 0.0007107138517312706\n",
      "============== Starting Testing ==============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(8186:8296894272,MainProcess):2024-10-06-18:33:36.425.431 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(8186:8296894272,MainProcess):2024-10-06-18:33:36.425.734 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(8186:8296894272,MainProcess):2024-10-06-18:33:36.425.935 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(8186:8296894272,MainProcess):2024-10-06-18:33:36.426.213 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(8186:8296894272,MainProcess):2024-10-06-18:33:36.426.476 [mindspore/dataset/core/validator_helpers.py:744] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Accuracy:{'Accuracy': 0.9817708333333334} ==============\n"
     ]
    }
   ],
   "source": [
    "device_target = \"CPU\" #choices=['Ascend', 'CPU']\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=device_target)\n",
    "dataset_sink_mode = not device_target == \"CPU\"\n",
    "# download mnist dataset\n",
    "# learning rate setting\n",
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "dataset_size = 1\n",
    "mnist_path = \"./MNIST\"\n",
    "# define the loss function\n",
    "net_loss = SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')\n",
    "train_epoch = 5\n",
    "# create the network\n",
    "net = LeNet5()\n",
    "# define the optimizer\n",
    "net_opt = nn.Momentum(net.trainable_params(), lr, momentum)\n",
    "config_ck = CheckpointConfig(save_checkpoint_steps=1875, keep_checkpoint_max=10)\n",
    "# save the network model and parameters for subsequence fine-tuning\n",
    "ckpoint = ModelCheckpoint(prefix=\"checkpoint_lenet\", config=config_ck)\n",
    "# group layers into an object with training and evaluation features\n",
    "model = Model(net, net_loss, net_opt, metrics={\"Accuracy\": Accuracy()})\n",
    "\n",
    "train_net(model, train_epoch, mnist_path, dataset_size, ckpoint, dataset_sink_mode)\n",
    "test_net(net, model, mnist_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68dd2ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore.train.serialization import load_checkpoint, save_checkpoint, export\n",
    "import numpy as np\n",
    "from mindspore import Tensor\n",
    "network = LeNet5()\n",
    "load_checkpoint(\"./checkpoint_lenet-1_1875.ckpt\", network)\n",
    "input_data = np.random.uniform(0.0, 1.0, size = [1, 1, 32, 32]).astype(np.float32)\n",
    "export(network, Tensor(input_data), file_name = './MNIST', file_format = 'ONNX') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecc9224",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

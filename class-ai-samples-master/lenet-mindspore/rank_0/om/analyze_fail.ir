# ===============================================================================================
# The following shows the last analyze fail log message.
# ===============================================================================================

----------------------------------------------------
- Caught exception:
----------------------------------------------------
For 'MatMul' the input dimensions must be equal, but got 'x1_col': 1024 and 'x2_row': 784.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore/core/ops/mat_mul.cc:107 InferShape

----------------------------------------------------
- The Traceback of Net Construct Code:
----------------------------------------------------
# 0 In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:417
        if not self.sense_flag:
# 1 In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:418
            return self._no_sens_impl(*inputs)
                   ^
# 2 In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:437
        if self.return_grad:
# 3 In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:433
        loss = self.network(*inputs)
               ^
# 4 In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:121
        out = self._backbone(data)
              ^
# 5 In file /var/folders/n4/p41p3m3n5c574nhxjsj9n7tc0000gn/T/ipykernel_8186/286663031.py:25
# 6 In file /var/folders/n4/p41p3m3n5c574nhxjsj9n7tc0000gn/T/ipykernel_8186/286663031.py:31
# 7 In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:628
        if self.has_bias:
# 8 In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:629
            x = self.bias_add(x, self.bias)
            ^
# 9 In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:630
        if self.activation_flag:
# 10 In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632
        if len(x_shape) != 2:
# 11 In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:627
        x = self.matmul(x, self.weight)
            ^

# ===============================================================================================
# The following shows the IR when the function graphs evaluation fails to help locate the problem.
# You can search the last ------------------------> to the node which is evaluated failure.
# Refer to https://www.mindspore.cn/search?inputValue=analyze_fail.ir to get more instructions.
# ===============================================================================================

# IR entry: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_723
# Total subgraphs: 164

# Attrs:
training : 1

# Total params: 21
# Params:
%para1_inputs0 : <null>
%para2_inputs1 : <null>
%para3_conv1.weight : <Ref[Tensor[Float32]], (6, 1, 3, 3), ref_key=:conv1.weight>  :  has_default
%para4_conv2.weight : <Ref[Tensor[Float32]], (16, 6, 3, 3), ref_key=:conv2.weight>  :  has_default
%para5_fc1.weight : <Ref[Tensor[Float32]], (120, 784), ref_key=:fc1.weight>  :  has_default
%para6_fc1.bias : <Ref[Tensor[Float32]], (120), ref_key=:fc1.bias>  :  has_default
%para7_fc2.weight : <Ref[Tensor[Float32]], (84, 120), ref_key=:fc2.weight>  :  has_default
%para8_fc2.bias : <Ref[Tensor[Float32]], (84), ref_key=:fc2.bias>  :  has_default
%para9_fc3.weight : <Ref[Tensor[Float32]], (10, 84), ref_key=:fc3.weight>  :  has_default
%para10_fc3.bias : <Ref[Tensor[Float32]], (10), ref_key=:fc3.bias>  :  has_default
%para11_global_step : <Ref[Tensor[Int32]], (1), ref_key=:global_step>  :  has_default
%para12_moments.conv1.weight : <Ref[Tensor[Float32]], (6, 1, 3, 3), ref_key=:moments.conv1.weight>  :  has_default
%para13_moments.conv2.weight : <Ref[Tensor[Float32]], (16, 6, 3, 3), ref_key=:moments.conv2.weight>  :  has_default
%para14_moments.fc1.weight : <Ref[Tensor[Float32]], (120, 784), ref_key=:moments.fc1.weight>  :  has_default
%para15_moments.fc1.bias : <Ref[Tensor[Float32]], (120), ref_key=:moments.fc1.bias>  :  has_default
%para16_moments.fc2.weight : <Ref[Tensor[Float32]], (84, 120), ref_key=:moments.fc2.weight>  :  has_default
%para17_moments.fc2.bias : <Ref[Tensor[Float32]], (84), ref_key=:moments.fc2.bias>  :  has_default
%para18_moments.fc3.weight : <Ref[Tensor[Float32]], (10, 84), ref_key=:moments.fc3.weight>  :  has_default
%para19_moments.fc3.bias : <Ref[Tensor[Float32]], (10), ref_key=:moments.fc3.bias>  :  has_default
%para20_momentum : <Ref[Tensor[Float32]], (), ref_key=:momentum>  :  has_default
%para21_learning_rate : <Ref[Tensor[Float32]], (), ref_key=:learning_rate>  :  has_default

subgraph attr:
training : 1
subgraph instance: mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_723 : 0x3090e0018
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:416/    def construct(self, *inputs):/
subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_723(%para1_inputs0, %para2_inputs1, %para3_conv1.weight, %para4_conv2.weight, %para5_fc1.weight, %para6_fc1.bias, %para7_fc2.weight, %para8_fc2.bias, %para9_fc3.weight, %para10_fc3.bias, %para11_global_step, %para12_moments.conv1.weight, %para13_moments.conv2.weight, %para14_moments.fc1.weight, %para15_moments.fc1.bias, %para16_moments.fc2.weight, %para17_moments.fc2.bias, %para18_moments.fc3.weight, %para19_moments.fc3.bias, %para20_momentum, %para21_learning_rate) {

#------------------------> 0
  %1(CNode_744) = call @✓mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_724()
      #scope: (Default)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:417/        if not self.sense_flag:/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:417/        if not self.sense_flag:/
}
# Order:
#   1: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_723:CNode_744{[0]: ValueNode<FuncGraph> ✓mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_724}
#   2: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_723:CNode_745{[0]: ValueNode<Primitive> Return, [1]: CNode_744}


subgraph attr:
training : 1
subgraph instance: ✓mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_724 : 0x3090e0618
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:416/    def construct(self, *inputs):/
subgraph @✓mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_724 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_723]() {
  %1(CNode_746) = $(mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_723):MakeTuple(%para1_inputs0, %para2_inputs1)
      : (<Tensor[Float32], (32, 1, 32, 32)>, <Tensor[Int32], (32)>) -> (<Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 32, 32), (32))>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:416/    def construct(self, *inputs):/

#------------------------> 1
  %2(CNode_747) = UnpackCall_unpack_call(@_no_sens_impl_748, %1)
      : (<Func, NoShape>, <Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 32, 32), (32))>) -> (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
  Return(%2)
      : (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
}
# Order:
#   1: @✓mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_724:CNode_747{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.749, [1]: ValueNode<FuncGraph> _no_sens_impl_748, [2]: CNode_746}
#   2: @✓mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_724:CNode_750{[0]: ValueNode<Primitive> Return, [1]: CNode_747}


subgraph attr:
core : 1
subgraph instance: UnpackCall_725 : 0x309122c18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
subgraph @UnpackCall_725(%para22_, %para23_) {
  %1(CNode_747) = TupleGetItem(%para23_727, I64(0))
      : (<Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 32, 32), (32))>, <Int64, NoShape>) -> (<Tensor[Float32], (32, 1, 32, 32)>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
  %2(CNode_747) = TupleGetItem(%para23_727, I64(1))
      : (<Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 32, 32), (32))>, <Int64, NoShape>) -> (<Tensor[Int32], (32)>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/

#------------------------> 2
  %3(CNode_747) = %para22_726(%1, %2)
      : (<Tensor[Float32], (32, 1, 32, 32)>, <Tensor[Int32], (32)>) -> (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
  Return(%3)
      : (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
}
# Order:
#   1: @UnpackCall_725:CNode_747{[0]: param_726, [1]: CNode_747, [2]: CNode_747}
#   2: @UnpackCall_725:CNode_747{[0]: ValueNode<Primitive> Return, [1]: CNode_747}


subgraph attr:
training : 1
subgraph instance: _no_sens_impl_728 : 0x309132418
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @_no_sens_impl_728 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_723](%para24_inputs0, %para25_inputs1) {

#------------------------> 3
  %1(CNode_751) = call @✗_no_sens_impl_729()
      #scope: (Default)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:437/        if self.return_grad:/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:437/        if self.return_grad:/
}
# Order:
#   1: @_no_sens_impl_728:loss{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.752, [1]: ValueNode<FuncGraph> mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_734, [2]: CNode_753}
#   2: @_no_sens_impl_728:grads{[0]: ValueNode<UnpackGraphPrimitive> UnpackGraph, [1]: ValueNode<FuncGraph> mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_734, [2]: CNode_753}
#   3: @_no_sens_impl_728:grads{[0]: ValueNode<DoSignaturePrimitive> S_Prim_grad, [1]: grads, [2]: CNode_754}
#   4: @_no_sens_impl_728:grads{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.755, [1]: grads, [2]: CNode_753}
#   5: @_no_sens_impl_728:grads{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Identity_construct_756, [1]: grads}
#   6: @_no_sens_impl_728:CNode_757{[0]: ValueNode<FuncGraph> mindspore_nn_optim_momentum_Momentum_construct_758, [1]: grads}
#   7: @_no_sens_impl_728:loss{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Depend, [1]: loss, [2]: CNode_757}
#   8: @_no_sens_impl_728:CNode_751{[0]: ValueNode<FuncGraph> ✗_no_sens_impl_729}
#   9: @_no_sens_impl_728:CNode_759{[0]: ValueNode<Primitive> Return, [1]: CNode_751}


subgraph attr:
training : 1
subgraph instance: ✗_no_sens_impl_729 : 0x309121e18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @✗_no_sens_impl_729 parent: [subgraph @_no_sens_impl_728]() {

#------------------------> 4
  %1(CNode_760) = call @↓_no_sens_impl_730()
      #scope: (Default)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:437/        if self.return_grad:/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:437/        if self.return_grad:/
}
# Order:
#   1: @✗_no_sens_impl_729:CNode_760{[0]: ValueNode<FuncGraph> ↓_no_sens_impl_730}
#   2: @✗_no_sens_impl_729:CNode_761{[0]: ValueNode<Primitive> Return, [1]: CNode_760}


subgraph attr:
training : 1
subgraph instance: ↓_no_sens_impl_730 : 0x3090ffc18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @↓_no_sens_impl_730 parent: [subgraph @_no_sens_impl_728]() {
  %1(CNode_753) = $(_no_sens_impl_728):MakeTuple(%para24_inputs0, %para25_inputs1)
      : (<Tensor[Float32], (32, 1, 32, 32)>, <Tensor[Int32], (32)>) -> (<Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 32, 32), (32))>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/

#------------------------> 5
  %2(loss) = $(_no_sens_impl_728):UnpackCall_unpack_call(@mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_734, %1)
      : (<Func, NoShape>, <Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 32, 32), (32))>) -> (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:433/        loss = self.network(*inputs)/
  %3(grads) = $(_no_sens_impl_728):UnpackGraph(@mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_734, %1)
      : (<null>, <Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 32, 32), (32))>) -> (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %4(CNode_754) = $(_no_sens_impl_728):MakeTuple(%para3_conv1.weight, %para4_conv2.weight, %para5_fc1.weight, %para6_fc1.bias, %para7_fc2.weight, %para8_fc2.bias, %para9_fc3.weight, %para10_fc3.bias)
      : (<Ref[Tensor[Float32]], (6, 1, 3, 3)>, <Ref[Tensor[Float32]], (16, 6, 3, 3)>, <Ref[Tensor[Float32]], (120, 784)>, <Ref[Tensor[Float32]], (120)>, <Ref[Tensor[Float32]], (84, 120)>, <Ref[Tensor[Float32]], (84)>, <Ref[Tensor[Float32]], (10, 84)>, <Ref[Tensor[Float32]], (10)>) -> (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %5(grads) = $(_no_sens_impl_728):S_Prim_grad(%3, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %6(grads) = $(_no_sens_impl_728):UnpackCall_unpack_call(%5, %1)
      : (<null>, <Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 32, 32), (32))>) -> (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %7(grads) = $(_no_sens_impl_728):call @mindspore_nn_layer_basic_Identity_construct_756(%6)
      : (<null>) -> (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:435/        grads = self.grad_reducer(grads)/
  %8(CNode_757) = $(_no_sens_impl_728):call @mindspore_nn_optim_momentum_Momentum_construct_758(%7)
      : (<null>) -> (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:436/        loss = F.depend(loss, self.optimizer(grads))/
  %9(loss) = $(_no_sens_impl_728):S_Prim_Depend[side_effect_propagate: I64(1)](%2, %8)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:436/        loss = F.depend(loss, self.optimizer(grads))/
  Return(%9)
      : (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:442/        return loss/
}
# Order:
#   1: @↓_no_sens_impl_730:CNode_762{[0]: ValueNode<Primitive> Return, [1]: loss}


subgraph attr:
core : 1
subgraph instance: UnpackCall_731 : 0x101a04018
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:433/        loss = self.network(*inputs)/
subgraph @UnpackCall_731(%para26_, %para27_) {
  %1(loss) = TupleGetItem(%para27_733, I64(0))
      : (<Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 32, 32), (32))>, <Int64, NoShape>) -> (<Tensor[Float32], (32, 1, 32, 32)>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:433/        loss = self.network(*inputs)/
  %2(loss) = TupleGetItem(%para27_733, I64(1))
      : (<Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 32, 32), (32))>, <Int64, NoShape>) -> (<Tensor[Int32], (32)>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:433/        loss = self.network(*inputs)/

#------------------------> 6
  %3(loss) = %para26_732(%1, %2)
      : (<Tensor[Float32], (32, 1, 32, 32)>, <Tensor[Int32], (32)>) -> (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:433/        loss = self.network(*inputs)/
  Return(%3)
      : (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:433/        loss = self.network(*inputs)/
}
# Order:
#   1: @UnpackCall_731:loss{[0]: param_732, [1]: loss, [2]: loss}
#   2: @UnpackCall_731:loss{[0]: ValueNode<Primitive> Return, [1]: loss}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_734 : 0x309113018
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:120/    def construct(self, data, label):/
subgraph @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_734 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_723](%para28_data, %para29_label) {

#------------------------> 7
  %1(out) = call @__main___LeNet5_construct_735(%para28_data)
      : (<Tensor[Float32], (32, 1, 32, 32)>) -> (<null>)
      #scope: (Default/network-WithLossCell)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:121/        out = self._backbone(data)/
  %2(CNode_764) = call @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_763(%1, %para29_label)
      : (<null>, <Tensor[Int32], (32)>) -> (<null>)
      #scope: (Default/network-WithLossCell)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:122/        return self._loss_fn(out, label)/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:122/        return self._loss_fn(out, label)/
}
# Order:
#   1: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_734:out{[0]: ValueNode<FuncGraph> __main___LeNet5_construct_735, [1]: param_data}
#   2: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_734:CNode_764{[0]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_763, [1]: out, [2]: param_label}
#   3: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_734:CNode_765{[0]: ValueNode<Primitive> Return, [1]: CNode_764}


subgraph attr:
training : 1
subgraph instance: __main___LeNet5_construct_735 : 0x309108818
# In file /var/folders/n4/p41p3m3n5c574nhxjsj9n7tc0000gn/T/ipykernel_8186/286663031.py:17/
subgraph @__main___LeNet5_construct_735 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_723](%para30_x) {
  %1(x) = call @mindspore_nn_layer_conv_Conv2d_construct_766(%para30_x)
      : (<Tensor[Float32], (32, 1, 32, 32)>) -> (<Tensor[Float32], (32, 6, 32, 32)>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /var/folders/n4/p41p3m3n5c574nhxjsj9n7tc0000gn/T/ipykernel_8186/286663031.py:18/
  %2(x) = call @mindspore_nn_layer_activation_ReLU_construct_767(%1)
      : (<Tensor[Float32], (32, 6, 32, 32)>) -> (<Tensor[Float32], (32, 6, 32, 32)>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /var/folders/n4/p41p3m3n5c574nhxjsj9n7tc0000gn/T/ipykernel_8186/286663031.py:19/
  %3(x) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_768(%2)
      : (<Tensor[Float32], (32, 6, 32, 32)>) -> (<Tensor[Float32], (32, 6, 16, 16)>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /var/folders/n4/p41p3m3n5c574nhxjsj9n7tc0000gn/T/ipykernel_8186/286663031.py:20/
  %4(x) = call @mindspore_nn_layer_conv_Conv2d_construct_769(%3)
      : (<Tensor[Float32], (32, 6, 16, 16)>) -> (<Tensor[Float32], (32, 16, 16, 16)>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /var/folders/n4/p41p3m3n5c574nhxjsj9n7tc0000gn/T/ipykernel_8186/286663031.py:21/
  %5(x) = call @mindspore_nn_layer_activation_ReLU_construct_767(%4)
      : (<Tensor[Float32], (32, 16, 16, 16)>) -> (<Tensor[Float32], (32, 16, 16, 16)>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /var/folders/n4/p41p3m3n5c574nhxjsj9n7tc0000gn/T/ipykernel_8186/286663031.py:22/
  %6(x) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_768(%5)
      : (<Tensor[Float32], (32, 16, 16, 16)>) -> (<Tensor[Float32], (32, 16, 8, 8)>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /var/folders/n4/p41p3m3n5c574nhxjsj9n7tc0000gn/T/ipykernel_8186/286663031.py:23/
  %7(x) = call @mindspore_nn_layer_basic_Flatten_construct_770(%6)
      : (<Tensor[Float32], (32, 16, 8, 8)>) -> (<Tensor[Float32], (32, 1024)>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /var/folders/n4/p41p3m3n5c574nhxjsj9n7tc0000gn/T/ipykernel_8186/286663031.py:24/

#------------------------> 8
  %8(x) = call @mindspore_nn_layer_basic_Dense_construct_736(%7)
      : (<Tensor[Float32], (32, 1024)>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /var/folders/n4/p41p3m3n5c574nhxjsj9n7tc0000gn/T/ipykernel_8186/286663031.py:25/
  %9(x) = call @mindspore_nn_layer_activation_ReLU_construct_767(%8)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /var/folders/n4/p41p3m3n5c574nhxjsj9n7tc0000gn/T/ipykernel_8186/286663031.py:26/
  %10(x) = call @mindspore_nn_layer_basic_Dropout_construct_771(%9)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /var/folders/n4/p41p3m3n5c574nhxjsj9n7tc0000gn/T/ipykernel_8186/286663031.py:27/
  %11(x) = call @mindspore_nn_layer_basic_Dense_construct_772(%10)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /var/folders/n4/p41p3m3n5c574nhxjsj9n7tc0000gn/T/ipykernel_8186/286663031.py:28/
  %12(x) = call @mindspore_nn_layer_activation_ReLU_construct_767(%11)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /var/folders/n4/p41p3m3n5c574nhxjsj9n7tc0000gn/T/ipykernel_8186/286663031.py:29/
  %13(x) = call @mindspore_nn_layer_basic_Dropout_construct_773(%12)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /var/folders/n4/p41p3m3n5c574nhxjsj9n7tc0000gn/T/ipykernel_8186/286663031.py:30/
  %14(x) = call @mindspore_nn_layer_basic_Dense_construct_774(%13)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /var/folders/n4/p41p3m3n5c574nhxjsj9n7tc0000gn/T/ipykernel_8186/286663031.py:31/
  %15(x) = call @mindspore_nn_layer_basic_Dropout_construct_775(%14)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /var/folders/n4/p41p3m3n5c574nhxjsj9n7tc0000gn/T/ipykernel_8186/286663031.py:32/
  Return(%15)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /var/folders/n4/p41p3m3n5c574nhxjsj9n7tc0000gn/T/ipykernel_8186/286663031.py:33/
}
# Order:
#   1: @__main___LeNet5_construct_735:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_766, [1]: param_x}
#   2: @__main___LeNet5_construct_735:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_767, [1]: x}
#   3: @__main___LeNet5_construct_735:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_768, [1]: x}
#   4: @__main___LeNet5_construct_735:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_769, [1]: x}
#   5: @__main___LeNet5_construct_735:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_767, [1]: x}
#   6: @__main___LeNet5_construct_735:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_768, [1]: x}
#   7: @__main___LeNet5_construct_735:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Flatten_construct_770, [1]: x}
#   8: @__main___LeNet5_construct_735:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dense_construct_736, [1]: x}
#   9: @__main___LeNet5_construct_735:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_767, [1]: x}
#  10: @__main___LeNet5_construct_735:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_771, [1]: x}
#  11: @__main___LeNet5_construct_735:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dense_construct_772, [1]: x}
#  12: @__main___LeNet5_construct_735:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_767, [1]: x}
#  13: @__main___LeNet5_construct_735:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_773, [1]: x}
#  14: @__main___LeNet5_construct_735:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dense_construct_774, [1]: x}
#  15: @__main___LeNet5_construct_735:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_775, [1]: x}
#  16: @__main___LeNet5_construct_735:CNode_776{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dense_construct_736 : 0x309165018
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dense_construct_736 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_723](%para31_x) {

#------------------------> 9
  %1(CNode_777) = call @L_mindspore_nn_layer_basic_Dense_construct_737(%para31_x, %para6_fc1.bias, %para5_fc1.weight)
      : (<Tensor[Float32], (32, 1024)>, <Ref[Tensor[Float32]], (120)>, <Ref[Tensor[Float32]], (120, 784)>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc1-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dense_construct_736:CNode_777{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_737, [1]: param_x, [2]: param_fc1.bias, [3]: param_fc1.weight}
#   2: @mindspore_nn_layer_basic_Dense_construct_736:CNode_778{[0]: ValueNode<Primitive> Return, [1]: CNode_777}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_737 : 0x309165618
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_737(%para32_x, %para33_, %para34_) {
  %1(x_shape) = S_Prim_Shape(%para32_x)
      : (<Tensor[Float32], (32, 1024)>) -> (<Tuple[Int64*2], TupleShape(NoShape, NoShape)>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:623/        x_shape = self.shape_op(x)/
  %2(CNode_779) = S_Prim_check_dense_input_shape[constexpr_prim: Bool(1)](%1, "Dense")
      : (<Tuple[Int64*2], TupleShape(NoShape, NoShape)>, <String, NoShape>) -> (<None, NoShape>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:624/        check_dense_input_shape(x_shape, self.cls_name)/
  %3(CNode_780) = StopGradient(%2)
      : (<None, NoShape>) -> (<None, NoShape>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
  %4(CNode_781) = S_Prim_inner_len(%1)
      : (<Tuple[Int64*2], TupleShape(NoShape, NoShape)>) -> (<Int64, NoShape>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
  %5(CNode_782) = S_Prim_not_equal(%4, I64(2))
      : (<Int64, NoShape>, <Int64, NoShape>) -> (<Bool, NoShape>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
  %6(CNode_783) = Cond(%5, Bool(0))
      : (<Bool, NoShape>, <Bool, NoShape>) -> (<Bool, NoShape>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
  %7(CNode_784) = Switch(%6, @L_✓mindspore_nn_layer_basic_Dense_construct_785, @L_✗mindspore_nn_layer_basic_Dense_construct_786)
      : (<Bool, NoShape>, <Func, NoShape>, <Func, NoShape>) -> (<Func, NoShape>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
  %8(CNode_787) = %7()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/

#------------------------> 10
  %9(CNode_788) = call @L_↓mindspore_nn_layer_basic_Dense_construct_738(%8)
      : (<Tensor[Float32], (32, 1024)>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /var/folders/n4/p41p3m3n5c574nhxjsj9n7tc0000gn/T/ipykernel_8186/286663031.py:31/
  %10(CNode_789) = Depend[side_effect_propagate: I64(1)](%9, %3)
      : (<null>, <None, NoShape>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /var/folders/n4/p41p3m3n5c574nhxjsj9n7tc0000gn/T/ipykernel_8186/286663031.py:31/
  Return(%10)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_737:x_shape{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_x}
#   2: @L_mindspore_nn_layer_basic_Dense_construct_737:CNode_779{[0]: ValueNode<DoSignaturePrimitive> S_Prim_check_dense_input_shape, [1]: x_shape, [2]: ValueNode<StringImm> Dense}
#   3: @L_mindspore_nn_layer_basic_Dense_construct_737:CNode_781{[0]: ValueNode<DoSignaturePrimitive> S_Prim_inner_len, [1]: x_shape}
#   4: @L_mindspore_nn_layer_basic_Dense_construct_737:CNode_782{[0]: ValueNode<DoSignaturePrimitive> S_Prim_not_equal, [1]: CNode_781, [2]: ValueNode<Int64Imm> 2}
#   5: @L_mindspore_nn_layer_basic_Dense_construct_737:CNode_783{[0]: ValueNode<Primitive> Cond, [1]: CNode_782, [2]: ValueNode<BoolImm> false}
#   6: @L_mindspore_nn_layer_basic_Dense_construct_737:CNode_784{[0]: ValueNode<Primitive> Switch, [1]: CNode_783, [2]: ValueNode<FuncGraph> L_✓mindspore_nn_layer_basic_Dense_construct_785, [3]: ValueNode<FuncGraph> L_✗mindspore_nn_layer_basic_Dense_construct_786}
#   7: @L_mindspore_nn_layer_basic_Dense_construct_737:CNode_787{[0]: CNode_784}
#   8: @L_mindspore_nn_layer_basic_Dense_construct_737:CNode_788{[0]: ValueNode<FuncGraph> L_↓mindspore_nn_layer_basic_Dense_construct_738, [1]: CNode_787}
#   9: @L_mindspore_nn_layer_basic_Dense_construct_737:CNode_789{[0]: ValueNode<Primitive> Depend, [1]: CNode_788, [2]: CNode_780}
#  10: @L_mindspore_nn_layer_basic_Dense_construct_737:CNode_790{[0]: ValueNode<Primitive> Return, [1]: CNode_789}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_↓mindspore_nn_layer_basic_Dense_construct_738 : 0x309178618
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_↓mindspore_nn_layer_basic_Dense_construct_738 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_737](%para35_) {

#------------------------> 11
  %1(CNode_791) = call @L_✓↓mindspore_nn_layer_basic_Dense_construct_739()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:628/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:628/        if self.has_bias:/
}
# Order:
#   1: @L_↓mindspore_nn_layer_basic_Dense_construct_738:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MatMul, [1]: param_фx, [2]: param_L_fc3.weight}
#   2: @L_↓mindspore_nn_layer_basic_Dense_construct_738:CNode_791{[0]: ValueNode<FuncGraph> L_✓↓mindspore_nn_layer_basic_Dense_construct_739}
#   3: @L_↓mindspore_nn_layer_basic_Dense_construct_738:CNode_792{[0]: ValueNode<Primitive> Return, [1]: CNode_791}


subgraph attr:
training : 1
subgraph instance: L_✓↓mindspore_nn_layer_basic_Dense_construct_739 : 0x309178c18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_✓↓mindspore_nn_layer_basic_Dense_construct_739 parent: [subgraph @L_↓mindspore_nn_layer_basic_Dense_construct_738]() {

#------------------------> 12
  %1(CNode_793) = call @L_2↓mindspore_nn_layer_basic_Dense_construct_740()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:629/            x = self.bias_add(x, self.bias)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:629/            x = self.bias_add(x, self.bias)/
}
# Order:
#   1: @L_✓↓mindspore_nn_layer_basic_Dense_construct_739:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BiasAdd, [1]: x, [2]: param_L_fc3.bias}
#   2: @L_✓↓mindspore_nn_layer_basic_Dense_construct_739:CNode_793{[0]: ValueNode<FuncGraph> L_2↓mindspore_nn_layer_basic_Dense_construct_740}
#   3: @L_✓↓mindspore_nn_layer_basic_Dense_construct_739:CNode_794{[0]: ValueNode<Primitive> Return, [1]: CNode_793}


subgraph attr:
training : 1
subgraph instance: L_2↓mindspore_nn_layer_basic_Dense_construct_740 : 0x309179218
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_2↓mindspore_nn_layer_basic_Dense_construct_740 parent: [subgraph @L_✓↓mindspore_nn_layer_basic_Dense_construct_739]() {

#------------------------> 13
  %1(CNode_795) = call @L_✗2↓mindspore_nn_layer_basic_Dense_construct_741()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:630/        if self.activation_flag:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:630/        if self.activation_flag:/
}
# Order:
#   1: @L_2↓mindspore_nn_layer_basic_Dense_construct_740:CNode_795{[0]: ValueNode<FuncGraph> L_✗2↓mindspore_nn_layer_basic_Dense_construct_741}
#   2: @L_2↓mindspore_nn_layer_basic_Dense_construct_740:CNode_796{[0]: ValueNode<Primitive> Return, [1]: CNode_795}


subgraph attr:
training : 1
subgraph instance: L_✗2↓mindspore_nn_layer_basic_Dense_construct_741 : 0x309179818
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_✗2↓mindspore_nn_layer_basic_Dense_construct_741 parent: [subgraph @L_✓↓mindspore_nn_layer_basic_Dense_construct_739]() {

#------------------------> 14
  %1(CNode_797) = call @L_3↓mindspore_nn_layer_basic_Dense_construct_742()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:630/        if self.activation_flag:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:630/        if self.activation_flag:/
}
# Order:
#   1: @L_✗2↓mindspore_nn_layer_basic_Dense_construct_741:CNode_797{[0]: ValueNode<FuncGraph> L_3↓mindspore_nn_layer_basic_Dense_construct_742}
#   2: @L_✗2↓mindspore_nn_layer_basic_Dense_construct_741:CNode_798{[0]: ValueNode<Primitive> Return, [1]: CNode_797}


subgraph attr:
training : 1
subgraph instance: L_3↓mindspore_nn_layer_basic_Dense_construct_742 : 0x309179e18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_3↓mindspore_nn_layer_basic_Dense_construct_742 parent: [subgraph @L_✓↓mindspore_nn_layer_basic_Dense_construct_739]() {
  %1(x_shape) = $(L_mindspore_nn_layer_basic_Dense_construct_737):S_Prim_Shape(%para32_x)
      : (<Tensor[Float32], (32, 1024)>) -> (<Tuple[Int64*2], TupleShape(NoShape, NoShape)>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:623/        x_shape = self.shape_op(x)/
  %2(CNode_799) = S_Prim_inner_len(%1)
      : (<Tuple[Int64*2], TupleShape(NoShape, NoShape)>) -> (<Int64, NoShape>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/
  %3(CNode_800) = S_Prim_not_equal(%2, I64(2))
      : (<Int64, NoShape>, <Int64, NoShape>) -> (<Bool, NoShape>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/
  %4(CNode_801) = Cond(%3, Bool(0))
      : (<Bool, NoShape>, <Bool, NoShape>) -> (<Bool, NoShape>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/
  %5(CNode_802) = Switch(%4, @L_✓3↓mindspore_nn_layer_basic_Dense_construct_803, @L_✗3↓mindspore_nn_layer_basic_Dense_construct_743)
      : (<Bool, NoShape>, <Func, NoShape>, <Func, NoShape>) -> (<Func, NoShape>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/

#------------------------> 15
  %6(CNode_804) = %5()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/
  %7(CNode_806) = call @L_4↓mindspore_nn_layer_basic_Dense_construct_805(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /var/folders/n4/p41p3m3n5c574nhxjsj9n7tc0000gn/T/ipykernel_8186/286663031.py:31/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/
}
# Order:
#   1: @L_3↓mindspore_nn_layer_basic_Dense_construct_742:CNode_799{[0]: ValueNode<DoSignaturePrimitive> S_Prim_inner_len, [1]: x_shape}
#   2: @L_3↓mindspore_nn_layer_basic_Dense_construct_742:CNode_800{[0]: ValueNode<DoSignaturePrimitive> S_Prim_not_equal, [1]: CNode_799, [2]: ValueNode<Int64Imm> 2}
#   3: @L_3↓mindspore_nn_layer_basic_Dense_construct_742:CNode_801{[0]: ValueNode<Primitive> Cond, [1]: CNode_800, [2]: ValueNode<BoolImm> false}
#   4: @L_3↓mindspore_nn_layer_basic_Dense_construct_742:CNode_802{[0]: ValueNode<Primitive> Switch, [1]: CNode_801, [2]: ValueNode<FuncGraph> L_✓3↓mindspore_nn_layer_basic_Dense_construct_803, [3]: ValueNode<FuncGraph> L_✗3↓mindspore_nn_layer_basic_Dense_construct_743}
#   5: @L_3↓mindspore_nn_layer_basic_Dense_construct_742:CNode_804{[0]: CNode_802}
#   6: @L_3↓mindspore_nn_layer_basic_Dense_construct_742:CNode_806{[0]: ValueNode<FuncGraph> L_4↓mindspore_nn_layer_basic_Dense_construct_805, [1]: CNode_804}
#   7: @L_3↓mindspore_nn_layer_basic_Dense_construct_742:CNode_807{[0]: ValueNode<Primitive> Return, [1]: CNode_806}


subgraph attr:
training : 1
subgraph instance: L_✗3↓mindspore_nn_layer_basic_Dense_construct_743 : 0x30917a418
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_✗3↓mindspore_nn_layer_basic_Dense_construct_743 parent: [subgraph @L_✓↓mindspore_nn_layer_basic_Dense_construct_739]() {

#------------------------> 16
  %1(x) = $(L_↓mindspore_nn_layer_basic_Dense_construct_738):S_Prim_MatMul[output_names: ["output"], transpose_a: Bool(0), input_names: ["x1", "x2"], transpose_x2: Bool(1), transpose_x1: Bool(0), transpose_b: Bool(1)](%para35_фx, %para34_L_fc3.weight)
      : (<Tensor[Float32], (32, 1024)>, <Ref[Tensor[Float32]], (120, 784)>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:627/        x = self.matmul(x, self.weight)/
  %2(x) = $(L_✓↓mindspore_nn_layer_basic_Dense_construct_739):S_Prim_BiasAdd[output_names: ["output"], format: "NCHW", input_names: ["x", "b"]](%1, %para33_L_fc3.bias)
      : (<null>, <Ref[Tensor[Float32]], (120)>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:629/            x = self.bias_add(x, self.bias)/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/
}
# Order:
#   1: @L_✗3↓mindspore_nn_layer_basic_Dense_construct_743:CNode_808{[0]: ValueNode<Primitive> Return, [1]: x}


# ===============================================================================================
# The total of function graphs in evaluation stack: 17/18 (Ignored 1 internal frames).
# ===============================================================================================


# ===============================================================================================
# The rest function graphs are the following:
# ===============================================================================================
subgraph attr:
training : 1
subgraph instance: _no_sens_impl_748 : 0x3090e4418
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @_no_sens_impl_748 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_723](%para36_inputs) {
  %1(CNode_751) = call @✗_no_sens_impl_809()
      #scope: (Default)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:437/        if self.return_grad:/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:437/        if self.return_grad:/
}
# Order:
#   1: @_no_sens_impl_748:loss{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.752, [1]: ValueNode<FuncGraph> mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_810, [2]: param_inputs}
#   2: @_no_sens_impl_748:grads{[0]: ValueNode<UnpackGraphPrimitive> UnpackGraph, [1]: ValueNode<FuncGraph> mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_810, [2]: param_inputs}
#   3: @_no_sens_impl_748:grads{[0]: ValueNode<DoSignaturePrimitive> S_Prim_grad, [1]: grads, [2]: CNode_754}
#   4: @_no_sens_impl_748:grads{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.755, [1]: grads, [2]: param_inputs}
#   5: @_no_sens_impl_748:grads{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Identity_construct_811, [1]: grads}
#   6: @_no_sens_impl_748:CNode_757{[0]: ValueNode<FuncGraph> mindspore_nn_optim_momentum_Momentum_construct_812, [1]: grads}
#   7: @_no_sens_impl_748:loss{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Depend, [1]: loss, [2]: CNode_757}
#   8: @_no_sens_impl_748:CNode_751{[0]: ValueNode<FuncGraph> ✗_no_sens_impl_809}
#   9: @_no_sens_impl_748:CNode_759{[0]: ValueNode<Primitive> Return, [1]: CNode_751}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: mindspore_nn_optim_momentum_Momentum_construct_812 : 0x3090e9e18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:215/    def construct(self, gradients):/
subgraph @mindspore_nn_optim_momentum_Momentum_construct_812 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_723](%para37_gradients) {
  %1(CNode_813) = S_Prim_AssignAdd[output_names: ["ref"], side_effect_mem: Bool(1), input_names: ["ref", "value"]](%para11_global_step, Tensor(shape=[1], dtype=Int32, value=[1]))
      : (<Ref[Tensor[Int32]], (1), ref_key=:global_step>, <null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:223/        self.assignadd(self.global_step, self.global_step_increase_tensor)/
  %2(CNode_814) = StopGradient(%1)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:214/    @jit/
  %3(CNode_816) = call @✗mindspore_nn_optim_momentum_Momentum_construct_815()
      #scope: (Default/optimizer-Momentum)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:224/        if self.use_dist_optimizer:/
  %4(CNode_817) = Depend[side_effect_propagate: I64(1)](%3, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:224/        if self.use_dist_optimizer:/
  Return(%4)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:224/        if self.use_dist_optimizer:/
}
# Order:
#   1: @mindspore_nn_optim_momentum_Momentum_construct_812:gradients{[0]: ValueNode<FuncGraph> flatten_gradients_818, [1]: param_gradients}
#   2: @mindspore_nn_optim_momentum_Momentum_construct_812:gradients{[0]: ValueNode<FuncGraph> decay_weight_819, [1]: gradients}
#   3: @mindspore_nn_optim_momentum_Momentum_construct_812:gradients{[0]: ValueNode<FuncGraph> gradients_centralization_820, [1]: gradients}
#   4: @mindspore_nn_optim_momentum_Momentum_construct_812:gradients{[0]: ValueNode<FuncGraph> scale_grad_821, [1]: gradients}
#   5: @mindspore_nn_optim_momentum_Momentum_construct_812:lr{[0]: ValueNode<FuncGraph> get_lr_822}
#   6: @mindspore_nn_optim_momentum_Momentum_construct_812:CNode_813{[0]: ValueNode<DoSignaturePrimitive> S_Prim_AssignAdd, [1]: param_global_step, [2]: ValueNode<Tensor> Tensor(shape=[1], dtype=Int32, value=[1])}
#   7: @mindspore_nn_optim_momentum_Momentum_construct_812:CNode_816{[0]: ValueNode<FuncGraph> ✗mindspore_nn_optim_momentum_Momentum_construct_815}
#   8: @mindspore_nn_optim_momentum_Momentum_construct_812:CNode_823{[0]: ValueNode<Primitive> Return, [1]: CNode_817}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Identity_construct_811 : 0x3090e9818
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:505/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Identity_construct_811(%para38_x) {
  Return(%para38_x)
      : (<null>)
      #scope: (Default/grad_reducer-Identity)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:506/        return x/
}
# Order:
#   1: @mindspore_nn_layer_basic_Identity_construct_811:CNode_824{[0]: ValueNode<Primitive> Return, [1]: param_x}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_810 : 0x3090ff618
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:120/    def construct(self, data, label):/
subgraph @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_810 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_723](%para39_data, %para40_label) {
  %1(out) = call @__main___LeNet5_construct_825(%para39_data)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:121/        out = self._backbone(data)/
  %2(CNode_764) = call @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_826(%1, %para40_label)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:122/        return self._loss_fn(out, label)/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:122/        return self._loss_fn(out, label)/
}
# Order:
#   1: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_810:out{[0]: ValueNode<FuncGraph> __main___LeNet5_construct_825, [1]: param_data}
#   2: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_810:CNode_764{[0]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_826, [1]: out, [2]: param_label}
#   3: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_810:CNode_765{[0]: ValueNode<Primitive> Return, [1]: CNode_764}


subgraph attr:
training : 1
subgraph instance: ✗_no_sens_impl_809 : 0x30914f218
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @✗_no_sens_impl_809 parent: [subgraph @_no_sens_impl_748]() {
  %1(CNode_760) = call @↓_no_sens_impl_827()
      #scope: (Default)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:437/        if self.return_grad:/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:437/        if self.return_grad:/
}
# Order:
#   1: @✗_no_sens_impl_809:CNode_760{[0]: ValueNode<FuncGraph> ↓_no_sens_impl_827}
#   2: @✗_no_sens_impl_809:CNode_761{[0]: ValueNode<Primitive> Return, [1]: CNode_760}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: scale_grad_821 : 0x3090f9c18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:471/    def scale_grad(self, gradients):/
subgraph @scale_grad_821(%para41_gradients) {
  %1(CNode_829) = call @✗scale_grad_828()
      #scope: (Default/optimizer-Momentum)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:485/        if self.need_scale:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:485/        if self.need_scale:/
}
# Order:
#   1: @scale_grad_821:CNode_829{[0]: ValueNode<FuncGraph> ✗scale_grad_828}
#   2: @scale_grad_821:CNode_830{[0]: ValueNode<Primitive> Return, [1]: CNode_829}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: gradients_centralization_820 : 0x3090f6a18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:452/    def gradients_centralization(self, gradients):/
subgraph @gradients_centralization_820(%para42_gradients) {
  %1(CNode_832) = call @✗gradients_centralization_831()
      #scope: (Default/optimizer-Momentum)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:466/        if self.is_group:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:466/        if self.is_group:/
}
# Order:
#   1: @gradients_centralization_820:CNode_832{[0]: ValueNode<FuncGraph> ✗gradients_centralization_831}
#   2: @gradients_centralization_820:CNode_833{[0]: ValueNode<Primitive> Return, [1]: CNode_832}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: decay_weight_819 : 0x3090f5818
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:429/    def decay_weight(self, gradients):/
subgraph @decay_weight_819(%para43_gradients) {
  %1(CNode_835) = call @✗decay_weight_834()
      #scope: (Default/optimizer-Momentum)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:442/        if self.exec_weight_decay:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:442/        if self.exec_weight_decay:/
}
# Order:
#   1: @decay_weight_819:CNode_835{[0]: ValueNode<FuncGraph> ✗decay_weight_834}
#   2: @decay_weight_819:CNode_836{[0]: ValueNode<Primitive> Return, [1]: CNode_835}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: flatten_gradients_818 : 0x3090f4618
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:410/    def flatten_gradients(self, gradients):/
subgraph @flatten_gradients_818(%para44_gradients) {
  %1(CNode_838) = call @✗flatten_gradients_837()
      #scope: (Default/optimizer-Momentum)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:424/        if self._use_flattened_params:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:424/        if self._use_flattened_params:/
}
# Order:
#   1: @flatten_gradients_818:CNode_838{[0]: ValueNode<FuncGraph> ✗flatten_gradients_837}
#   2: @flatten_gradients_818:CNode_839{[0]: ValueNode<Primitive> Return, [1]: CNode_838}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: get_lr_822 : 0x3090f8a18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:739/    def get_lr(self):/
subgraph @get_lr_822 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_723]() {
  %1(CNode_841) = call @✗get_lr_840()
      #scope: (Default/optimizer-Momentum)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:748/        if self.dynamic_lr:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:748/        if self.dynamic_lr:/
}
# Order:
#   1: @get_lr_822:CNode_841{[0]: ValueNode<FuncGraph> ✗get_lr_840}
#   2: @get_lr_822:CNode_842{[0]: ValueNode<Primitive> Return, [1]: CNode_841}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ✗mindspore_nn_optim_momentum_Momentum_construct_815 : 0x3090fae18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:215/    def construct(self, gradients):/
subgraph @✗mindspore_nn_optim_momentum_Momentum_construct_815 parent: [subgraph @mindspore_nn_optim_momentum_Momentum_construct_812]() {
  %1(CNode_844) = call @2✗mindspore_nn_optim_momentum_Momentum_construct_843()
      #scope: (Default/optimizer-Momentum)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:234/            if self.is_group_lr:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:234/            if self.is_group_lr:/
}
# Order:
#   1: @✗mindspore_nn_optim_momentum_Momentum_construct_815:CNode_844{[0]: ValueNode<FuncGraph> 2✗mindspore_nn_optim_momentum_Momentum_construct_843}
#   2: @✗mindspore_nn_optim_momentum_Momentum_construct_815:CNode_845{[0]: ValueNode<Primitive> Return, [1]: CNode_844}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_826 : 0x30914aa18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:777/    def construct(self, logits, labels):/
subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_826(%para45_logits, %para46_labels) {
  %1(CNode_846) = S_Prim__check_is_tensor[constexpr_prim: Bool(1)]("logits", %para45_logits, "SoftmaxCrossEntropyWithLogits")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:778/        _check_is_tensor('logits', logits, self.cls_name)/
  %2(CNode_847) = S_Prim__check_is_tensor[constexpr_prim: Bool(1)]("labels", %para46_labels, "SoftmaxCrossEntropyWithLogits")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:779/        _check_is_tensor('labels', labels, self.cls_name)/
  %3(CNode_848) = MakeTuple(%1, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:777/    def construct(self, logits, labels):/
  %4(CNode_849) = StopGradient(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:777/    def construct(self, logits, labels):/
  %5(CNode_851) = call @✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_850()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:780/        if self.sparse:/
  %6(CNode_852) = Depend[side_effect_propagate: I64(1)](%5, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:780/        if self.sparse:/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:780/        if self.sparse:/
}
# Order:
#   1: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_826:CNode_846{[0]: ValueNode<DoSignaturePrimitive> S_Prim__check_is_tensor, [1]: ValueNode<StringImm> logits, [2]: param_logits, [3]: ValueNode<StringImm> SoftmaxCrossEntropyWithLogits}
#   2: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_826:CNode_847{[0]: ValueNode<DoSignaturePrimitive> S_Prim__check_is_tensor, [1]: ValueNode<StringImm> labels, [2]: param_labels, [3]: ValueNode<StringImm> SoftmaxCrossEntropyWithLogits}
#   3: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_826:CNode_851{[0]: ValueNode<FuncGraph> ✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_850}
#   4: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_826:CNode_853{[0]: ValueNode<Primitive> Return, [1]: CNode_852}


subgraph attr:
training : 1
subgraph instance: __main___LeNet5_construct_825 : 0x309100418
# In file /var/folders/n4/p41p3m3n5c574nhxjsj9n7tc0000gn/T/ipykernel_8186/286663031.py:17/
subgraph @__main___LeNet5_construct_825 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_723](%para47_x) {
  %1(x) = call @mindspore_nn_layer_conv_Conv2d_construct_854(%para47_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /var/folders/n4/p41p3m3n5c574nhxjsj9n7tc0000gn/T/ipykernel_8186/286663031.py:18/
  %2(x) = call @mindspore_nn_layer_activation_ReLU_construct_855(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /var/folders/n4/p41p3m3n5c574nhxjsj9n7tc0000gn/T/ipykernel_8186/286663031.py:19/
  %3(x) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_856(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /var/folders/n4/p41p3m3n5c574nhxjsj9n7tc0000gn/T/ipykernel_8186/286663031.py:20/
  %4(x) = call @mindspore_nn_layer_conv_Conv2d_construct_857(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /var/folders/n4/p41p3m3n5c574nhxjsj9n7tc0000gn/T/ipykernel_8186/286663031.py:21/
  %5(x) = call @mindspore_nn_layer_activation_ReLU_construct_855(%4)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /var/folders/n4/p41p3m3n5c574nhxjsj9n7tc0000gn/T/ipykernel_8186/286663031.py:22/
  %6(x) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_856(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /var/folders/n4/p41p3m3n5c574nhxjsj9n7tc0000gn/T/ipykernel_8186/286663031.py:23/
  %7(x) = call @mindspore_nn_layer_basic_Flatten_construct_858(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /var/folders/n4/p41p3m3n5c574nhxjsj9n7tc0000gn/T/ipykernel_8186/286663031.py:24/
  %8(x) = call @mindspore_nn_layer_basic_Dense_construct_859(%7)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /var/folders/n4/p41p3m3n5c574nhxjsj9n7tc0000gn/T/ipykernel_8186/286663031.py:25/
  %9(x) = call @mindspore_nn_layer_activation_ReLU_construct_855(%8)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /var/folders/n4/p41p3m3n5c574nhxjsj9n7tc0000gn/T/ipykernel_8186/286663031.py:26/
  %10(x) = call @mindspore_nn_layer_basic_Dropout_construct_860(%9)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /var/folders/n4/p41p3m3n5c574nhxjsj9n7tc0000gn/T/ipykernel_8186/286663031.py:27/
  %11(x) = call @mindspore_nn_layer_basic_Dense_construct_861(%10)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /var/folders/n4/p41p3m3n5c574nhxjsj9n7tc0000gn/T/ipykernel_8186/286663031.py:28/
  %12(x) = call @mindspore_nn_layer_activation_ReLU_construct_855(%11)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /var/folders/n4/p41p3m3n5c574nhxjsj9n7tc0000gn/T/ipykernel_8186/286663031.py:29/
  %13(x) = call @mindspore_nn_layer_basic_Dropout_construct_862(%12)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /var/folders/n4/p41p3m3n5c574nhxjsj9n7tc0000gn/T/ipykernel_8186/286663031.py:30/
  %14(x) = call @mindspore_nn_layer_basic_Dense_construct_863(%13)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /var/folders/n4/p41p3m3n5c574nhxjsj9n7tc0000gn/T/ipykernel_8186/286663031.py:31/
  %15(x) = call @mindspore_nn_layer_basic_Dropout_construct_864(%14)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /var/folders/n4/p41p3m3n5c574nhxjsj9n7tc0000gn/T/ipykernel_8186/286663031.py:32/
  Return(%15)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /var/folders/n4/p41p3m3n5c574nhxjsj9n7tc0000gn/T/ipykernel_8186/286663031.py:33/
}
# Order:
#   1: @__main___LeNet5_construct_825:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_854, [1]: param_x}
#   2: @__main___LeNet5_construct_825:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_855, [1]: x}
#   3: @__main___LeNet5_construct_825:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_856, [1]: x}
#   4: @__main___LeNet5_construct_825:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_857, [1]: x}
#   5: @__main___LeNet5_construct_825:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_855, [1]: x}
#   6: @__main___LeNet5_construct_825:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_856, [1]: x}
#   7: @__main___LeNet5_construct_825:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Flatten_construct_858, [1]: x}
#   8: @__main___LeNet5_construct_825:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dense_construct_859, [1]: x}
#   9: @__main___LeNet5_construct_825:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_855, [1]: x}
#  10: @__main___LeNet5_construct_825:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_860, [1]: x}
#  11: @__main___LeNet5_construct_825:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dense_construct_861, [1]: x}
#  12: @__main___LeNet5_construct_825:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_855, [1]: x}
#  13: @__main___LeNet5_construct_825:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_862, [1]: x}
#  14: @__main___LeNet5_construct_825:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dense_construct_863, [1]: x}
#  15: @__main___LeNet5_construct_825:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_864, [1]: x}
#  16: @__main___LeNet5_construct_825:CNode_776{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
training : 1
subgraph instance: ↓_no_sens_impl_827 : 0x30914f818
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @↓_no_sens_impl_827 parent: [subgraph @_no_sens_impl_748]() {
  %1(loss) = $(_no_sens_impl_748):UnpackCall_unpack_call(@mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_810, %para36_inputs)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:433/        loss = self.network(*inputs)/
  %2(grads) = $(_no_sens_impl_748):UnpackGraph(@mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_810, %para36_inputs)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %3(CNode_754) = $(_no_sens_impl_748):MakeTuple(%para3_conv1.weight, %para4_conv2.weight, %para5_fc1.weight, %para6_fc1.bias, %para7_fc2.weight, %para8_fc2.bias, %para9_fc3.weight, %para10_fc3.bias)
      : (<Ref[Tensor[Float32]], (6, 1, 3, 3), ref_key=:conv1.weight>, <Ref[Tensor[Float32]], (16, 6, 3, 3), ref_key=:conv2.weight>, <Ref[Tensor[Float32]], (120, 784), ref_key=:fc1.weight>, <Ref[Tensor[Float32]], (120), ref_key=:fc1.bias>, <Ref[Tensor[Float32]], (84, 120), ref_key=:fc2.weight>, <Ref[Tensor[Float32]], (84), ref_key=:fc2.bias>, <Ref[Tensor[Float32]], (10, 84), ref_key=:fc3.weight>, <Ref[Tensor[Float32]], (10), ref_key=:fc3.bias>) -> (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %4(grads) = $(_no_sens_impl_748):S_Prim_grad(%2, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %5(grads) = $(_no_sens_impl_748):UnpackCall_unpack_call(%4, %para36_inputs)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %6(grads) = $(_no_sens_impl_748):call @mindspore_nn_layer_basic_Identity_construct_811(%5)
      : (<null>) -> (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:435/        grads = self.grad_reducer(grads)/
  %7(CNode_757) = $(_no_sens_impl_748):call @mindspore_nn_optim_momentum_Momentum_construct_812(%6)
      : (<null>) -> (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:436/        loss = F.depend(loss, self.optimizer(grads))/
  %8(loss) = $(_no_sens_impl_748):S_Prim_Depend[side_effect_propagate: I64(1)](%1, %7)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:436/        loss = F.depend(loss, self.optimizer(grads))/
  Return(%8)
      : (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/wrap/cell_wrapper.py:442/        return loss/
}
# Order:
#   1: @↓_no_sens_impl_827:CNode_762{[0]: ValueNode<Primitive> Return, [1]: loss}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ✗scale_grad_828 : 0x3090fa218
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:471/    def scale_grad(self, gradients):/
subgraph @✗scale_grad_828 parent: [subgraph @scale_grad_821]() {
  %1(CNode_866) = call @↓scale_grad_865()
      #scope: (Default/optimizer-Momentum)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:485/        if self.need_scale:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:485/        if self.need_scale:/
}
# Order:
#   1: @✗scale_grad_828:CNode_866{[0]: ValueNode<FuncGraph> ↓scale_grad_865}
#   2: @✗scale_grad_828:CNode_867{[0]: ValueNode<Primitive> Return, [1]: CNode_866}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ✗gradients_centralization_831 : 0x3090f7018
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:452/    def gradients_centralization(self, gradients):/
subgraph @✗gradients_centralization_831 parent: [subgraph @gradients_centralization_820]() {
  %1(CNode_869) = call @↓gradients_centralization_868()
      #scope: (Default/optimizer-Momentum)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:466/        if self.is_group:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:466/        if self.is_group:/
}
# Order:
#   1: @✗gradients_centralization_831:CNode_869{[0]: ValueNode<FuncGraph> ↓gradients_centralization_868}
#   2: @✗gradients_centralization_831:CNode_870{[0]: ValueNode<Primitive> Return, [1]: CNode_869}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ✗decay_weight_834 : 0x3090f5e18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:429/    def decay_weight(self, gradients):/
subgraph @✗decay_weight_834 parent: [subgraph @decay_weight_819]() {
  %1(CNode_872) = call @↓decay_weight_871()
      #scope: (Default/optimizer-Momentum)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:442/        if self.exec_weight_decay:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:442/        if self.exec_weight_decay:/
}
# Order:
#   1: @✗decay_weight_834:CNode_872{[0]: ValueNode<FuncGraph> ↓decay_weight_871}
#   2: @✗decay_weight_834:CNode_873{[0]: ValueNode<Primitive> Return, [1]: CNode_872}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ✗flatten_gradients_837 : 0x3090f4c18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:410/    def flatten_gradients(self, gradients):/
subgraph @✗flatten_gradients_837 parent: [subgraph @flatten_gradients_818]() {
  %1(CNode_875) = call @↓flatten_gradients_874()
      #scope: (Default/optimizer-Momentum)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:424/        if self._use_flattened_params:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:424/        if self._use_flattened_params:/
}
# Order:
#   1: @✗flatten_gradients_837:CNode_875{[0]: ValueNode<FuncGraph> ↓flatten_gradients_874}
#   2: @✗flatten_gradients_837:CNode_876{[0]: ValueNode<Primitive> Return, [1]: CNode_875}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ✗get_lr_840 : 0x3090f9018
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:739/    def get_lr(self):/
subgraph @✗get_lr_840 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_723]() {
  %1(CNode_878) = call @↓get_lr_877()
      #scope: (Default/optimizer-Momentum)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:748/        if self.dynamic_lr:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:748/        if self.dynamic_lr:/
}
# Order:
#   1: @✗get_lr_840:CNode_878{[0]: ValueNode<FuncGraph> ↓get_lr_877}
#   2: @✗get_lr_840:CNode_879{[0]: ValueNode<Primitive> Return, [1]: CNode_878}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: 2✗mindspore_nn_optim_momentum_Momentum_construct_843 : 0x3090fb418
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:215/    def construct(self, gradients):/
subgraph @2✗mindspore_nn_optim_momentum_Momentum_construct_843 parent: [subgraph @mindspore_nn_optim_momentum_Momentum_construct_812]() {
  %1(CNode_881) = call @↓✗mindspore_nn_optim_momentum_Momentum_construct_880()
      #scope: (Default/optimizer-Momentum)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:238/                success = self.hyper_map_reverse(F.partial(_momentum_opt, self.opt, self.momentum, lr),/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:238/                success = self.hyper_map_reverse(F.partial(_momentum_opt, self.opt, self.momentum, lr),/
}
# Order:
#   1: @2✗mindspore_nn_optim_momentum_Momentum_construct_843:CNode_882{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Partial, [1]: ValueNode<DoSignaturePrimitive> S_Prim_momentum_opt, [2]: ValueNode<DoSignaturePrimitive> S_Prim_ApplyMomentum, [3]: param_momentum, [4]: lr}
#   2: @2✗mindspore_nn_optim_momentum_Momentum_construct_843:success{[0]: ValueNode<DoSignaturePrimitive> S_Prim_hyper_map, [1]: CNode_882, [2]: gradients, [3]: CNode_883, [4]: CNode_884, [5]: ValueNode<ValueTuple> (false, false, false, false, false, false, false, false), [6]: ValueNode<ValueTuple> (false, false, false, false, false, false, false, false)}
#   3: @2✗mindspore_nn_optim_momentum_Momentum_construct_843:CNode_881{[0]: ValueNode<FuncGraph> ↓✗mindspore_nn_optim_momentum_Momentum_construct_880}
#   4: @2✗mindspore_nn_optim_momentum_Momentum_construct_843:CNode_885{[0]: ValueNode<Primitive> Return, [1]: CNode_881}


subgraph attr:
training : 1
subgraph instance: ✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_850 : 0x30914b018
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:777/    def construct(self, logits, labels):/
subgraph @✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_850 parent: [subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_826]() {
  %1(CNode_886) = S_Prim_equal("mean", "mean")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:781/            if self.reduction == 'mean':/
  %2(CNode_887) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:781/            if self.reduction == 'mean':/
  %3(CNode_888) = Switch(%2, @2✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_889, @✗✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_890)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:781/            if self.reduction == 'mean':/
  %4(CNode_891) = %3()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:781/            if self.reduction == 'mean':/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:781/            if self.reduction == 'mean':/
}
# Order:
#   1: @✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_850:CNode_886{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: ValueNode<StringImm> mean, [2]: ValueNode<StringImm> mean}
#   2: @✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_850:CNode_887{[0]: ValueNode<Primitive> Cond, [1]: CNode_886, [2]: ValueNode<BoolImm> false}
#   3: @✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_850:CNode_888{[0]: ValueNode<Primitive> Switch, [1]: CNode_887, [2]: ValueNode<FuncGraph> 2✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_889, [3]: ValueNode<FuncGraph> ✗✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_890}
#   4: @✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_850:CNode_891{[0]: CNode_888}
#   5: @✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_850:CNode_892{[0]: ValueNode<Primitive> Return, [1]: CNode_891}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_864 : 0x309147a18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_864(%para48_x) {
  %1(CNode_893) = S_Prim_logical_not(Bool(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %2(CNode_894) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %3(CNode_895) = Switch(%2, @↰mindspore_nn_layer_basic_Dropout_construct_896, @↱mindspore_nn_layer_basic_Dropout_construct_897)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %4(CNode_898) = %3()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %5(CNode_899) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %6(CNode_900) = Switch(%5, @✓mindspore_nn_layer_basic_Dropout_construct_901, @✗mindspore_nn_layer_basic_Dropout_construct_902)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %7(CNode_903) = %6()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_864:CNode_893{[0]: ValueNode<DoSignaturePrimitive> S_Prim_logical_not, [1]: ValueNode<BoolImm> true}
#   2: @mindspore_nn_layer_basic_Dropout_construct_864:CNode_894{[0]: ValueNode<Primitive> Cond, [1]: CNode_893, [2]: ValueNode<BoolImm> false}
#   3: @mindspore_nn_layer_basic_Dropout_construct_864:CNode_895{[0]: ValueNode<Primitive> Switch, [1]: CNode_894, [2]: ValueNode<FuncGraph> ↰mindspore_nn_layer_basic_Dropout_construct_896, [3]: ValueNode<FuncGraph> ↱mindspore_nn_layer_basic_Dropout_construct_897}
#   4: @mindspore_nn_layer_basic_Dropout_construct_864:CNode_898{[0]: CNode_895}
#   5: @mindspore_nn_layer_basic_Dropout_construct_864:CNode_899{[0]: ValueNode<Primitive> Cond, [1]: CNode_898, [2]: ValueNode<BoolImm> false}
#   6: @mindspore_nn_layer_basic_Dropout_construct_864:CNode_900{[0]: ValueNode<Primitive> Switch, [1]: CNode_899, [2]: ValueNode<FuncGraph> ✓mindspore_nn_layer_basic_Dropout_construct_901, [3]: ValueNode<FuncGraph> ✗mindspore_nn_layer_basic_Dropout_construct_902}
#   7: @mindspore_nn_layer_basic_Dropout_construct_864:CNode_903{[0]: CNode_900}
#   8: @mindspore_nn_layer_basic_Dropout_construct_864:CNode_904{[0]: ValueNode<Primitive> Return, [1]: CNode_903}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dense_construct_863 : 0x309147418
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dense_construct_863 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_723](%para49_x) {
  %1(CNode_906) = call @L_mindspore_nn_layer_basic_Dense_construct_905(%para49_x, %para10_fc3.bias, %para9_fc3.weight)
      : (<null>, <Ref[Tensor[Float32]], (10), ref_key=:fc3.bias>, <Ref[Tensor[Float32]], (10, 84), ref_key=:fc3.weight>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dense_construct_863:CNode_906{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_905, [1]: param_x, [2]: param_fc3.bias, [3]: param_fc3.weight}
#   2: @mindspore_nn_layer_basic_Dense_construct_863:CNode_790{[0]: ValueNode<Primitive> Return, [1]: CNode_906}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_862 : 0x309144418
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_862(%para50_x) {
  %1(CNode_907) = S_Prim_logical_not(Bool(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp2-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %2(CNode_908) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp2-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %3(CNode_909) = Switch(%2, @↰mindspore_nn_layer_basic_Dropout_construct_910, @↱mindspore_nn_layer_basic_Dropout_construct_911)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp2-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %4(CNode_912) = %3()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp2-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %5(CNode_913) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp2-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %6(CNode_914) = Switch(%5, @✓mindspore_nn_layer_basic_Dropout_construct_915, @✗mindspore_nn_layer_basic_Dropout_construct_916)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp2-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %7(CNode_917) = %6()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp2-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp2-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_862:CNode_907{[0]: ValueNode<DoSignaturePrimitive> S_Prim_logical_not, [1]: ValueNode<BoolImm> true}
#   2: @mindspore_nn_layer_basic_Dropout_construct_862:CNode_908{[0]: ValueNode<Primitive> Cond, [1]: CNode_907, [2]: ValueNode<BoolImm> false}
#   3: @mindspore_nn_layer_basic_Dropout_construct_862:CNode_909{[0]: ValueNode<Primitive> Switch, [1]: CNode_908, [2]: ValueNode<FuncGraph> ↰mindspore_nn_layer_basic_Dropout_construct_910, [3]: ValueNode<FuncGraph> ↱mindspore_nn_layer_basic_Dropout_construct_911}
#   4: @mindspore_nn_layer_basic_Dropout_construct_862:CNode_912{[0]: CNode_909}
#   5: @mindspore_nn_layer_basic_Dropout_construct_862:CNode_913{[0]: ValueNode<Primitive> Cond, [1]: CNode_912, [2]: ValueNode<BoolImm> false}
#   6: @mindspore_nn_layer_basic_Dropout_construct_862:CNode_914{[0]: ValueNode<Primitive> Switch, [1]: CNode_913, [2]: ValueNode<FuncGraph> ✓mindspore_nn_layer_basic_Dropout_construct_915, [3]: ValueNode<FuncGraph> ✗mindspore_nn_layer_basic_Dropout_construct_916}
#   7: @mindspore_nn_layer_basic_Dropout_construct_862:CNode_917{[0]: CNode_914}
#   8: @mindspore_nn_layer_basic_Dropout_construct_862:CNode_918{[0]: ValueNode<Primitive> Return, [1]: CNode_917}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_activation_ReLU_construct_855 : 0x309143e18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/activation.py:437/    def construct(self, x):/
subgraph @mindspore_nn_layer_activation_ReLU_construct_855(%para51_x) {
  %1(CNode_919) = S_Prim_ReLU[output_names: ["output"], input_names: ["x"]](%para51_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/relu-ReLU)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/activation.py:438/        return self.relu(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/relu-ReLU)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/activation.py:438/        return self.relu(x)/
}
# Order:
#   1: @mindspore_nn_layer_activation_ReLU_construct_855:CNode_919{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReLU, [1]: param_x}
#   2: @mindspore_nn_layer_activation_ReLU_construct_855:CNode_920{[0]: ValueNode<Primitive> Return, [1]: CNode_919}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dense_construct_861 : 0x309143818
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dense_construct_861 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_723](%para52_x) {
  %1(CNode_921) = call @L_mindspore_nn_layer_basic_Dense_construct_905(%para52_x, %para8_fc2.bias, %para7_fc2.weight)
      : (<null>, <Ref[Tensor[Float32]], (84), ref_key=:fc2.bias>, <Ref[Tensor[Float32]], (84, 120), ref_key=:fc2.weight>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc2-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dense_construct_861:CNode_921{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_905, [1]: param_x, [2]: param_fc2.bias, [3]: param_fc2.weight}
#   2: @mindspore_nn_layer_basic_Dense_construct_861:CNode_922{[0]: ValueNode<Primitive> Return, [1]: CNode_921}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_860 : 0x309140818
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_860(%para53_x) {
  %1(CNode_923) = S_Prim_logical_not(Bool(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp1-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %2(CNode_924) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp1-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %3(CNode_925) = Switch(%2, @↰mindspore_nn_layer_basic_Dropout_construct_926, @↱mindspore_nn_layer_basic_Dropout_construct_927)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp1-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %4(CNode_928) = %3()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp1-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %5(CNode_929) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp1-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %6(CNode_930) = Switch(%5, @✓mindspore_nn_layer_basic_Dropout_construct_931, @✗mindspore_nn_layer_basic_Dropout_construct_932)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp1-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %7(CNode_933) = %6()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp1-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp1-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_860:CNode_923{[0]: ValueNode<DoSignaturePrimitive> S_Prim_logical_not, [1]: ValueNode<BoolImm> true}
#   2: @mindspore_nn_layer_basic_Dropout_construct_860:CNode_924{[0]: ValueNode<Primitive> Cond, [1]: CNode_923, [2]: ValueNode<BoolImm> false}
#   3: @mindspore_nn_layer_basic_Dropout_construct_860:CNode_925{[0]: ValueNode<Primitive> Switch, [1]: CNode_924, [2]: ValueNode<FuncGraph> ↰mindspore_nn_layer_basic_Dropout_construct_926, [3]: ValueNode<FuncGraph> ↱mindspore_nn_layer_basic_Dropout_construct_927}
#   4: @mindspore_nn_layer_basic_Dropout_construct_860:CNode_928{[0]: CNode_925}
#   5: @mindspore_nn_layer_basic_Dropout_construct_860:CNode_929{[0]: ValueNode<Primitive> Cond, [1]: CNode_928, [2]: ValueNode<BoolImm> false}
#   6: @mindspore_nn_layer_basic_Dropout_construct_860:CNode_930{[0]: ValueNode<Primitive> Switch, [1]: CNode_929, [2]: ValueNode<FuncGraph> ✓mindspore_nn_layer_basic_Dropout_construct_931, [3]: ValueNode<FuncGraph> ✗mindspore_nn_layer_basic_Dropout_construct_932}
#   7: @mindspore_nn_layer_basic_Dropout_construct_860:CNode_933{[0]: CNode_930}
#   8: @mindspore_nn_layer_basic_Dropout_construct_860:CNode_934{[0]: ValueNode<Primitive> Return, [1]: CNode_933}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dense_construct_859 : 0x30911d418
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dense_construct_859 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_723](%para54_x) {
  %1(CNode_777) = call @L_mindspore_nn_layer_basic_Dense_construct_905(%para54_x, %para6_fc1.bias, %para5_fc1.weight)
      : (<null>, <Ref[Tensor[Float32]], (120), ref_key=:fc1.bias>, <Ref[Tensor[Float32]], (120, 784), ref_key=:fc1.weight>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc1-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dense_construct_859:CNode_777{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_905, [1]: param_x, [2]: param_fc1.bias, [3]: param_fc1.weight}
#   2: @mindspore_nn_layer_basic_Dense_construct_859:CNode_778{[0]: ValueNode<Primitive> Return, [1]: CNode_777}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Flatten_construct_858 : 0x309115218
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:461/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Flatten_construct_858(%para55_x) {
  %1(x_rank) = call @rank_935(%para55_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:462/        x_rank = F.rank(x)/
  %2(CNode_936) = S_Prim_not_equal(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
  %3(CNode_937) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
  %4(CNode_938) = Switch(%3, @↰mindspore_nn_layer_basic_Flatten_construct_939, @↱mindspore_nn_layer_basic_Flatten_construct_940)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
  %5(ndim) = %4()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
  %6(CNode_942) = call @check_axis_valid_941(I64(1), %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:464/        self.check_axis_valid(self.start_dim, ndim)/
  %7(CNode_943) = call @check_axis_valid_941(I64(-1), %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:465/        self.check_axis_valid(self.end_dim, ndim)/
  %8(CNode_944) = MakeTuple(%6, %7)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:461/    def construct(self, x):/
  %9(CNode_945) = StopGradient(%8)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:461/    def construct(self, x):/
  %10(CNode_946) = S_Prim_MakeTuple(%para55_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  %11(CNode_947) = S_Prim_MakeTuple("start_dim", "end_dim")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  %12(CNode_948) = S_Prim_MakeTuple(I64(1), I64(-1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  %13(CNode_949) = S_Prim_make_dict(%11, %12)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  %14(CNode_950) = UnpackCall_unpack_call(@flatten_951, %10, %13)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  %15(CNode_952) = Depend[side_effect_propagate: I64(1)](%14, %9)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  Return(%15)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
}
# Order:
#   1: @mindspore_nn_layer_basic_Flatten_construct_858:x_rank{[0]: ValueNode<FuncGraph> rank_935, [1]: param_x}
#   2: @mindspore_nn_layer_basic_Flatten_construct_858:CNode_936{[0]: ValueNode<DoSignaturePrimitive> S_Prim_not_equal, [1]: x_rank, [2]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_basic_Flatten_construct_858:CNode_937{[0]: ValueNode<Primitive> Cond, [1]: CNode_936, [2]: ValueNode<BoolImm> false}
#   4: @mindspore_nn_layer_basic_Flatten_construct_858:CNode_938{[0]: ValueNode<Primitive> Switch, [1]: CNode_937, [2]: ValueNode<FuncGraph> ↰mindspore_nn_layer_basic_Flatten_construct_939, [3]: ValueNode<FuncGraph> ↱mindspore_nn_layer_basic_Flatten_construct_940}
#   5: @mindspore_nn_layer_basic_Flatten_construct_858:ndim{[0]: CNode_938}
#   6: @mindspore_nn_layer_basic_Flatten_construct_858:CNode_942{[0]: ValueNode<FuncGraph> check_axis_valid_941, [1]: ValueNode<Int64Imm> 1, [2]: ndim}
#   7: @mindspore_nn_layer_basic_Flatten_construct_858:CNode_943{[0]: ValueNode<FuncGraph> check_axis_valid_941, [1]: ValueNode<Int64Imm> -1, [2]: ndim}
#   8: @mindspore_nn_layer_basic_Flatten_construct_858:CNode_946{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: param_x}
#   9: @mindspore_nn_layer_basic_Flatten_construct_858:CNode_947{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: ValueNode<StringImm> start_dim, [2]: ValueNode<StringImm> end_dim}
#  10: @mindspore_nn_layer_basic_Flatten_construct_858:CNode_948{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: ValueNode<Int64Imm> 1, [2]: ValueNode<Int64Imm> -1}
#  11: @mindspore_nn_layer_basic_Flatten_construct_858:CNode_949{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_dict, [1]: CNode_947, [2]: CNode_948}
#  12: @mindspore_nn_layer_basic_Flatten_construct_858:CNode_950{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.953, [1]: ValueNode<FuncGraph> flatten_951, [2]: CNode_946, [3]: CNode_949}
#  13: @mindspore_nn_layer_basic_Flatten_construct_858:CNode_954{[0]: ValueNode<Primitive> Return, [1]: CNode_952}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_856 : 0x309107c18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_856(%para56_x) {
  %1(CNode_955) = getattr(%para56_x, "ndim")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:570/        if x.ndim == 3:/
  %2(CNode_956) = S_Prim_equal(%1, I64(3))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:570/        if x.ndim == 3:/
  %3(CNode_957) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:570/        if x.ndim == 3:/
  %4(CNode_958) = Switch(%3, @✓mindspore_nn_layer_pooling_MaxPool2d_construct_959, @✗mindspore_nn_layer_pooling_MaxPool2d_construct_960)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:570/        if x.ndim == 3:/
  %5(CNode_961) = %4()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:570/        if x.ndim == 3:/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:570/        if x.ndim == 3:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_856:CNode_955{[0]: ValueNode<Primitive> getattr, [1]: param_x, [2]: ValueNode<StringImm> ndim}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_856:CNode_956{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: CNode_955, [2]: ValueNode<Int64Imm> 3}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_856:CNode_957{[0]: ValueNode<Primitive> Cond, [1]: CNode_956, [2]: ValueNode<BoolImm> false}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_856:CNode_958{[0]: ValueNode<Primitive> Switch, [1]: CNode_957, [2]: ValueNode<FuncGraph> ✓mindspore_nn_layer_pooling_MaxPool2d_construct_959, [3]: ValueNode<FuncGraph> ✗mindspore_nn_layer_pooling_MaxPool2d_construct_960}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_856:CNode_961{[0]: CNode_958}
#   6: @mindspore_nn_layer_pooling_MaxPool2d_construct_856:CNode_962{[0]: ValueNode<Primitive> Return, [1]: CNode_961}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_857 : 0x309106a18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_857 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_723](%para57_x) {
  %1(CNode_964) = call @✗mindspore_nn_layer_conv_Conv2d_construct_963()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv2-Conv2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv2-Conv2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_857:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_conv2.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_857:CNode_964{[0]: ValueNode<FuncGraph> ✗mindspore_nn_layer_conv_Conv2d_construct_963}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_857:CNode_965{[0]: ValueNode<Primitive> Return, [1]: CNode_964}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_854 : 0x3090f7e18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_854 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_723](%para58_x) {
  %1(CNode_967) = call @✗mindspore_nn_layer_conv_Conv2d_construct_966()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv1-Conv2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv1-Conv2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_854:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_conv1.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_854:CNode_967{[0]: ValueNode<FuncGraph> ✗mindspore_nn_layer_conv_Conv2d_construct_966}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_854:CNode_968{[0]: ValueNode<Primitive> Return, [1]: CNode_967}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ↓scale_grad_865 : 0x3090fa818
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:471/    def scale_grad(self, gradients):/
subgraph @↓scale_grad_865 parent: [subgraph @scale_grad_821]() {
  Return(%para41_gradients)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:488/        return gradients/
}
# Order:
#   1: @↓scale_grad_865:CNode_969{[0]: ValueNode<Primitive> Return, [1]: param_gradients}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ↓gradients_centralization_868 : 0x3090f8418
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:452/    def gradients_centralization(self, gradients):/
subgraph @↓gradients_centralization_868 parent: [subgraph @gradients_centralization_820]() {
  Return(%para42_gradients)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:469/        return gradients/
}
# Order:
#   1: @↓gradients_centralization_868:CNode_970{[0]: ValueNode<Primitive> Return, [1]: param_gradients}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ↓decay_weight_871 : 0x3090f6418
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:429/    def decay_weight(self, gradients):/
subgraph @↓decay_weight_871 parent: [subgraph @decay_weight_819]() {
  Return(%para43_gradients)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:450/        return gradients/
}
# Order:
#   1: @↓decay_weight_871:CNode_971{[0]: ValueNode<Primitive> Return, [1]: param_gradients}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ↓flatten_gradients_874 : 0x3090f5218
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:410/    def flatten_gradients(self, gradients):/
subgraph @↓flatten_gradients_874 parent: [subgraph @flatten_gradients_818]() {
  Return(%para44_gradients)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:427/        return gradients/
}
# Order:
#   1: @↓flatten_gradients_874:CNode_972{[0]: ValueNode<Primitive> Return, [1]: param_gradients}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ↓get_lr_877 : 0x3090f9618
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:739/    def get_lr(self):/
subgraph @↓get_lr_877 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_723]() {
  Return(%para21_learning_rate)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:756/        return lr/
}
# Order:
#   1: @↓get_lr_877:CNode_973{[0]: ValueNode<Primitive> Return, [1]: param_learning_rate}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ↓✗mindspore_nn_optim_momentum_Momentum_construct_880 : 0x3090e3418
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:215/    def construct(self, gradients):/
subgraph @↓✗mindspore_nn_optim_momentum_Momentum_construct_880 parent: [subgraph @2✗mindspore_nn_optim_momentum_Momentum_construct_843]() {
  %1(CNode_975) = call @↓mindspore_nn_optim_momentum_Momentum_construct_974()
      #scope: (Default/optimizer-Momentum)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:234/            if self.is_group_lr:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:234/            if self.is_group_lr:/
}
# Order:
#   1: @↓✗mindspore_nn_optim_momentum_Momentum_construct_880:CNode_975{[0]: ValueNode<FuncGraph> ↓mindspore_nn_optim_momentum_Momentum_construct_974}
#   2: @↓✗mindspore_nn_optim_momentum_Momentum_construct_880:CNode_976{[0]: ValueNode<Primitive> Return, [1]: CNode_975}


subgraph attr:
training : 1
subgraph instance: 2✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_889 : 0x30914ec18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:777/    def construct(self, logits, labels):/
subgraph @2✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_889 parent: [subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_826]() {
  %1(x) = S_Prim_SparseSoftmaxCrossEntropyWithLogits[output_names: ["output"], input_names: ["features", "labels"], sens: F32(1), is_grad: Bool(0)](%para45_logits, %para46_labels)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:782/                x = self.sparse_softmax_cross_entropy(logits, labels)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:783/                return x/
}
# Order:
#   1: @2✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_889:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_SparseSoftmaxCrossEntropyWithLogits, [1]: param_logits, [2]: param_labels}
#   2: @2✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_889:CNode_977{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
training : 1
subgraph instance: ✗✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_890 : 0x30914b618
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:777/    def construct(self, logits, labels):/
subgraph @✗✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_890 parent: [subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_826]() {
  %1(CNode_979) = call @↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_978()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:781/            if self.reduction == 'mean':/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:781/            if self.reduction == 'mean':/
}
# Order:
#   1: @✗✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_890:CNode_979{[0]: ValueNode<FuncGraph> ↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_978}
#   2: @✗✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_890:CNode_980{[0]: ValueNode<Primitive> Return, [1]: CNode_979}


subgraph attr:
training : 1
subgraph instance: ✓mindspore_nn_layer_basic_Dropout_construct_901 : 0x30914a418
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:190/    def construct(self, x):/
subgraph @✓mindspore_nn_layer_basic_Dropout_construct_901 parent: [subgraph @mindspore_nn_layer_basic_Dropout_construct_864]() {
  Return(%para48_x)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:192/            return x/
}
# Order:
#   1: @✓mindspore_nn_layer_basic_Dropout_construct_901:CNode_981{[0]: ValueNode<Primitive> Return, [1]: param_x}


subgraph attr:
training : 1
subgraph instance: ✗mindspore_nn_layer_basic_Dropout_construct_902 : 0x309149818
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:190/    def construct(self, x):/
subgraph @✗mindspore_nn_layer_basic_Dropout_construct_902 parent: [subgraph @mindspore_nn_layer_basic_Dropout_construct_864]() {
  %1(CNode_983) = call @↓mindspore_nn_layer_basic_Dropout_construct_982()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @✗mindspore_nn_layer_basic_Dropout_construct_902:CNode_983{[0]: ValueNode<FuncGraph> ↓mindspore_nn_layer_basic_Dropout_construct_982}
#   2: @✗mindspore_nn_layer_basic_Dropout_construct_902:CNode_984{[0]: ValueNode<Primitive> Return, [1]: CNode_983}


subgraph attr:
training : 1
subgraph instance: ↰mindspore_nn_layer_basic_Dropout_construct_896 : 0x309149218
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:190/    def construct(self, x):/
subgraph @↰mindspore_nn_layer_basic_Dropout_construct_896 parent: [subgraph @mindspore_nn_layer_basic_Dropout_construct_864]() {
  %1(CNode_893) = $(mindspore_nn_layer_basic_Dropout_construct_864):S_Prim_logical_not(Bool(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @↰mindspore_nn_layer_basic_Dropout_construct_896:CNode_985{[0]: ValueNode<Primitive> Return, [1]: CNode_893}


subgraph attr:
training : 1
subgraph instance: ↱mindspore_nn_layer_basic_Dropout_construct_897 : 0x309148018
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:190/    def construct(self, x):/
subgraph @↱mindspore_nn_layer_basic_Dropout_construct_897() {
  %1(CNode_986) = S_Prim_equal(F32(0.5), I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %2(CNode_987) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %3(CNode_988) = Switch(%2, @↰↱mindspore_nn_layer_basic_Dropout_construct_989, @2↱mindspore_nn_layer_basic_Dropout_construct_990)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %4(CNode_991) = %3()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @↱mindspore_nn_layer_basic_Dropout_construct_897:CNode_986{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: ValueNode<FP32Imm> 0.5, [2]: ValueNode<Int64Imm> 1}
#   2: @↱mindspore_nn_layer_basic_Dropout_construct_897:CNode_987{[0]: ValueNode<Primitive> Cond, [1]: CNode_986, [2]: ValueNode<BoolImm> false}
#   3: @↱mindspore_nn_layer_basic_Dropout_construct_897:CNode_988{[0]: ValueNode<Primitive> Switch, [1]: CNode_987, [2]: ValueNode<FuncGraph> ↰↱mindspore_nn_layer_basic_Dropout_construct_989, [3]: ValueNode<FuncGraph> 2↱mindspore_nn_layer_basic_Dropout_construct_990}
#   4: @↱mindspore_nn_layer_basic_Dropout_construct_897:CNode_991{[0]: CNode_988}
#   5: @↱mindspore_nn_layer_basic_Dropout_construct_897:CNode_992{[0]: ValueNode<Primitive> Return, [1]: CNode_991}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_905 : 0x30911da18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_905(%para59_x, %para60_, %para61_) {
  %1(x_shape) = S_Prim_Shape(%para59_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:623/        x_shape = self.shape_op(x)/
  %2(CNode_779) = S_Prim_check_dense_input_shape[constexpr_prim: Bool(1)](%1, "Dense")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:624/        check_dense_input_shape(x_shape, self.cls_name)/
  %3(CNode_780) = StopGradient(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
  %4(CNode_781) = S_Prim_inner_len(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
  %5(CNode_782) = S_Prim_not_equal(%4, I64(2))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
  %6(CNode_783) = Cond(%5, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
  %7(CNode_784) = Switch(%6, @L_✓mindspore_nn_layer_basic_Dense_construct_993, @L_✗mindspore_nn_layer_basic_Dense_construct_994)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
  %8(CNode_787) = %7()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
  %9(CNode_788) = call @L_↓mindspore_nn_layer_basic_Dense_construct_995(%8)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /var/folders/n4/p41p3m3n5c574nhxjsj9n7tc0000gn/T/ipykernel_8186/286663031.py:31/
  %10(CNode_789) = Depend[side_effect_propagate: I64(1)](%9, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /var/folders/n4/p41p3m3n5c574nhxjsj9n7tc0000gn/T/ipykernel_8186/286663031.py:31/
  Return(%10)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_905:x_shape{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_x}
#   2: @L_mindspore_nn_layer_basic_Dense_construct_905:CNode_779{[0]: ValueNode<DoSignaturePrimitive> S_Prim_check_dense_input_shape, [1]: x_shape, [2]: ValueNode<StringImm> Dense}
#   3: @L_mindspore_nn_layer_basic_Dense_construct_905:CNode_781{[0]: ValueNode<DoSignaturePrimitive> S_Prim_inner_len, [1]: x_shape}
#   4: @L_mindspore_nn_layer_basic_Dense_construct_905:CNode_782{[0]: ValueNode<DoSignaturePrimitive> S_Prim_not_equal, [1]: CNode_781, [2]: ValueNode<Int64Imm> 2}
#   5: @L_mindspore_nn_layer_basic_Dense_construct_905:CNode_783{[0]: ValueNode<Primitive> Cond, [1]: CNode_782, [2]: ValueNode<BoolImm> false}
#   6: @L_mindspore_nn_layer_basic_Dense_construct_905:CNode_784{[0]: ValueNode<Primitive> Switch, [1]: CNode_783, [2]: ValueNode<FuncGraph> L_✓mindspore_nn_layer_basic_Dense_construct_993, [3]: ValueNode<FuncGraph> L_✗mindspore_nn_layer_basic_Dense_construct_994}
#   7: @L_mindspore_nn_layer_basic_Dense_construct_905:CNode_787{[0]: CNode_784}
#   8: @L_mindspore_nn_layer_basic_Dense_construct_905:CNode_788{[0]: ValueNode<FuncGraph> L_↓mindspore_nn_layer_basic_Dense_construct_995, [1]: CNode_787}
#   9: @L_mindspore_nn_layer_basic_Dense_construct_905:CNode_789{[0]: ValueNode<Primitive> Depend, [1]: CNode_788, [2]: CNode_780}
#  10: @L_mindspore_nn_layer_basic_Dense_construct_905:CNode_790{[0]: ValueNode<Primitive> Return, [1]: CNode_789}


subgraph attr:
training : 1
subgraph instance: ✓mindspore_nn_layer_basic_Dropout_construct_915 : 0x309146e18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:190/    def construct(self, x):/
subgraph @✓mindspore_nn_layer_basic_Dropout_construct_915 parent: [subgraph @mindspore_nn_layer_basic_Dropout_construct_862]() {
  Return(%para50_x)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp2-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:192/            return x/
}
# Order:
#   1: @✓mindspore_nn_layer_basic_Dropout_construct_915:CNode_996{[0]: ValueNode<Primitive> Return, [1]: param_x}


subgraph attr:
training : 1
subgraph instance: ✗mindspore_nn_layer_basic_Dropout_construct_916 : 0x309146218
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:190/    def construct(self, x):/
subgraph @✗mindspore_nn_layer_basic_Dropout_construct_916 parent: [subgraph @mindspore_nn_layer_basic_Dropout_construct_862]() {
  %1(CNode_998) = call @↓mindspore_nn_layer_basic_Dropout_construct_997()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp2-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp2-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @✗mindspore_nn_layer_basic_Dropout_construct_916:CNode_998{[0]: ValueNode<FuncGraph> ↓mindspore_nn_layer_basic_Dropout_construct_997}
#   2: @✗mindspore_nn_layer_basic_Dropout_construct_916:CNode_999{[0]: ValueNode<Primitive> Return, [1]: CNode_998}


subgraph attr:
training : 1
subgraph instance: ↰mindspore_nn_layer_basic_Dropout_construct_910 : 0x309145c18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:190/    def construct(self, x):/
subgraph @↰mindspore_nn_layer_basic_Dropout_construct_910 parent: [subgraph @mindspore_nn_layer_basic_Dropout_construct_862]() {
  %1(CNode_907) = $(mindspore_nn_layer_basic_Dropout_construct_862):S_Prim_logical_not(Bool(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp2-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp2-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @↰mindspore_nn_layer_basic_Dropout_construct_910:CNode_1000{[0]: ValueNode<Primitive> Return, [1]: CNode_907}


subgraph attr:
training : 1
subgraph instance: ↱mindspore_nn_layer_basic_Dropout_construct_911 : 0x309144a18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:190/    def construct(self, x):/
subgraph @↱mindspore_nn_layer_basic_Dropout_construct_911() {
  %1(CNode_1001) = S_Prim_equal(F32(0.5), I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp2-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %2(CNode_1002) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp2-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %3(CNode_1003) = Switch(%2, @↰↱mindspore_nn_layer_basic_Dropout_construct_1004, @2↱mindspore_nn_layer_basic_Dropout_construct_1005)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp2-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %4(CNode_1006) = %3()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp2-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp2-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @↱mindspore_nn_layer_basic_Dropout_construct_911:CNode_1001{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: ValueNode<FP32Imm> 0.5, [2]: ValueNode<Int64Imm> 1}
#   2: @↱mindspore_nn_layer_basic_Dropout_construct_911:CNode_1002{[0]: ValueNode<Primitive> Cond, [1]: CNode_1001, [2]: ValueNode<BoolImm> false}
#   3: @↱mindspore_nn_layer_basic_Dropout_construct_911:CNode_1003{[0]: ValueNode<Primitive> Switch, [1]: CNode_1002, [2]: ValueNode<FuncGraph> ↰↱mindspore_nn_layer_basic_Dropout_construct_1004, [3]: ValueNode<FuncGraph> 2↱mindspore_nn_layer_basic_Dropout_construct_1005}
#   4: @↱mindspore_nn_layer_basic_Dropout_construct_911:CNode_1006{[0]: CNode_1003}
#   5: @↱mindspore_nn_layer_basic_Dropout_construct_911:CNode_1007{[0]: ValueNode<Primitive> Return, [1]: CNode_1006}


subgraph attr:
training : 1
subgraph instance: ✓mindspore_nn_layer_basic_Dropout_construct_931 : 0x309143218
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:190/    def construct(self, x):/
subgraph @✓mindspore_nn_layer_basic_Dropout_construct_931 parent: [subgraph @mindspore_nn_layer_basic_Dropout_construct_860]() {
  Return(%para53_x)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp1-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:192/            return x/
}
# Order:
#   1: @✓mindspore_nn_layer_basic_Dropout_construct_931:CNode_1008{[0]: ValueNode<Primitive> Return, [1]: param_x}


subgraph attr:
training : 1
subgraph instance: ✗mindspore_nn_layer_basic_Dropout_construct_932 : 0x309142618
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:190/    def construct(self, x):/
subgraph @✗mindspore_nn_layer_basic_Dropout_construct_932 parent: [subgraph @mindspore_nn_layer_basic_Dropout_construct_860]() {
  %1(CNode_1010) = call @↓mindspore_nn_layer_basic_Dropout_construct_1009()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp1-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp1-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @✗mindspore_nn_layer_basic_Dropout_construct_932:CNode_1010{[0]: ValueNode<FuncGraph> ↓mindspore_nn_layer_basic_Dropout_construct_1009}
#   2: @✗mindspore_nn_layer_basic_Dropout_construct_932:CNode_1011{[0]: ValueNode<Primitive> Return, [1]: CNode_1010}


subgraph attr:
training : 1
subgraph instance: ↰mindspore_nn_layer_basic_Dropout_construct_926 : 0x309142018
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:190/    def construct(self, x):/
subgraph @↰mindspore_nn_layer_basic_Dropout_construct_926 parent: [subgraph @mindspore_nn_layer_basic_Dropout_construct_860]() {
  %1(CNode_923) = $(mindspore_nn_layer_basic_Dropout_construct_860):S_Prim_logical_not(Bool(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp1-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp1-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @↰mindspore_nn_layer_basic_Dropout_construct_926:CNode_1012{[0]: ValueNode<Primitive> Return, [1]: CNode_923}


subgraph attr:
training : 1
subgraph instance: ↱mindspore_nn_layer_basic_Dropout_construct_927 : 0x309140e18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:190/    def construct(self, x):/
subgraph @↱mindspore_nn_layer_basic_Dropout_construct_927() {
  %1(CNode_1013) = S_Prim_equal(F32(0.5), I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp1-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %2(CNode_1014) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp1-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %3(CNode_1015) = Switch(%2, @↰↱mindspore_nn_layer_basic_Dropout_construct_1016, @2↱mindspore_nn_layer_basic_Dropout_construct_1017)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp1-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %4(CNode_1018) = %3()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp1-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp1-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @↱mindspore_nn_layer_basic_Dropout_construct_927:CNode_1013{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: ValueNode<FP32Imm> 0.5, [2]: ValueNode<Int64Imm> 1}
#   2: @↱mindspore_nn_layer_basic_Dropout_construct_927:CNode_1014{[0]: ValueNode<Primitive> Cond, [1]: CNode_1013, [2]: ValueNode<BoolImm> false}
#   3: @↱mindspore_nn_layer_basic_Dropout_construct_927:CNode_1015{[0]: ValueNode<Primitive> Switch, [1]: CNode_1014, [2]: ValueNode<FuncGraph> ↰↱mindspore_nn_layer_basic_Dropout_construct_1016, [3]: ValueNode<FuncGraph> 2↱mindspore_nn_layer_basic_Dropout_construct_1017}
#   4: @↱mindspore_nn_layer_basic_Dropout_construct_927:CNode_1018{[0]: CNode_1015}
#   5: @↱mindspore_nn_layer_basic_Dropout_construct_927:CNode_1019{[0]: ValueNode<Primitive> Return, [1]: CNode_1018}


subgraph attr:
training : 1
subgraph instance: check_axis_valid_941 : 0x309115818
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @check_axis_valid_941(%para62_axis, %para63_ndim) {
  %1(CNode_1020) = S_Prim_negative(%para63_ndim)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  %2(CNode_1021) = S_Prim_less(%para62_axis, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  %3(CNode_1022) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  %4(CNode_1023) = Switch(%3, @↰check_axis_valid_1024, @↱check_axis_valid_1025)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  %5(CNode_1026) = %4()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  %6(CNode_1027) = Cond(%5, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  %7(CNode_1028) = Switch(%6, @✓check_axis_valid_1029, @✗check_axis_valid_1030)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  %8(CNode_1031) = %7()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @check_axis_valid_941:CNode_1020{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: param_ndim}
#   2: @check_axis_valid_941:CNode_1021{[0]: ValueNode<DoSignaturePrimitive> S_Prim_less, [1]: param_axis, [2]: CNode_1020}
#   3: @check_axis_valid_941:CNode_1022{[0]: ValueNode<Primitive> Cond, [1]: CNode_1021, [2]: ValueNode<BoolImm> false}
#   4: @check_axis_valid_941:CNode_1023{[0]: ValueNode<Primitive> Switch, [1]: CNode_1022, [2]: ValueNode<FuncGraph> ↰check_axis_valid_1024, [3]: ValueNode<FuncGraph> ↱check_axis_valid_1025}
#   5: @check_axis_valid_941:CNode_1026{[0]: CNode_1023}
#   6: @check_axis_valid_941:CNode_1027{[0]: ValueNode<Primitive> Cond, [1]: CNode_1026, [2]: ValueNode<BoolImm> false}
#   7: @check_axis_valid_941:CNode_1028{[0]: ValueNode<Primitive> Switch, [1]: CNode_1027, [2]: ValueNode<FuncGraph> ✓check_axis_valid_1029, [3]: ValueNode<FuncGraph> ✗check_axis_valid_1030}
#   8: @check_axis_valid_941:CNode_1031{[0]: CNode_1028}
#   9: @check_axis_valid_941:CNode_1032{[0]: ValueNode<Primitive> Return, [1]: CNode_1031}


subgraph attr:
subgraph instance: rank_935 : 0x3090fe818
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1541/def rank(input_x):/
subgraph @rank_935(%para64_input_x) {
  %1(CNode_1033) = S_Prim_Rank(%para64_input_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1571/    return rank_(input_x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1571/    return rank_(input_x)/
}
# Order:
#   1: @rank_935:CNode_1033{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Rank, [1]: param_input_x}
#   2: @rank_935:CNode_1034{[0]: ValueNode<Primitive> Return, [1]: CNode_1033}


subgraph attr:
training : 1
subgraph instance: ↰mindspore_nn_layer_basic_Flatten_construct_939 : 0x30911ce18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
subgraph @↰mindspore_nn_layer_basic_Flatten_construct_939 parent: [subgraph @mindspore_nn_layer_basic_Flatten_construct_858]() {
  %1(x_rank) = $(mindspore_nn_layer_basic_Flatten_construct_858):call @rank_935(%para55_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:462/        x_rank = F.rank(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
}
# Order:
#   1: @↰mindspore_nn_layer_basic_Flatten_construct_939:CNode_1035{[0]: ValueNode<Primitive> Return, [1]: x_rank}


subgraph attr:
training : 1
subgraph instance: ↱mindspore_nn_layer_basic_Flatten_construct_940 : 0x3090fee18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
subgraph @↱mindspore_nn_layer_basic_Flatten_construct_940() {
  Return(I64(1))
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
}
# Order:
#   1: @↱mindspore_nn_layer_basic_Flatten_construct_940:CNode_1036{[0]: ValueNode<Primitive> Return, [1]: ValueNode<Int64Imm> 1}


subgraph attr:
subgraph instance: flatten_951 : 0x3090f7618
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_951(%para65_input, %para66_order, %para67_start_dim, %para68_end_dim) {
  %1(CNode_1037) = S_Prim_isinstance(%para65_input, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1731/    if not isinstance(input, Tensor):/
  %2(CNode_1038) = S_Prim_logical_not(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1731/    if not isinstance(input, Tensor):/
  %3(CNode_1039) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1731/    if not isinstance(input, Tensor):/
  %4(CNode_1040) = Switch(%3, @✓flatten_1041, @✗flatten_1042)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1731/    if not isinstance(input, Tensor):/
  %5(CNode_1043) = %4()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1731/    if not isinstance(input, Tensor):/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1731/    if not isinstance(input, Tensor):/
}
# Order:
#   1: @flatten_951:CNode_1037{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: param_input, [2]: ValueNode<ClassType> class 'mindspore.common.tensor.Tensor'}
#   2: @flatten_951:CNode_1038{[0]: ValueNode<DoSignaturePrimitive> S_Prim_logical_not, [1]: CNode_1037}
#   3: @flatten_951:CNode_1039{[0]: ValueNode<Primitive> Cond, [1]: CNode_1038, [2]: ValueNode<BoolImm> false}
#   4: @flatten_951:CNode_1040{[0]: ValueNode<Primitive> Switch, [1]: CNode_1039, [2]: ValueNode<FuncGraph> ✓flatten_1041, [3]: ValueNode<FuncGraph> ✗flatten_1042}
#   5: @flatten_951:CNode_1043{[0]: CNode_1040}
#   6: @flatten_951:CNode_1044{[0]: ValueNode<Primitive> Return, [1]: CNode_1043}


subgraph attr:
training : 1
subgraph instance: ✓mindspore_nn_layer_pooling_MaxPool2d_construct_959 : 0x309114c18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @✓mindspore_nn_layer_pooling_MaxPool2d_construct_959 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_856]() {
  %1(CNode_1045) = getattr(%para56_x, "unsqueeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:571/            x = x.unsqueeze(0)/
  %2(x) = %1(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:571/            x = x.unsqueeze(0)/
  %3(CNode_1047) = call @↓mindspore_nn_layer_pooling_MaxPool2d_construct_1046(%2, Bool(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /var/folders/n4/p41p3m3n5c574nhxjsj9n7tc0000gn/T/ipykernel_8186/286663031.py:23/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:571/            x = x.unsqueeze(0)/
}
# Order:
#   1: @✓mindspore_nn_layer_pooling_MaxPool2d_construct_959:CNode_1045{[0]: ValueNode<Primitive> getattr, [1]: param_x, [2]: ValueNode<StringImm> unsqueeze}
#   2: @✓mindspore_nn_layer_pooling_MaxPool2d_construct_959:x{[0]: CNode_1045, [1]: ValueNode<Int64Imm> 0}
#   3: @✓mindspore_nn_layer_pooling_MaxPool2d_construct_959:CNode_1048{[0]: ValueNode<Primitive> Return, [1]: CNode_1047}
#   4: @✓mindspore_nn_layer_pooling_MaxPool2d_construct_959:CNode_1047{[0]: ValueNode<FuncGraph> ↓mindspore_nn_layer_pooling_MaxPool2d_construct_1046, [1]: x, [2]: ValueNode<BoolImm> true}


subgraph attr:
training : 1
subgraph instance: ✗mindspore_nn_layer_pooling_MaxPool2d_construct_960 : 0x309109818
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @✗mindspore_nn_layer_pooling_MaxPool2d_construct_960 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_856]() {
  %1(CNode_1049) = call @↓mindspore_nn_layer_pooling_MaxPool2d_construct_1046(%para56_x, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /var/folders/n4/p41p3m3n5c574nhxjsj9n7tc0000gn/T/ipykernel_8186/286663031.py:23/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:570/        if x.ndim == 3:/
}
# Order:
#   1: @✗mindspore_nn_layer_pooling_MaxPool2d_construct_960:CNode_1050{[0]: ValueNode<Primitive> Return, [1]: CNode_1049}
#   2: @✗mindspore_nn_layer_pooling_MaxPool2d_construct_960:CNode_1049{[0]: ValueNode<FuncGraph> ↓mindspore_nn_layer_pooling_MaxPool2d_construct_1046, [1]: param_x, [2]: ValueNode<BoolImm> false}


subgraph attr:
training : 1
subgraph instance: ✗mindspore_nn_layer_conv_Conv2d_construct_963 : 0x309107018
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:360/    def construct(self, x):/
subgraph @✗mindspore_nn_layer_conv_Conv2d_construct_963 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_857]() {
  %1(CNode_1052) = call @↓mindspore_nn_layer_conv_Conv2d_construct_1051()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv2-Conv2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv2-Conv2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @✗mindspore_nn_layer_conv_Conv2d_construct_963:CNode_1052{[0]: ValueNode<FuncGraph> ↓mindspore_nn_layer_conv_Conv2d_construct_1051}
#   2: @✗mindspore_nn_layer_conv_Conv2d_construct_963:CNode_1053{[0]: ValueNode<Primitive> Return, [1]: CNode_1052}


subgraph attr:
training : 1
subgraph instance: ✗mindspore_nn_layer_conv_Conv2d_construct_966 : 0x3090ee018
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:360/    def construct(self, x):/
subgraph @✗mindspore_nn_layer_conv_Conv2d_construct_966 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_854]() {
  %1(CNode_1055) = call @↓mindspore_nn_layer_conv_Conv2d_construct_1054()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv1-Conv2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv1-Conv2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @✗mindspore_nn_layer_conv_Conv2d_construct_966:CNode_1055{[0]: ValueNode<FuncGraph> ↓mindspore_nn_layer_conv_Conv2d_construct_1054}
#   2: @✗mindspore_nn_layer_conv_Conv2d_construct_966:CNode_1056{[0]: ValueNode<Primitive> Return, [1]: CNode_1055}


subgraph attr:
training : 1
skip_auto_parallel_compile : 1
subgraph instance: ↓mindspore_nn_optim_momentum_Momentum_construct_974 : 0x3090e3a18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:215/    def construct(self, gradients):/
subgraph @↓mindspore_nn_optim_momentum_Momentum_construct_974 parent: [subgraph @2✗mindspore_nn_optim_momentum_Momentum_construct_843]() {
  %1(lr) = $(mindspore_nn_optim_momentum_Momentum_construct_812):call @get_lr_822()
      #scope: (Default/optimizer-Momentum)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:222/        lr = self.get_lr()/
  %2(CNode_882) = $(2✗mindspore_nn_optim_momentum_Momentum_construct_843):S_Prim_Partial[side_effect_propagate: I64(1)](S_Prim_momentum_opt, S_Prim_ApplyMomentum[output_names: ["output"], side_effect_mem: Bool(1), use_nesterov: Bool(0), input_names: ["variable", "accumulation", "learning_rate", "gradient", "momentum"], use_locking: Bool(0), gradient_scale: F32(1)], %para20_momentum, %1)
      : (<null>, <null>, <Ref[Tensor[Float32]], (), ref_key=:momentum>, <null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:238/                success = self.hyper_map_reverse(F.partial(_momentum_opt, self.opt, self.momentum, lr),/
  %3(gradients) = $(mindspore_nn_optim_momentum_Momentum_construct_812):call @flatten_gradients_818(%para37_gradients)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:218/        gradients = self.flatten_gradients(gradients)/
  %4(gradients) = $(mindspore_nn_optim_momentum_Momentum_construct_812):call @decay_weight_819(%3)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:219/        gradients = self.decay_weight(gradients)/
  %5(gradients) = $(mindspore_nn_optim_momentum_Momentum_construct_812):call @gradients_centralization_820(%4)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:220/        gradients = self.gradients_centralization(gradients)/
  %6(gradients) = $(mindspore_nn_optim_momentum_Momentum_construct_812):call @scale_grad_821(%5)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:221/        gradients = self.scale_grad(gradients)/
  %7(CNode_883) = $(mindspore_nn_optim_momentum_Momentum_construct_812):MakeTuple(%para3_conv1.weight, %para4_conv2.weight, %para5_fc1.weight, %para6_fc1.bias, %para7_fc2.weight, %para8_fc2.bias, %para9_fc3.weight, %para10_fc3.bias)
      : (<Ref[Tensor[Float32]], (6, 1, 3, 3), ref_key=:conv1.weight>, <Ref[Tensor[Float32]], (16, 6, 3, 3), ref_key=:conv2.weight>, <Ref[Tensor[Float32]], (120, 784), ref_key=:fc1.weight>, <Ref[Tensor[Float32]], (120), ref_key=:fc1.bias>, <Ref[Tensor[Float32]], (84, 120), ref_key=:fc2.weight>, <Ref[Tensor[Float32]], (84), ref_key=:fc2.bias>, <Ref[Tensor[Float32]], (10, 84), ref_key=:fc3.weight>, <Ref[Tensor[Float32]], (10), ref_key=:fc3.bias>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:216/        params = self.params/
  %8(CNode_884) = $(mindspore_nn_optim_momentum_Momentum_construct_812):MakeTuple(%para12_moments.conv1.weight, %para13_moments.conv2.weight, %para14_moments.fc1.weight, %para15_moments.fc1.bias, %para16_moments.fc2.weight, %para17_moments.fc2.bias, %para18_moments.fc3.weight, %para19_moments.fc3.bias)
      : (<Ref[Tensor[Float32]], (6, 1, 3, 3), ref_key=:moments.conv1.weight>, <Ref[Tensor[Float32]], (16, 6, 3, 3), ref_key=:moments.conv2.weight>, <Ref[Tensor[Float32]], (120, 784), ref_key=:moments.fc1.weight>, <Ref[Tensor[Float32]], (120), ref_key=:moments.fc1.bias>, <Ref[Tensor[Float32]], (84, 120), ref_key=:moments.fc2.weight>, <Ref[Tensor[Float32]], (84), ref_key=:moments.fc2.bias>, <Ref[Tensor[Float32]], (10, 84), ref_key=:moments.fc3.weight>, <Ref[Tensor[Float32]], (10), ref_key=:moments.fc3.bias>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:217/        moments = self.moments/
  %9(success) = $(2✗mindspore_nn_optim_momentum_Momentum_construct_843):S_Prim_hyper_map(%2, %6, %7, %8, (Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0)), (Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0)))
      : (<null>, <null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:238/                success = self.hyper_map_reverse(F.partial(_momentum_opt, self.opt, self.momentum, lr),/
  Return(%9)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/optim/momentum.py:240/        return success/
}
# Order:
#   1: @↓mindspore_nn_optim_momentum_Momentum_construct_974:CNode_1057{[0]: ValueNode<Primitive> Return, [1]: success}


subgraph attr:
after_block : 1
training : 1
subgraph instance: ↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_978 : 0x30914bc18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:777/    def construct(self, logits, labels):/
subgraph @↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_978 parent: [subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_826]() {
  %1(CNode_1059) = call @↓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1058()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:781/            if self.reduction == 'mean':/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:781/            if self.reduction == 'mean':/
}
# Order:
#   1: @↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_978:CNode_1060{[0]: ValueNode<FuncGraph> shape_1061, [1]: param_logits}
#   2: @↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_978:CNode_1062{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   3: @↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_978:CNode_1063{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1060, [2]: CNode_1062}
#   4: @↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_978:labels{[0]: ValueNode<DoSignaturePrimitive> S_Prim_OneHot, [1]: param_labels, [2]: CNode_1063, [3]: ValueNode<Tensor> Tensor(shape=[], dtype=Float32, value=1), [4]: ValueNode<Tensor> Tensor(shape=[], dtype=Float32, value=0)}
#   5: @↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_978:CNode_1059{[0]: ValueNode<FuncGraph> ↓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1058}
#   6: @↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_978:CNode_1064{[0]: ValueNode<Primitive> Return, [1]: CNode_1059}


subgraph attr:
after_block : 1
training : 1
subgraph instance: ↓mindspore_nn_layer_basic_Dropout_construct_982 : 0x309149e18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:190/    def construct(self, x):/
subgraph @↓mindspore_nn_layer_basic_Dropout_construct_982 parent: [subgraph @mindspore_nn_layer_basic_Dropout_construct_864]() {
  %1(CNode_1065) = S_Prim_Dropout[side_effect_hidden: Bool(1), Seed0: I64(0), keep_prob: F32(0.9), Seed1: I64(0)](%para48_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:194/        out, _ = self.dropout(x)/
  %2(out) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:194/        out, _ = self.dropout(x)/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:195/        return out/
}
# Order:
#   1: @↓mindspore_nn_layer_basic_Dropout_construct_982:CNode_1065{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Dropout, [1]: param_x}
#   2: @↓mindspore_nn_layer_basic_Dropout_construct_982:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1065, [2]: ValueNode<Int64Imm> 0}
#   3: @↓mindspore_nn_layer_basic_Dropout_construct_982:CNode_1066{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: ↰↱mindspore_nn_layer_basic_Dropout_construct_989 : 0x309148c18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:190/    def construct(self, x):/
subgraph @↰↱mindspore_nn_layer_basic_Dropout_construct_989 parent: [subgraph @↱mindspore_nn_layer_basic_Dropout_construct_897]() {
  %1(CNode_986) = $(↱mindspore_nn_layer_basic_Dropout_construct_897):S_Prim_equal(F32(0.5), I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @↰↱mindspore_nn_layer_basic_Dropout_construct_989:CNode_1067{[0]: ValueNode<Primitive> Return, [1]: CNode_986}


subgraph attr:
training : 1
subgraph instance: 2↱mindspore_nn_layer_basic_Dropout_construct_990 : 0x309148618
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:190/    def construct(self, x):/
subgraph @2↱mindspore_nn_layer_basic_Dropout_construct_990() {
  %1(CNode_1068) = S_Prim_equal(F32(0.1), I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @2↱mindspore_nn_layer_basic_Dropout_construct_990:CNode_1068{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: ValueNode<FP32Imm> 0.1, [2]: ValueNode<Int64Imm> 0}
#   2: @2↱mindspore_nn_layer_basic_Dropout_construct_990:CNode_1069{[0]: ValueNode<Primitive> Return, [1]: CNode_1068}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_↓mindspore_nn_layer_basic_Dense_construct_995 : 0x30913d218
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_↓mindspore_nn_layer_basic_Dense_construct_995 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_905](%para69_) {
  %1(CNode_791) = call @L_✓↓mindspore_nn_layer_basic_Dense_construct_1070()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:628/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:628/        if self.has_bias:/
}
# Order:
#   1: @L_↓mindspore_nn_layer_basic_Dense_construct_995:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MatMul, [1]: param_фx, [2]: param_L_fc3.weight}
#   2: @L_↓mindspore_nn_layer_basic_Dense_construct_995:CNode_791{[0]: ValueNode<FuncGraph> L_✓↓mindspore_nn_layer_basic_Dense_construct_1070}
#   3: @L_↓mindspore_nn_layer_basic_Dense_construct_995:CNode_792{[0]: ValueNode<Primitive> Return, [1]: CNode_791}


subgraph attr:
training : 1
subgraph instance: L_✓mindspore_nn_layer_basic_Dense_construct_993 : 0x30913cc18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_✓mindspore_nn_layer_basic_Dense_construct_993 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_905]() {
  %1(CNode_1071) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
  %2(x_shape) = $(L_mindspore_nn_layer_basic_Dense_construct_905):S_Prim_Shape(%para59_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:623/        x_shape = self.shape_op(x)/
  %3(CNode_1072) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
  %4(CNode_1073) = S_Prim_getitem(%2, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
  %5(CNode_1074) = S_Prim_MakeTuple(%1, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
  %6(x) = S_Prim_Reshape[output_names: ["output"], input_names: ["tensor", "shape"]](%para59_x, %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
}
# Order:
#   1: @L_✓mindspore_nn_layer_basic_Dense_construct_993:CNode_1071{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @L_✓mindspore_nn_layer_basic_Dense_construct_993:CNode_1072{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   3: @L_✓mindspore_nn_layer_basic_Dense_construct_993:CNode_1073{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: x_shape, [2]: CNode_1072}
#   4: @L_✓mindspore_nn_layer_basic_Dense_construct_993:CNode_1074{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_1071, [2]: CNode_1073}
#   5: @L_✓mindspore_nn_layer_basic_Dense_construct_993:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: param_x, [2]: CNode_1074}
#   6: @L_✓mindspore_nn_layer_basic_Dense_construct_993:CNode_1075{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
training : 1
subgraph instance: L_✗mindspore_nn_layer_basic_Dense_construct_994 : 0x30911e018
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_✗mindspore_nn_layer_basic_Dense_construct_994 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_905]() {
  Return(%para59_x)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @L_✗mindspore_nn_layer_basic_Dense_construct_994:CNode_1076{[0]: ValueNode<Primitive> Return, [1]: param_x}


subgraph attr:
after_block : 1
training : 1
subgraph instance: ↓mindspore_nn_layer_basic_Dropout_construct_997 : 0x309146818
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:190/    def construct(self, x):/
subgraph @↓mindspore_nn_layer_basic_Dropout_construct_997 parent: [subgraph @mindspore_nn_layer_basic_Dropout_construct_862]() {
  %1(CNode_1077) = S_Prim_Dropout[side_effect_hidden: Bool(1), Seed0: I64(0), keep_prob: F32(0.5), Seed1: I64(0)](%para50_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp2-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:194/        out, _ = self.dropout(x)/
  %2(out) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp2-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:194/        out, _ = self.dropout(x)/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp2-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:195/        return out/
}
# Order:
#   1: @↓mindspore_nn_layer_basic_Dropout_construct_997:CNode_1077{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Dropout, [1]: param_x}
#   2: @↓mindspore_nn_layer_basic_Dropout_construct_997:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1077, [2]: ValueNode<Int64Imm> 0}
#   3: @↓mindspore_nn_layer_basic_Dropout_construct_997:CNode_1078{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: ↰↱mindspore_nn_layer_basic_Dropout_construct_1004 : 0x309145618
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:190/    def construct(self, x):/
subgraph @↰↱mindspore_nn_layer_basic_Dropout_construct_1004 parent: [subgraph @↱mindspore_nn_layer_basic_Dropout_construct_911]() {
  %1(CNode_1001) = $(↱mindspore_nn_layer_basic_Dropout_construct_911):S_Prim_equal(F32(0.5), I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp2-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp2-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @↰↱mindspore_nn_layer_basic_Dropout_construct_1004:CNode_1079{[0]: ValueNode<Primitive> Return, [1]: CNode_1001}


subgraph attr:
training : 1
subgraph instance: 2↱mindspore_nn_layer_basic_Dropout_construct_1005 : 0x309145018
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:190/    def construct(self, x):/
subgraph @2↱mindspore_nn_layer_basic_Dropout_construct_1005() {
  %1(CNode_1080) = S_Prim_equal(F32(0.5), I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp2-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp2-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @2↱mindspore_nn_layer_basic_Dropout_construct_1005:CNode_1080{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: ValueNode<FP32Imm> 0.5, [2]: ValueNode<Int64Imm> 0}
#   2: @2↱mindspore_nn_layer_basic_Dropout_construct_1005:CNode_1081{[0]: ValueNode<Primitive> Return, [1]: CNode_1080}


subgraph attr:
after_block : 1
training : 1
subgraph instance: ↓mindspore_nn_layer_basic_Dropout_construct_1009 : 0x309142c18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:190/    def construct(self, x):/
subgraph @↓mindspore_nn_layer_basic_Dropout_construct_1009 parent: [subgraph @mindspore_nn_layer_basic_Dropout_construct_860]() {
  %1(CNode_1082) = S_Prim_Dropout[side_effect_hidden: Bool(1), Seed0: I64(0), keep_prob: F32(0.5), Seed1: I64(0)](%para53_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp1-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:194/        out, _ = self.dropout(x)/
  %2(out) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp1-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:194/        out, _ = self.dropout(x)/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp1-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:195/        return out/
}
# Order:
#   1: @↓mindspore_nn_layer_basic_Dropout_construct_1009:CNode_1082{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Dropout, [1]: param_x}
#   2: @↓mindspore_nn_layer_basic_Dropout_construct_1009:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1082, [2]: ValueNode<Int64Imm> 0}
#   3: @↓mindspore_nn_layer_basic_Dropout_construct_1009:CNode_1083{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: ↰↱mindspore_nn_layer_basic_Dropout_construct_1016 : 0x309141a18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:190/    def construct(self, x):/
subgraph @↰↱mindspore_nn_layer_basic_Dropout_construct_1016 parent: [subgraph @↱mindspore_nn_layer_basic_Dropout_construct_927]() {
  %1(CNode_1013) = $(↱mindspore_nn_layer_basic_Dropout_construct_927):S_Prim_equal(F32(0.5), I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp1-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp1-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @↰↱mindspore_nn_layer_basic_Dropout_construct_1016:CNode_1084{[0]: ValueNode<Primitive> Return, [1]: CNode_1013}


subgraph attr:
training : 1
subgraph instance: 2↱mindspore_nn_layer_basic_Dropout_construct_1017 : 0x309141418
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:190/    def construct(self, x):/
subgraph @2↱mindspore_nn_layer_basic_Dropout_construct_1017() {
  %1(CNode_1085) = S_Prim_equal(F32(0.5), I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp1-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/dp1-Dropout)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @2↱mindspore_nn_layer_basic_Dropout_construct_1017:CNode_1085{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: ValueNode<FP32Imm> 0.5, [2]: ValueNode<Int64Imm> 0}
#   2: @2↱mindspore_nn_layer_basic_Dropout_construct_1017:CNode_1086{[0]: ValueNode<Primitive> Return, [1]: CNode_1085}


subgraph attr:
training : 1
subgraph instance: ✓check_axis_valid_1029 : 0x3090ed818
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @✓check_axis_valid_1029() {
  %1(CNode_1087) = raise[side_effect_io: Bool(1)]("ValueError", "'start_dim' or 'end_dim' out of range.", "None")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:459/            raise ValueError("'start_dim' or 'end_dim' out of range.")/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:459/            raise ValueError("'start_dim' or 'end_dim' out of range.")/
}
# Order:
#   1: @✓check_axis_valid_1029:CNode_1087{[0]: ValueNode<Primitive> raise, [1]: ValueNode<StringImm> ValueError, [2]: ValueNode<StringImm> 'start_dim' or 'end_dim' out of range., [3]: ValueNode<StringImm> None}
#   2: @✓check_axis_valid_1029:CNode_1088{[0]: ValueNode<Primitive> Return, [1]: CNode_1087}


subgraph attr:
training : 1
subgraph instance: ✗check_axis_valid_1030 : 0x3090fd618
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @✗check_axis_valid_1030() {
  %1(CNode_1090) = call @↓check_axis_valid_1089()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @✗check_axis_valid_1030:CNode_1090{[0]: ValueNode<FuncGraph> ↓check_axis_valid_1089}
#   2: @✗check_axis_valid_1030:CNode_1091{[0]: ValueNode<Primitive> Return, [1]: CNode_1090}


subgraph attr:
training : 1
subgraph instance: ↰check_axis_valid_1024 : 0x3090fd018
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @↰check_axis_valid_1024 parent: [subgraph @check_axis_valid_941]() {
  %1(CNode_1020) = $(check_axis_valid_941):S_Prim_negative(%para63_ndim)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  %2(CNode_1021) = $(check_axis_valid_941):S_Prim_less(%para62_axis, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @↰check_axis_valid_1024:CNode_1092{[0]: ValueNode<Primitive> Return, [1]: CNode_1021}


subgraph attr:
training : 1
subgraph instance: ↱check_axis_valid_1025 : 0x3090fca18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @↱check_axis_valid_1025 parent: [subgraph @check_axis_valid_941]() {
  %1(CNode_1093) = S_Prim_greater_equal(%para62_axis, %para63_ndim)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:458/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @↱check_axis_valid_1025:CNode_1093{[0]: ValueNode<DoSignaturePrimitive> S_Prim_greater_equal, [1]: param_axis, [2]: param_ndim}
#   2: @↱check_axis_valid_1025:CNode_1094{[0]: ValueNode<Primitive> Return, [1]: CNode_1093}


subgraph attr:
subgraph instance: ✓flatten_1041 : 0x3090fe218
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✓flatten_1041() {
  %1(CNode_1095) = JoinedStr("For 'flatten', argument 'input' must be Tensor.")
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1732/        raise TypeError(f"For 'flatten', argument 'input' must be Tensor.")/
  %2(CNode_1096) = raise[side_effect_io: Bool(1)]("TypeError", %1, "None")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1732/        raise TypeError(f"For 'flatten', argument 'input' must be Tensor.")/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1732/        raise TypeError(f"For 'flatten', argument 'input' must be Tensor.")/
}
# Order:
#   1: @✓flatten_1041:CNode_1095{[0]: ValueNode<Primitive> JoinedStr, [1]: ValueNode<StringImm> For 'flatten', argument 'input' must be Tensor.}
#   2: @✓flatten_1041:CNode_1096{[0]: ValueNode<Primitive> raise, [1]: ValueNode<StringImm> TypeError, [2]: CNode_1095, [3]: ValueNode<StringImm> None}
#   3: @✓flatten_1041:CNode_1097{[0]: ValueNode<Primitive> Return, [1]: CNode_1096}


subgraph attr:
subgraph instance: ✗flatten_1042 : 0x309109218
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✗flatten_1042 parent: [subgraph @flatten_951]() {
  %1(CNode_1099) = call @↓flatten_1098()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1731/    if not isinstance(input, Tensor):/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1731/    if not isinstance(input, Tensor):/
}
# Order:
#   1: @✗flatten_1042:CNode_1099{[0]: ValueNode<FuncGraph> ↓flatten_1098}
#   2: @✗flatten_1042:CNode_1100{[0]: ValueNode<Primitive> Return, [1]: CNode_1099}


subgraph attr:
after_block : 1
training : 1
subgraph instance: ↓mindspore_nn_layer_pooling_MaxPool2d_construct_1046 : 0x309109e18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @↓mindspore_nn_layer_pooling_MaxPool2d_construct_1046(%para70_, %para71_) {
  %1(CNode_1102) = call @✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_1101()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:573/        if self.use_pad:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:573/        if self.use_pad:/
}
# Order:
#   1: @↓mindspore_nn_layer_pooling_MaxPool2d_construct_1046:CNode_1102{[0]: ValueNode<FuncGraph> ✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_1101}
#   2: @↓mindspore_nn_layer_pooling_MaxPool2d_construct_1046:CNode_1103{[0]: ValueNode<Primitive> Return, [1]: CNode_1102}


subgraph attr:
training : 1
subgraph instance: ↓mindspore_nn_layer_conv_Conv2d_construct_1051 : 0x309107618
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:360/    def construct(self, x):/
subgraph @↓mindspore_nn_layer_conv_Conv2d_construct_1051 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_857]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_857):S_Prim_Conv2D[kernel_size: (I64(3), I64(3)), mode: I64(1), out_channel: I64(16), input_names: ["x", "w"], pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), format: "NCHW", pad_list: (I64(1), I64(1), I64(1), I64(1)), groups: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), group: I64(1), dilation: (I64(1), I64(1), I64(1), I64(1)), output_names: ["output"]](%para57_x, %para4_conv2.weight)
      : (<null>, <Ref[Tensor[Float32]], (16, 6, 3, 3), ref_key=:conv2.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv2-Conv2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv2-Conv2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:364/        return output/
}
# Order:
#   1: @↓mindspore_nn_layer_conv_Conv2d_construct_1051:CNode_1104{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: ↓mindspore_nn_layer_conv_Conv2d_construct_1054 : 0x309106418
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:360/    def construct(self, x):/
subgraph @↓mindspore_nn_layer_conv_Conv2d_construct_1054 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_854]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_854):S_Prim_Conv2D[kernel_size: (I64(3), I64(3)), mode: I64(1), out_channel: I64(6), input_names: ["x", "w"], pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), format: "NCHW", pad_list: (I64(1), I64(1), I64(1), I64(1)), groups: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), group: I64(1), dilation: (I64(1), I64(1), I64(1), I64(1)), output_names: ["output"]](%para58_x, %para3_conv1.weight)
      : (<null>, <Ref[Tensor[Float32]], (6, 1, 3, 3), ref_key=:conv1.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv1-Conv2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv1-Conv2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/conv.py:364/        return output/
}
# Order:
#   1: @↓mindspore_nn_layer_conv_Conv2d_construct_1054:CNode_1105{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
subgraph instance: shape_1061 : 0x30914c218
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1484/def shape(input_x):/
subgraph @shape_1061(%para72_input_x) {
  %1(CNode_1106) = S_Prim_Shape(%para72_input_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1510/    return shape_(input_x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1510/    return shape_(input_x)/
}
# Order:
#   1: @shape_1061:CNode_1106{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_input_x}
#   2: @shape_1061:CNode_1107{[0]: ValueNode<Primitive> Return, [1]: CNode_1106}


subgraph attr:
training : 1
subgraph instance: ↓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1058 : 0x30914c818
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:777/    def construct(self, logits, labels):/
subgraph @↓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1058 parent: [subgraph @↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_978]() {
  %1(CNode_1060) = $(↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_978):call @shape_1061(%para45_logits)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:784/            labels = self.one_hot(labels, F.shape(logits)[-1], self.on_value, self.off_value)/
  %2(CNode_1062) = $(↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_978):S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:784/            labels = self.one_hot(labels, F.shape(logits)[-1], self.on_value, self.off_value)/
  %3(CNode_1063) = $(↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_978):S_Prim_getitem(%1, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:784/            labels = self.one_hot(labels, F.shape(logits)[-1], self.on_value, self.off_value)/
  %4(labels) = $(↓✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_978):S_Prim_OneHot[output_names: ["output"], input_names: ["indices", "depth", "on_value", "off_value"], axis: I64(-1)](%para46_labels, %3, Tensor(shape=[], dtype=Float32, value=1), Tensor(shape=[], dtype=Float32, value=0))
      : (<null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:784/            labels = self.one_hot(labels, F.shape(logits)[-1], self.on_value, self.off_value)/
  %5(CNode_1108) = S_Prim_SoftmaxCrossEntropyWithLogits(%para45_logits, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:785/        x = self.softmax_cross_entropy(logits, labels)[0]/
  %6(x) = S_Prim_getitem(%5, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:785/        x = self.softmax_cross_entropy(logits, labels)[0]/
  %7(CNode_1110) = call @get_loss_1109(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:786/        return self.get_loss(x)/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:786/        return self.get_loss(x)/
}
# Order:
#   1: @↓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1058:CNode_1108{[0]: ValueNode<DoSignaturePrimitive> S_Prim_SoftmaxCrossEntropyWithLogits, [1]: param_logits, [2]: labels}
#   2: @↓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1058:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1108, [2]: ValueNode<Int64Imm> 0}
#   3: @↓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1058:CNode_1110{[0]: ValueNode<FuncGraph> get_loss_1109, [1]: x}
#   4: @↓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1058:CNode_1111{[0]: ValueNode<Primitive> Return, [1]: CNode_1110}


subgraph attr:
training : 1
subgraph instance: L_✓↓mindspore_nn_layer_basic_Dense_construct_1070 : 0x30913d818
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_✓↓mindspore_nn_layer_basic_Dense_construct_1070 parent: [subgraph @L_↓mindspore_nn_layer_basic_Dense_construct_995]() {
  %1(CNode_793) = call @L_2↓mindspore_nn_layer_basic_Dense_construct_1112()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:629/            x = self.bias_add(x, self.bias)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:629/            x = self.bias_add(x, self.bias)/
}
# Order:
#   1: @L_✓↓mindspore_nn_layer_basic_Dense_construct_1070:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BiasAdd, [1]: x, [2]: param_L_fc3.bias}
#   2: @L_✓↓mindspore_nn_layer_basic_Dense_construct_1070:CNode_793{[0]: ValueNode<FuncGraph> L_2↓mindspore_nn_layer_basic_Dense_construct_1112}
#   3: @L_✓↓mindspore_nn_layer_basic_Dense_construct_1070:CNode_794{[0]: ValueNode<Primitive> Return, [1]: CNode_793}


subgraph attr:
after_block : 1
training : 1
subgraph instance: ↓check_axis_valid_1089 : 0x3090fdc18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @↓check_axis_valid_1089() {
  Return(None)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:457/    def check_axis_valid(self, axis, ndim):/
}
# Order:
#   1: @↓check_axis_valid_1089:CNode_1113{[0]: ValueNode<Primitive> Return, [1]: ValueNode<None> None}


subgraph attr:
after_block : 1
subgraph instance: ↓flatten_1098 : 0x309108218
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @↓flatten_1098 parent: [subgraph @flatten_951]() {
  %1(CNode_1114) = S_Prim_isinstance(%para67_start_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %2(CNode_1115) = S_Prim_logical_not(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %3(CNode_1116) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %4(CNode_1117) = Switch(%3, @↰↓flatten_1118, @↱↓flatten_1119)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %5(CNode_1120) = %4()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %6(CNode_1121) = Cond(%5, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %7(CNode_1122) = Switch(%6, @✓↓flatten_1123, @✗↓flatten_1124)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %8(CNode_1125) = %7()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @↓flatten_1098:CNode_1114{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: param_start_dim, [2]: ValueNode<ClassType> class 'int'}
#   2: @↓flatten_1098:CNode_1115{[0]: ValueNode<DoSignaturePrimitive> S_Prim_logical_not, [1]: CNode_1114}
#   3: @↓flatten_1098:CNode_1116{[0]: ValueNode<Primitive> Cond, [1]: CNode_1115, [2]: ValueNode<BoolImm> false}
#   4: @↓flatten_1098:CNode_1117{[0]: ValueNode<Primitive> Switch, [1]: CNode_1116, [2]: ValueNode<FuncGraph> ↰↓flatten_1118, [3]: ValueNode<FuncGraph> ↱↓flatten_1119}
#   5: @↓flatten_1098:CNode_1120{[0]: CNode_1117}
#   6: @↓flatten_1098:CNode_1121{[0]: ValueNode<Primitive> Cond, [1]: CNode_1120, [2]: ValueNode<BoolImm> false}
#   7: @↓flatten_1098:CNode_1122{[0]: ValueNode<Primitive> Switch, [1]: CNode_1121, [2]: ValueNode<FuncGraph> ✓↓flatten_1123, [3]: ValueNode<FuncGraph> ✗↓flatten_1124}
#   8: @↓flatten_1098:CNode_1125{[0]: CNode_1122}
#   9: @↓flatten_1098:CNode_1126{[0]: ValueNode<Primitive> Return, [1]: CNode_1125}


subgraph attr:
training : 1
subgraph instance: ✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_1101 : 0x30910a418
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_1101 parent: [subgraph @↓mindspore_nn_layer_pooling_MaxPool2d_construct_1046]() {
  %1(CNode_1128) = call @2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1127()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:581/            out = self.max_pool(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:581/            out = self.max_pool(x)/
}
# Order:
#   1: @✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_1101:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MaxPool, [1]: param_фx}
#   2: @✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_1101:CNode_1128{[0]: ValueNode<FuncGraph> 2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1127}
#   3: @✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_1101:CNode_1129{[0]: ValueNode<Primitive> Return, [1]: CNode_1128}


subgraph attr:
training : 1
subgraph instance: get_loss_1109 : 0x30914ce18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:125/    def get_loss(self, x, weights=1.0):/
subgraph @get_loss_1109(%para73_x, %para74_weights) {
  %1(CNode_1131) = call @✓get_loss_1130()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:143/        if self.reduce and self.average:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:143/        if self.reduce and self.average:/
}
# Order:
#   1: @get_loss_1109:input_dtype{[0]: ValueNode<Primitive> getattr, [1]: param_x, [2]: ValueNode<StringImm> dtype}
#   2: @get_loss_1109:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Cast, [1]: param_x, [2]: ValueNode<Float> Float32}
#   3: @get_loss_1109:weights{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Cast, [1]: param_weights, [2]: ValueNode<Float> Float32}
#   4: @get_loss_1109:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Mul, [1]: weights, [2]: x}
#   5: @get_loss_1109:CNode_1131{[0]: ValueNode<FuncGraph> ✓get_loss_1130}
#   6: @get_loss_1109:CNode_1132{[0]: ValueNode<Primitive> Return, [1]: CNode_1131}


subgraph attr:
training : 1
subgraph instance: L_2↓mindspore_nn_layer_basic_Dense_construct_1112 : 0x30913de18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_2↓mindspore_nn_layer_basic_Dense_construct_1112 parent: [subgraph @L_✓↓mindspore_nn_layer_basic_Dense_construct_1070]() {
  %1(CNode_795) = call @L_✗2↓mindspore_nn_layer_basic_Dense_construct_1133()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:630/        if self.activation_flag:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:630/        if self.activation_flag:/
}
# Order:
#   1: @L_2↓mindspore_nn_layer_basic_Dense_construct_1112:CNode_795{[0]: ValueNode<FuncGraph> L_✗2↓mindspore_nn_layer_basic_Dense_construct_1133}
#   2: @L_2↓mindspore_nn_layer_basic_Dense_construct_1112:CNode_796{[0]: ValueNode<Primitive> Return, [1]: CNode_795}


subgraph attr:
subgraph instance: ✓↓flatten_1123 : 0x3090ece18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✓↓flatten_1123() {
  %1(CNode_1134) = JoinedStr("For 'flatten', both 'start_dim' and 'end_dim' must be int.")
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1735/        raise TypeError(f"For 'flatten', both 'start_dim' and 'end_dim' must be int.")/
  %2(CNode_1135) = raise[side_effect_io: Bool(1)]("TypeError", %1, "None")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1735/        raise TypeError(f"For 'flatten', both 'start_dim' and 'end_dim' must be int.")/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1735/        raise TypeError(f"For 'flatten', both 'start_dim' and 'end_dim' must be int.")/
}
# Order:
#   1: @✓↓flatten_1123:CNode_1134{[0]: ValueNode<Primitive> JoinedStr, [1]: ValueNode<StringImm> For 'flatten', both 'start_dim' and 'end_dim' must be int.}
#   2: @✓↓flatten_1123:CNode_1135{[0]: ValueNode<Primitive> raise, [1]: ValueNode<StringImm> TypeError, [2]: CNode_1134, [3]: ValueNode<StringImm> None}
#   3: @✓↓flatten_1123:CNode_1136{[0]: ValueNode<Primitive> Return, [1]: CNode_1135}


subgraph attr:
subgraph instance: ✗↓flatten_1124 : 0x309120a18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✗↓flatten_1124 parent: [subgraph @flatten_951]() {
  %1(CNode_1138) = call @2↓flatten_1137()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @✗↓flatten_1124:CNode_1138{[0]: ValueNode<FuncGraph> 2↓flatten_1137}
#   2: @✗↓flatten_1124:CNode_1139{[0]: ValueNode<Primitive> Return, [1]: CNode_1138}


subgraph attr:
subgraph instance: ↰↓flatten_1118 : 0x309120418
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @↰↓flatten_1118 parent: [subgraph @↓flatten_1098]() {
  %1(CNode_1114) = $(↓flatten_1098):S_Prim_isinstance(%para67_start_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %2(CNode_1115) = $(↓flatten_1098):S_Prim_logical_not(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @↰↓flatten_1118:CNode_1140{[0]: ValueNode<Primitive> Return, [1]: CNode_1115}


subgraph attr:
subgraph instance: ↱↓flatten_1119 : 0x30911e618
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @↱↓flatten_1119 parent: [subgraph @flatten_951]() {
  %1(CNode_1141) = S_Prim_isinstance(%para68_end_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %2(CNode_1142) = S_Prim_logical_not(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %3(CNode_1143) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  %4(CNode_1144) = Switch(%3, @↰↱↓flatten_1145, @2↱↓flatten_1146)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  %5(CNode_1147) = %4()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @↱↓flatten_1119:CNode_1141{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: param_end_dim, [2]: ValueNode<ClassType> class 'int'}
#   2: @↱↓flatten_1119:CNode_1142{[0]: ValueNode<DoSignaturePrimitive> S_Prim_logical_not, [1]: CNode_1141}
#   3: @↱↓flatten_1119:CNode_1143{[0]: ValueNode<Primitive> Cond, [1]: CNode_1142, [2]: ValueNode<BoolImm> false}
#   4: @↱↓flatten_1119:CNode_1144{[0]: ValueNode<Primitive> Switch, [1]: CNode_1143, [2]: ValueNode<FuncGraph> ↰↱↓flatten_1145, [3]: ValueNode<FuncGraph> 2↱↓flatten_1146}
#   5: @↱↓flatten_1119:CNode_1147{[0]: CNode_1144}
#   6: @↱↓flatten_1119:CNode_1148{[0]: ValueNode<Primitive> Return, [1]: CNode_1147}


subgraph attr:
training : 1
subgraph instance: 2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1127 : 0x30910b818
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1127 parent: [subgraph @✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_1101]() {
  %1(CNode_1149) = Cond(%para71_фexpand_batch, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:582/        if expand_batch:/
  %2(CNode_1150) = Switch(%1, @✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1151, @✗2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1152)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:582/        if expand_batch:/
  %3(CNode_1153) = %2()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:582/        if expand_batch:/
  %4(CNode_1155) = call @3↓mindspore_nn_layer_pooling_MaxPool2d_construct_1154(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /var/folders/n4/p41p3m3n5c574nhxjsj9n7tc0000gn/T/ipykernel_8186/286663031.py:23/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:582/        if expand_batch:/
}
# Order:
#   1: @2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1127:CNode_1149{[0]: ValueNode<Primitive> Cond, [1]: param_фexpand_batch, [2]: ValueNode<BoolImm> false}
#   2: @2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1127:CNode_1150{[0]: ValueNode<Primitive> Switch, [1]: CNode_1149, [2]: ValueNode<FuncGraph> ✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1151, [3]: ValueNode<FuncGraph> ✗2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1152}
#   3: @2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1127:CNode_1153{[0]: CNode_1150}
#   4: @2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1127:CNode_1155{[0]: ValueNode<FuncGraph> 3↓mindspore_nn_layer_pooling_MaxPool2d_construct_1154, [1]: CNode_1153}
#   5: @2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1127:CNode_1156{[0]: ValueNode<Primitive> Return, [1]: CNode_1155}


subgraph attr:
training : 1
subgraph instance: ✓get_loss_1130 : 0x30911c618
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:125/    def get_loss(self, x, weights=1.0):/
subgraph @✓get_loss_1130 parent: [subgraph @get_loss_1109]() {
  %1(CNode_1158) = call @↓get_loss_1157()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:144/            x = self.reduce_mean(x, self.get_axis(x))/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:144/            x = self.reduce_mean(x, self.get_axis(x))/
}
# Order:
#   1: @✓get_loss_1130:CNode_1159{[0]: ValueNode<FuncGraph> get_axis_1160, [1]: x}
#   2: @✓get_loss_1130:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReduceMean, [1]: x, [2]: CNode_1159}
#   3: @✓get_loss_1130:CNode_1158{[0]: ValueNode<FuncGraph> ↓get_loss_1157}
#   4: @✓get_loss_1130:CNode_1161{[0]: ValueNode<Primitive> Return, [1]: CNode_1158}


subgraph attr:
training : 1
subgraph instance: L_✗2↓mindspore_nn_layer_basic_Dense_construct_1133 : 0x30913e418
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_✗2↓mindspore_nn_layer_basic_Dense_construct_1133 parent: [subgraph @L_✓↓mindspore_nn_layer_basic_Dense_construct_1070]() {
  %1(CNode_797) = call @L_3↓mindspore_nn_layer_basic_Dense_construct_1162()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:630/        if self.activation_flag:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:630/        if self.activation_flag:/
}
# Order:
#   1: @L_✗2↓mindspore_nn_layer_basic_Dense_construct_1133:CNode_797{[0]: ValueNode<FuncGraph> L_3↓mindspore_nn_layer_basic_Dense_construct_1162}
#   2: @L_✗2↓mindspore_nn_layer_basic_Dense_construct_1133:CNode_798{[0]: ValueNode<Primitive> Return, [1]: CNode_797}


subgraph attr:
after_block : 1
subgraph instance: 2↓flatten_1137 : 0x309121818
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @2↓flatten_1137 parent: [subgraph @flatten_951]() {
  %1(CNode_1163) = S_Prim_check_flatten_order[constexpr_prim: Bool(1)](%para66_order)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1736/    check_flatten_order_const(order)/
  %2(CNode_1164) = StopGradient(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
  %3(CNode_1165) = S_Prim_equal(%para66_order, "F")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1737/    if order == 'F':/
  %4(CNode_1166) = Cond(%3, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1737/    if order == 'F':/
  %5(CNode_1167) = Switch(%4, @✓2↓flatten_1168, @✗2↓flatten_1169)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1737/    if order == 'F':/
  %6(CNode_1170) = %5()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1737/    if order == 'F':/
  %7(CNode_1171) = Depend[side_effect_propagate: I64(1)](%6, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1737/    if order == 'F':/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1737/    if order == 'F':/
}
# Order:
#   1: @2↓flatten_1137:CNode_1163{[0]: ValueNode<DoSignaturePrimitive> S_Prim_check_flatten_order, [1]: param_order}
#   2: @2↓flatten_1137:CNode_1165{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: param_order, [2]: ValueNode<StringImm> F}
#   3: @2↓flatten_1137:CNode_1166{[0]: ValueNode<Primitive> Cond, [1]: CNode_1165, [2]: ValueNode<BoolImm> false}
#   4: @2↓flatten_1137:CNode_1167{[0]: ValueNode<Primitive> Switch, [1]: CNode_1166, [2]: ValueNode<FuncGraph> ✓2↓flatten_1168, [3]: ValueNode<FuncGraph> ✗2↓flatten_1169}
#   5: @2↓flatten_1137:CNode_1170{[0]: CNode_1167}
#   6: @2↓flatten_1137:CNode_1172{[0]: ValueNode<Primitive> Return, [1]: CNode_1171}


subgraph attr:
subgraph instance: ↰↱↓flatten_1145 : 0x30911fe18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @↰↱↓flatten_1145 parent: [subgraph @↱↓flatten_1119]() {
  %1(CNode_1141) = $(↱↓flatten_1119):S_Prim_isinstance(%para68_end_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %2(CNode_1142) = $(↱↓flatten_1119):S_Prim_logical_not(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
}
# Order:
#   1: @↰↱↓flatten_1145:CNode_1173{[0]: ValueNode<Primitive> Return, [1]: CNode_1142}


subgraph attr:
subgraph instance: 2↱↓flatten_1146 : 0x30911ec18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @2↱↓flatten_1146 parent: [subgraph @flatten_951]() {
  %1(CNode_1174) = S_Prim_isinstance(%para67_start_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  %2(CNode_1175) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  %3(CNode_1176) = Switch(%2, @↰2↱↓flatten_1177, @3↱↓flatten_1178)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  %4(CNode_1179) = %3()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @2↱↓flatten_1146:CNode_1174{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: param_start_dim, [2]: ValueNode<ClassType> class 'bool'}
#   2: @2↱↓flatten_1146:CNode_1175{[0]: ValueNode<Primitive> Cond, [1]: CNode_1174, [2]: ValueNode<BoolImm> false}
#   3: @2↱↓flatten_1146:CNode_1176{[0]: ValueNode<Primitive> Switch, [1]: CNode_1175, [2]: ValueNode<FuncGraph> ↰2↱↓flatten_1177, [3]: ValueNode<FuncGraph> 3↱↓flatten_1178}
#   4: @2↱↓flatten_1146:CNode_1179{[0]: CNode_1176}
#   5: @2↱↓flatten_1146:CNode_1180{[0]: ValueNode<Primitive> Return, [1]: CNode_1179}


subgraph attr:
after_block : 1
training : 1
subgraph instance: 3↓mindspore_nn_layer_pooling_MaxPool2d_construct_1154 : 0x309113a18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @3↓mindspore_nn_layer_pooling_MaxPool2d_construct_1154(%para75_) {
  %1(CNode_1182) = call @✗3↓mindspore_nn_layer_pooling_MaxPool2d_construct_1181()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:587/        if self.use_pad and not self.return_indices:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:587/        if self.use_pad and not self.return_indices:/
}
# Order:
#   1: @3↓mindspore_nn_layer_pooling_MaxPool2d_construct_1154:CNode_1182{[0]: ValueNode<FuncGraph> ✗3↓mindspore_nn_layer_pooling_MaxPool2d_construct_1181}
#   2: @3↓mindspore_nn_layer_pooling_MaxPool2d_construct_1154:CNode_1183{[0]: ValueNode<Primitive> Return, [1]: CNode_1182}


subgraph attr:
training : 1
subgraph instance: ✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1151 : 0x30910c418
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1151 parent: [subgraph @✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_1101]() {
  %1(out) = $(✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_1101):S_Prim_MaxPool[pad_mode: I64(2), output_names: ["output"], kernel_size: (I64(1), I64(1), I64(2), I64(2)), format: "NCHW", strides: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"]](%para70_фx)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:581/            out = self.max_pool(x)/
  %2(CNode_1184) = S_Prim_isinstance(%1, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:583/            if isinstance(out, tuple):/
  %3(CNode_1185) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:583/            if isinstance(out, tuple):/
  %4(CNode_1186) = Switch(%3, @2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1187, @✗✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1188)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:583/            if isinstance(out, tuple):/
  %5(CNode_1189) = %4()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:583/            if isinstance(out, tuple):/
  %6(CNode_1191) = call @↓✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1190(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /var/folders/n4/p41p3m3n5c574nhxjsj9n7tc0000gn/T/ipykernel_8186/286663031.py:23/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:583/            if isinstance(out, tuple):/
}
# Order:
#   1: @✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1151:CNode_1184{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: out, [2]: ValueNode<ClassType> class 'tuple'}
#   2: @✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1151:CNode_1185{[0]: ValueNode<Primitive> Cond, [1]: CNode_1184, [2]: ValueNode<BoolImm> false}
#   3: @✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1151:CNode_1186{[0]: ValueNode<Primitive> Switch, [1]: CNode_1185, [2]: ValueNode<FuncGraph> 2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1187, [3]: ValueNode<FuncGraph> ✗✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1188}
#   4: @✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1151:CNode_1189{[0]: CNode_1186}
#   5: @✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1151:CNode_1191{[0]: ValueNode<FuncGraph> ↓✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1190, [1]: CNode_1189}
#   6: @✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1151:CNode_1192{[0]: ValueNode<Primitive> Return, [1]: CNode_1191}


subgraph attr:
training : 1
subgraph instance: ✗2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1152 : 0x30910be18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @✗2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1152 parent: [subgraph @✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_1101]() {
  %1(out) = $(✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_1101):S_Prim_MaxPool[pad_mode: I64(2), output_names: ["output"], kernel_size: (I64(1), I64(1), I64(2), I64(2)), format: "NCHW", strides: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"]](%para70_фx)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:581/            out = self.max_pool(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:582/        if expand_batch:/
}
# Order:
#   1: @✗2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1152:CNode_1193{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: get_axis_1160 : 0x30914d418
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:113/    def get_axis(self, x):/
subgraph @get_axis_1160(%para76_x) {
  %1(shape) = call @shape_1061(%para76_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:120/        shape = F.shape(x)/
  %2(length) = S_Prim_sequence_len(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:121/        length = F.tuple_len(shape)/
  %3(perm) = S_Prim_make_range(I64(0), %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:122/        perm = F.make_range(0, length)/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:123/        return perm/
}
# Order:
#   1: @get_axis_1160:shape{[0]: ValueNode<FuncGraph> shape_1061, [1]: param_x}
#   2: @get_axis_1160:length{[0]: ValueNode<DoSignaturePrimitive> S_Prim_sequence_len, [1]: shape}
#   3: @get_axis_1160:perm{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_range, [1]: ValueNode<Int64Imm> 0, [2]: length}
#   4: @get_axis_1160:CNode_1194{[0]: ValueNode<Primitive> Return, [1]: perm}


subgraph attr:
training : 1
subgraph instance: ↓get_loss_1157 : 0x30914da18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:125/    def get_loss(self, x, weights=1.0):/
subgraph @↓get_loss_1157 parent: [subgraph @✓get_loss_1130]() {
  %1(CNode_1196) = call @✗↓get_loss_1195()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:145/        if self.reduce and not self.average:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:145/        if self.reduce and not self.average:/
}
# Order:
#   1: @↓get_loss_1157:CNode_1196{[0]: ValueNode<FuncGraph> ✗↓get_loss_1195}
#   2: @↓get_loss_1157:CNode_1197{[0]: ValueNode<Primitive> Return, [1]: CNode_1196}


subgraph attr:
training : 1
subgraph instance: L_3↓mindspore_nn_layer_basic_Dense_construct_1162 : 0x30913ea18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_3↓mindspore_nn_layer_basic_Dense_construct_1162 parent: [subgraph @L_✓↓mindspore_nn_layer_basic_Dense_construct_1070]() {
  %1(x_shape) = $(L_mindspore_nn_layer_basic_Dense_construct_905):S_Prim_Shape(%para59_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:623/        x_shape = self.shape_op(x)/
  %2(CNode_799) = S_Prim_inner_len(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/
  %3(CNode_800) = S_Prim_not_equal(%2, I64(2))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/
  %4(CNode_801) = Cond(%3, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/
  %5(CNode_802) = Switch(%4, @L_✓3↓mindspore_nn_layer_basic_Dense_construct_1198, @L_✗3↓mindspore_nn_layer_basic_Dense_construct_1199)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/
  %6(CNode_804) = %5()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/
  %7(CNode_806) = call @L_4↓mindspore_nn_layer_basic_Dense_construct_1200(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file /var/folders/n4/p41p3m3n5c574nhxjsj9n7tc0000gn/T/ipykernel_8186/286663031.py:31/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/
}
# Order:
#   1: @L_3↓mindspore_nn_layer_basic_Dense_construct_1162:CNode_799{[0]: ValueNode<DoSignaturePrimitive> S_Prim_inner_len, [1]: x_shape}
#   2: @L_3↓mindspore_nn_layer_basic_Dense_construct_1162:CNode_800{[0]: ValueNode<DoSignaturePrimitive> S_Prim_not_equal, [1]: CNode_799, [2]: ValueNode<Int64Imm> 2}
#   3: @L_3↓mindspore_nn_layer_basic_Dense_construct_1162:CNode_801{[0]: ValueNode<Primitive> Cond, [1]: CNode_800, [2]: ValueNode<BoolImm> false}
#   4: @L_3↓mindspore_nn_layer_basic_Dense_construct_1162:CNode_802{[0]: ValueNode<Primitive> Switch, [1]: CNode_801, [2]: ValueNode<FuncGraph> L_✓3↓mindspore_nn_layer_basic_Dense_construct_1198, [3]: ValueNode<FuncGraph> L_✗3↓mindspore_nn_layer_basic_Dense_construct_1199}
#   5: @L_3↓mindspore_nn_layer_basic_Dense_construct_1162:CNode_804{[0]: CNode_802}
#   6: @L_3↓mindspore_nn_layer_basic_Dense_construct_1162:CNode_806{[0]: ValueNode<FuncGraph> L_4↓mindspore_nn_layer_basic_Dense_construct_1200, [1]: CNode_804}
#   7: @L_3↓mindspore_nn_layer_basic_Dense_construct_1162:CNode_807{[0]: ValueNode<Primitive> Return, [1]: CNode_806}


subgraph attr:
subgraph instance: ✓2↓flatten_1168 : 0x309131e18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✓2↓flatten_1168 parent: [subgraph @flatten_951]() {
  %1(x_rank) = S_Prim_Rank(%para65_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1738/        x_rank = rank_(input)/
  %2(CNode_1201) = S_Prim_MakeTuple(I64(0), I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1740/        if x_rank in (0, 1):/
  %3(CNode_1202) = S_Prim_in(%1, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1740/        if x_rank in (0, 1):/
  %4(CNode_1203) = Cond(%3, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1740/        if x_rank in (0, 1):/
  %5(CNode_1204) = Switch(%4, @2✓2↓flatten_1205, @✗✓2↓flatten_1206)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1740/        if x_rank in (0, 1):/
  %6(CNode_1207) = %5()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1740/        if x_rank in (0, 1):/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1740/        if x_rank in (0, 1):/
}
# Order:
#   1: @✓2↓flatten_1168:x_rank{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Rank, [1]: param_input}
#   2: @✓2↓flatten_1168:CNode_1201{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: ValueNode<Int64Imm> 0, [2]: ValueNode<Int64Imm> 1}
#   3: @✓2↓flatten_1168:CNode_1202{[0]: ValueNode<DoSignaturePrimitive> S_Prim_in, [1]: x_rank, [2]: CNode_1201}
#   4: @✓2↓flatten_1168:CNode_1203{[0]: ValueNode<Primitive> Cond, [1]: CNode_1202, [2]: ValueNode<BoolImm> false}
#   5: @✓2↓flatten_1168:CNode_1204{[0]: ValueNode<Primitive> Switch, [1]: CNode_1203, [2]: ValueNode<FuncGraph> 2✓2↓flatten_1205, [3]: ValueNode<FuncGraph> ✗✓2↓flatten_1206}
#   6: @✓2↓flatten_1168:CNode_1207{[0]: CNode_1204}
#   7: @✓2↓flatten_1168:CNode_1208{[0]: ValueNode<Primitive> Return, [1]: CNode_1207}


subgraph attr:
subgraph instance: ✗2↓flatten_1169 : 0x309122618
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✗2↓flatten_1169 parent: [subgraph @flatten_951]() {
  %1(CNode_1210) = call @3↓flatten_1209(%para65_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1737/    if order == 'F':/
}
# Order:
#   1: @✗2↓flatten_1169:CNode_1211{[0]: ValueNode<Primitive> Return, [1]: CNode_1210}
#   2: @✗2↓flatten_1169:CNode_1210{[0]: ValueNode<FuncGraph> 3↓flatten_1209, [1]: param_input}


subgraph attr:
subgraph instance: ↰2↱↓flatten_1177 : 0x30911f818
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @↰2↱↓flatten_1177 parent: [subgraph @2↱↓flatten_1146]() {
  %1(CNode_1174) = $(2↱↓flatten_1146):S_Prim_isinstance(%para67_start_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
}
# Order:
#   1: @↰2↱↓flatten_1177:CNode_1212{[0]: ValueNode<Primitive> Return, [1]: CNode_1174}


subgraph attr:
subgraph instance: 3↱↓flatten_1178 : 0x30911f218
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @3↱↓flatten_1178 parent: [subgraph @flatten_951]() {
  %1(CNode_1213) = S_Prim_isinstance(%para68_end_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @3↱↓flatten_1178:CNode_1213{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: param_end_dim, [2]: ValueNode<ClassType> class 'bool'}
#   2: @3↱↓flatten_1178:CNode_1214{[0]: ValueNode<Primitive> Return, [1]: CNode_1213}


subgraph attr:
training : 1
subgraph instance: ✗3↓mindspore_nn_layer_pooling_MaxPool2d_construct_1181 : 0x309114018
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @✗3↓mindspore_nn_layer_pooling_MaxPool2d_construct_1181 parent: [subgraph @3↓mindspore_nn_layer_pooling_MaxPool2d_construct_1154]() {
  %1(CNode_1216) = call @4↓mindspore_nn_layer_pooling_MaxPool2d_construct_1215()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:587/        if self.use_pad and not self.return_indices:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:587/        if self.use_pad and not self.return_indices:/
}
# Order:
#   1: @✗3↓mindspore_nn_layer_pooling_MaxPool2d_construct_1181:CNode_1216{[0]: ValueNode<FuncGraph> 4↓mindspore_nn_layer_pooling_MaxPool2d_construct_1215}
#   2: @✗3↓mindspore_nn_layer_pooling_MaxPool2d_construct_1181:CNode_1217{[0]: ValueNode<Primitive> Return, [1]: CNode_1216}


subgraph attr:
after_block : 1
training : 1
subgraph instance: ↓✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1190 : 0x3090e4a18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @↓✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1190(%para77_) {
  Return(%para77_фout)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:583/            if isinstance(out, tuple):/
}
# Order:
#   1: @↓✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1190:CNode_1218{[0]: ValueNode<Primitive> Return, [1]: param_фout}


subgraph attr:
training : 1
subgraph instance: 2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1187 : 0x30910d018
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1187 parent: [subgraph @✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_1101]() {
  %1(out) = $(✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_1101):S_Prim_MaxPool[pad_mode: I64(2), output_names: ["output"], kernel_size: (I64(1), I64(1), I64(2), I64(2)), format: "NCHW", strides: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"]](%para70_фx)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:581/            out = self.max_pool(x)/
  %2(CNode_1219) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %3(CNode_1220) = getattr(%2, "squeeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %4(CNode_1221) = %3(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %5(CNode_1222) = S_Prim_getitem(%1, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %6(CNode_1223) = getattr(%5, "squeeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %7(CNode_1224) = %6(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %8(out) = S_Prim_MakeTuple(%4, %7)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
}
# Order:
#   1: @2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1187:CNode_1219{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: out, [2]: ValueNode<Int64Imm> 0}
#   2: @2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1187:CNode_1220{[0]: ValueNode<Primitive> getattr, [1]: CNode_1219, [2]: ValueNode<StringImm> squeeze}
#   3: @2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1187:CNode_1221{[0]: CNode_1220, [1]: ValueNode<Int64Imm> 0}
#   4: @2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1187:CNode_1222{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: out, [2]: ValueNode<Int64Imm> 1}
#   5: @2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1187:CNode_1223{[0]: ValueNode<Primitive> getattr, [1]: CNode_1222, [2]: ValueNode<StringImm> squeeze}
#   6: @2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1187:CNode_1224{[0]: CNode_1223, [1]: ValueNode<Int64Imm> 0}
#   7: @2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1187:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_1221, [2]: CNode_1224}
#   8: @2✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1187:CNode_1225{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: ✗✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1188 : 0x30910ca18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @✗✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1188 parent: [subgraph @✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_1101]() {
  %1(out) = $(✗↓mindspore_nn_layer_pooling_MaxPool2d_construct_1101):S_Prim_MaxPool[pad_mode: I64(2), output_names: ["output"], kernel_size: (I64(1), I64(1), I64(2), I64(2)), format: "NCHW", strides: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"]](%para70_фx)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:581/            out = self.max_pool(x)/
  %2(CNode_1226) = getattr(%1, "squeeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:586/                out = out.squeeze(0)/
  %3(out) = %2(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:586/                out = out.squeeze(0)/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:586/                out = out.squeeze(0)/
}
# Order:
#   1: @✗✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1188:CNode_1226{[0]: ValueNode<Primitive> getattr, [1]: out, [2]: ValueNode<StringImm> squeeze}
#   2: @✗✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1188:out{[0]: CNode_1226, [1]: ValueNode<Int64Imm> 0}
#   3: @✗✓2↓mindspore_nn_layer_pooling_MaxPool2d_construct_1188:CNode_1227{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: ✗↓get_loss_1195 : 0x30914e018
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:125/    def get_loss(self, x, weights=1.0):/
subgraph @✗↓get_loss_1195 parent: [subgraph @✓get_loss_1130]() {
  %1(CNode_1229) = call @2↓get_loss_1228()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:145/        if self.reduce and not self.average:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:145/        if self.reduce and not self.average:/
}
# Order:
#   1: @✗↓get_loss_1195:CNode_1229{[0]: ValueNode<FuncGraph> 2↓get_loss_1228}
#   2: @✗↓get_loss_1195:CNode_1230{[0]: ValueNode<Primitive> Return, [1]: CNode_1229}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_4↓mindspore_nn_layer_basic_Dense_construct_1200 : 0x309140218
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_4↓mindspore_nn_layer_basic_Dense_construct_1200(%para78_) {
  Return(%para78_фx)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:635/        return x/
}
# Order:
#   1: @L_4↓mindspore_nn_layer_basic_Dense_construct_1200:CNode_1231{[0]: ValueNode<Primitive> Return, [1]: param_фx}


subgraph attr:
training : 1
subgraph instance: L_✓3↓mindspore_nn_layer_basic_Dense_construct_1198 : 0x30913f618
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_✓3↓mindspore_nn_layer_basic_Dense_construct_1198 parent: [subgraph @L_✓↓mindspore_nn_layer_basic_Dense_construct_1070]() {
  %1(x) = $(L_↓mindspore_nn_layer_basic_Dense_construct_995):S_Prim_MatMul[output_names: ["output"], transpose_a: Bool(0), input_names: ["x1", "x2"], transpose_x2: Bool(1), transpose_x1: Bool(0), transpose_b: Bool(1)](%para69_фx, %para61_L_fc3.weight)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:627/        x = self.matmul(x, self.weight)/
  %2(x) = $(L_✓↓mindspore_nn_layer_basic_Dense_construct_1070):S_Prim_BiasAdd[output_names: ["output"], format: "NCHW", input_names: ["x", "b"]](%1, %para60_L_fc3.bias)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:629/            x = self.bias_add(x, self.bias)/
  %3(x_shape) = $(L_mindspore_nn_layer_basic_Dense_construct_905):S_Prim_Shape(%para59_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:623/        x_shape = self.shape_op(x)/
  %4(CNode_1232) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %5(CNode_1233) = S_Prim_make_slice(None, %4, None)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %6(CNode_1234) = S_Prim_getitem(%3, %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %7(CNode_1236) = call @L_shape_1235(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %8(CNode_1237) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %9(CNode_1238) = S_Prim_getitem(%7, %8)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %10(CNode_1239) = S_Prim_MakeTuple(%9)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %11(out_shape) = S_Prim_add(%6, %10)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %12(x) = S_Prim_Reshape[output_names: ["output"], input_names: ["tensor", "shape"]](%2, %11)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:634/            x = self.reshape(x, out_shape)/
  Return(%12)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
}
# Order:
#   1: @L_✓3↓mindspore_nn_layer_basic_Dense_construct_1198:CNode_1232{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @L_✓3↓mindspore_nn_layer_basic_Dense_construct_1198:CNode_1233{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_slice, [1]: ValueNode<None> None, [2]: CNode_1232, [3]: ValueNode<None> None}
#   3: @L_✓3↓mindspore_nn_layer_basic_Dense_construct_1198:CNode_1234{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: x_shape, [2]: CNode_1233}
#   4: @L_✓3↓mindspore_nn_layer_basic_Dense_construct_1198:CNode_1236{[0]: ValueNode<FuncGraph> L_shape_1235, [1]: x}
#   5: @L_✓3↓mindspore_nn_layer_basic_Dense_construct_1198:CNode_1237{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   6: @L_✓3↓mindspore_nn_layer_basic_Dense_construct_1198:CNode_1238{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1236, [2]: CNode_1237}
#   7: @L_✓3↓mindspore_nn_layer_basic_Dense_construct_1198:CNode_1239{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_1238}
#   8: @L_✓3↓mindspore_nn_layer_basic_Dense_construct_1198:out_shape{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: CNode_1234, [2]: CNode_1239}
#   9: @L_✓3↓mindspore_nn_layer_basic_Dense_construct_1198:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: x, [2]: out_shape}
#  10: @L_✓3↓mindspore_nn_layer_basic_Dense_construct_1198:CNode_1240{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
training : 1
subgraph instance: L_✗3↓mindspore_nn_layer_basic_Dense_construct_1199 : 0x30913f018
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/    def construct(self, x):/
subgraph @L_✗3↓mindspore_nn_layer_basic_Dense_construct_1199 parent: [subgraph @L_✓↓mindspore_nn_layer_basic_Dense_construct_1070]() {
  %1(x) = $(L_↓mindspore_nn_layer_basic_Dense_construct_995):S_Prim_MatMul[output_names: ["output"], transpose_a: Bool(0), input_names: ["x1", "x2"], transpose_x2: Bool(1), transpose_x1: Bool(0), transpose_b: Bool(1)](%para69_фx, %para61_L_fc3.weight)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:627/        x = self.matmul(x, self.weight)/
  %2(x) = $(L_✓↓mindspore_nn_layer_basic_Dense_construct_1070):S_Prim_BiasAdd[output_names: ["output"], format: "NCHW", input_names: ["x", "b"]](%1, %para60_L_fc3.bias)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:629/            x = self.bias_add(x, self.bias)/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:632/        if len(x_shape) != 2:/
}
# Order:
#   1: @L_✗3↓mindspore_nn_layer_basic_Dense_construct_1199:CNode_808{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
subgraph instance: 2✓2↓flatten_1205 : 0x3090ec818
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @2✓2↓flatten_1205 parent: [subgraph @flatten_951]() {
  %1(CNode_1241) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1741/            return reshape_(input, (-1,))/
  %2(CNode_1242) = S_Prim_MakeTuple(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1741/            return reshape_(input, (-1,))/
  %3(CNode_1243) = S_Prim_Reshape[output_names: ["output"], input_names: ["tensor", "shape"]](%para65_input, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1741/            return reshape_(input, (-1,))/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1741/            return reshape_(input, (-1,))/
}
# Order:
#   1: @2✓2↓flatten_1205:CNode_1241{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @2✓2↓flatten_1205:CNode_1242{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_1241}
#   3: @2✓2↓flatten_1205:CNode_1243{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: param_input, [2]: CNode_1242}
#   4: @2✓2↓flatten_1205:CNode_1244{[0]: ValueNode<Primitive> Return, [1]: CNode_1243}


subgraph attr:
subgraph instance: ✗✓2↓flatten_1206 : 0x30910f618
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✗✓2↓flatten_1206 parent: [subgraph @✓2↓flatten_1168]() {
  %1(CNode_1246) = call @↓✓2↓flatten_1245()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1740/        if x_rank in (0, 1):/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1740/        if x_rank in (0, 1):/
}
# Order:
#   1: @✗✓2↓flatten_1206:CNode_1246{[0]: ValueNode<FuncGraph> ↓✓2↓flatten_1245}
#   2: @✗✓2↓flatten_1206:CNode_1247{[0]: ValueNode<Primitive> Return, [1]: CNode_1246}


subgraph attr:
after_block : 1
subgraph instance: 3↓flatten_1209 : 0x309123418
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @3↓flatten_1209 parent: [subgraph @flatten_951](%para79_) {
  %1(CNode_1248) = S_Prim_equal(%para67_start_dim, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %2(CNode_1249) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %3(CNode_1250) = Switch(%2, @↰3↓flatten_1251, @↱3↓flatten_1252)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %4(CNode_1253) = %3()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %5(CNode_1254) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %6(CNode_1255) = Switch(%5, @✓3↓flatten_1256, @✗3↓flatten_1257)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %7(CNode_1258) = %6()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
}
# Order:
#   1: @3↓flatten_1209:x_shape{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_фinput}
#   2: @3↓flatten_1209:x_rank{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Rank, [1]: param_фinput}
#   3: @3↓flatten_1209:CNode_1248{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: param_start_dim, [2]: ValueNode<Int64Imm> 1}
#   4: @3↓flatten_1209:CNode_1249{[0]: ValueNode<Primitive> Cond, [1]: CNode_1248, [2]: ValueNode<BoolImm> false}
#   5: @3↓flatten_1209:CNode_1250{[0]: ValueNode<Primitive> Switch, [1]: CNode_1249, [2]: ValueNode<FuncGraph> ↰3↓flatten_1251, [3]: ValueNode<FuncGraph> ↱3↓flatten_1252}
#   6: @3↓flatten_1209:CNode_1253{[0]: CNode_1250}
#   7: @3↓flatten_1209:CNode_1254{[0]: ValueNode<Primitive> Cond, [1]: CNode_1253, [2]: ValueNode<BoolImm> false}
#   8: @3↓flatten_1209:CNode_1255{[0]: ValueNode<Primitive> Switch, [1]: CNode_1254, [2]: ValueNode<FuncGraph> ✓3↓flatten_1256, [3]: ValueNode<FuncGraph> ✗3↓flatten_1257}
#   9: @3↓flatten_1209:CNode_1258{[0]: CNode_1255}
#  10: @3↓flatten_1209:CNode_1259{[0]: ValueNode<Primitive> Return, [1]: CNode_1258}


subgraph attr:
training : 1
subgraph instance: 4↓mindspore_nn_layer_pooling_MaxPool2d_construct_1215 : 0x309114618
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:568/    def construct(self, x):/
subgraph @4↓mindspore_nn_layer_pooling_MaxPool2d_construct_1215 parent: [subgraph @3↓mindspore_nn_layer_pooling_MaxPool2d_construct_1154]() {
  Return(%para75_фout)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/pooling.py:589/        return out/
}
# Order:
#   1: @4↓mindspore_nn_layer_pooling_MaxPool2d_construct_1215:CNode_1260{[0]: ValueNode<Primitive> Return, [1]: param_фout}


subgraph attr:
training : 1
subgraph instance: 2↓get_loss_1228 : 0x30914e618
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:125/    def get_loss(self, x, weights=1.0):/
subgraph @2↓get_loss_1228 parent: [subgraph @✓get_loss_1130]() {
  %1(weights) = $(get_loss_1109):S_Prim_Cast[output_names: ["output"], input_names: ["x", "dst_type"]](%para74_weights, F32)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:141/        weights = self.cast(weights, mstype.float32)/
  %2(x) = $(get_loss_1109):S_Prim_Cast[output_names: ["output"], input_names: ["x", "dst_type"]](%para73_x, F32)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:140/        x = self.cast(x, mstype.float32)/
  %3(x) = $(get_loss_1109):S_Prim_Mul[output_names: ["output"], input_names: ["x", "y"]](%1, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:142/        x = self.mul(weights, x)/
  %4(CNode_1159) = $(✓get_loss_1130):call @get_axis_1160(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:144/            x = self.reduce_mean(x, self.get_axis(x))/
  %5(x) = $(✓get_loss_1130):S_Prim_ReduceMean[output_names: ["y"], keep_dims: Bool(0), input_names: ["input_x", "axis"]](%3, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:144/            x = self.reduce_mean(x, self.get_axis(x))/
  %6(input_dtype) = $(get_loss_1109):getattr(%para73_x, "dtype")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:139/        input_dtype = x.dtype/
  %7(x) = S_Prim_Cast[output_names: ["output"], input_names: ["x", "dst_type"]](%5, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:147/        x = self.cast(x, input_dtype)/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/loss/loss.py:148/        return x/
}
# Order:
#   1: @2↓get_loss_1228:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Cast, [1]: x, [2]: input_dtype}
#   2: @2↓get_loss_1228:CNode_1261{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
subgraph instance: L_shape_1235 : 0x30913fc18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1484/def shape(input_x):/
subgraph @L_shape_1235(%para80_input_x) {
  %1(CNode_1106) = S_Prim_Shape(%para80_input_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1510/    return shape_(input_x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1510/    return shape_(input_x)/
}
# Order:
#   1: @L_shape_1235:CNode_1106{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_input_x}
#   2: @L_shape_1235:CNode_1107{[0]: ValueNode<Primitive> Return, [1]: CNode_1106}


subgraph attr:
after_block : 1
subgraph instance: ↓✓2↓flatten_1245 : 0x3090fc218
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @↓✓2↓flatten_1245 parent: [subgraph @✓2↓flatten_1168]() {
  %1(CNode_1263) = call @_get_cache_prim_1262(ClassType)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1744/        input = _get_cache_prim(P.Transpose)()(input, new_order)/
  %2(CNode_1264) = %1()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1744/        input = _get_cache_prim(P.Transpose)()(input, new_order)/
  %3(x_rank) = $(✓2↓flatten_1168):S_Prim_Rank(%para65_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1738/        x_rank = rank_(input)/
  %4(perm) = S_Prim_make_range(I64(0), %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1742/        perm = ops.make_range(0, x_rank)/
  %5(new_order) = S_Prim_tuple_reversed(%4)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1743/        new_order = ops.tuple_reversed(perm)/
  %6(input) = %2(%para65_input, %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1744/        input = _get_cache_prim(P.Transpose)()(input, new_order)/
  %7(CNode_1265) = call @3↓flatten_1209(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1738/        x_rank = rank_(input)/
}
# Order:
#   1: @↓✓2↓flatten_1245:perm{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_range, [1]: ValueNode<Int64Imm> 0, [2]: x_rank}
#   2: @↓✓2↓flatten_1245:new_order{[0]: ValueNode<DoSignaturePrimitive> S_Prim_tuple_reversed, [1]: perm}
#   3: @↓✓2↓flatten_1245:CNode_1263{[0]: ValueNode<FuncGraph> _get_cache_prim_1262, [1]: ValueNode<ClassType> class 'mindspore.ops.operations.array_ops.Transpose'}
#   4: @↓✓2↓flatten_1245:CNode_1264{[0]: CNode_1263}
#   5: @↓✓2↓flatten_1245:input{[0]: CNode_1264, [1]: param_input, [2]: new_order}
#   6: @↓✓2↓flatten_1245:CNode_1266{[0]: ValueNode<Primitive> Return, [1]: CNode_1265}
#   7: @↓✓2↓flatten_1245:CNode_1265{[0]: ValueNode<FuncGraph> 3↓flatten_1209, [1]: input}


subgraph attr:
subgraph instance: ✓3↓flatten_1256 : 0x30912f418
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✓3↓flatten_1256 parent: [subgraph @3↓flatten_1209]() {
  %1(x_rank) = $(3↓flatten_1209):S_Prim_Rank(%para79_фinput)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1748/    x_rank = rank_(input)/
  %2(CNode_1267) = S_Prim_MakeTuple(I64(0), I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1750/        if x_rank in (0, 1):/
  %3(CNode_1268) = S_Prim_in(%1, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1750/        if x_rank in (0, 1):/
  %4(CNode_1269) = Cond(%3, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1750/        if x_rank in (0, 1):/
  %5(CNode_1270) = Switch(%4, @2✓3↓flatten_1271, @✗✓3↓flatten_1272)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1750/        if x_rank in (0, 1):/
  %6(CNode_1273) = %5()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1750/        if x_rank in (0, 1):/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1750/        if x_rank in (0, 1):/
}
# Order:
#   1: @✓3↓flatten_1256:CNode_1267{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: ValueNode<Int64Imm> 0, [2]: ValueNode<Int64Imm> 1}
#   2: @✓3↓flatten_1256:CNode_1268{[0]: ValueNode<DoSignaturePrimitive> S_Prim_in, [1]: x_rank, [2]: CNode_1267}
#   3: @✓3↓flatten_1256:CNode_1269{[0]: ValueNode<Primitive> Cond, [1]: CNode_1268, [2]: ValueNode<BoolImm> false}
#   4: @✓3↓flatten_1256:CNode_1270{[0]: ValueNode<Primitive> Switch, [1]: CNode_1269, [2]: ValueNode<FuncGraph> 2✓3↓flatten_1271, [3]: ValueNode<FuncGraph> ✗✓3↓flatten_1272}
#   5: @✓3↓flatten_1256:CNode_1273{[0]: CNode_1270}
#   6: @✓3↓flatten_1256:CNode_1274{[0]: ValueNode<Primitive> Return, [1]: CNode_1273}


subgraph attr:
subgraph instance: ✗3↓flatten_1257 : 0x309124e18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✗3↓flatten_1257 parent: [subgraph @3↓flatten_1209]() {
  %1(CNode_1276) = call @4↓flatten_1275()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
}
# Order:
#   1: @✗3↓flatten_1257:CNode_1276{[0]: ValueNode<FuncGraph> 4↓flatten_1275}
#   2: @✗3↓flatten_1257:CNode_1277{[0]: ValueNode<Primitive> Return, [1]: CNode_1276}


subgraph attr:
subgraph instance: ↰3↓flatten_1251 : 0x309124818
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @↰3↓flatten_1251 parent: [subgraph @flatten_951]() {
  %1(CNode_1278) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %2(CNode_1279) = S_Prim_equal(%para68_end_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
}
# Order:
#   1: @↰3↓flatten_1251:CNode_1278{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @↰3↓flatten_1251:CNode_1279{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: param_end_dim, [2]: CNode_1278}
#   3: @↰3↓flatten_1251:CNode_1280{[0]: ValueNode<Primitive> Return, [1]: CNode_1279}


subgraph attr:
subgraph instance: ↱3↓flatten_1252 : 0x309124218
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @↱3↓flatten_1252 parent: [subgraph @3↓flatten_1209]() {
  %1(CNode_1248) = $(3↓flatten_1209):S_Prim_equal(%para67_start_dim, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
}
# Order:
#   1: @↱3↓flatten_1252:CNode_1281{[0]: ValueNode<Primitive> Return, [1]: CNode_1248}


subgraph attr:
subgraph instance: _get_cache_prim_1262 : 0x309130618
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/_primitive_cache.py:35/def _get_cache_prim(cls: Primitive) -> Primitive:/
subgraph @_get_cache_prim_1262(%para81_cls) {
  %1(CNode_1283) = call @✓_get_cache_prim_1282()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/_primitive_cache.py:88/    if _is_need_compile(_temp_func): # @jit.cond: True/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/_primitive_cache.py:88/    if _is_need_compile(_temp_func): # @jit.cond: True/
}
# Order:
#   1: @_get_cache_prim_1262:CNode_1283{[0]: ValueNode<FuncGraph> ✓_get_cache_prim_1282}
#   2: @_get_cache_prim_1262:CNode_1284{[0]: ValueNode<Primitive> Return, [1]: CNode_1283}


subgraph attr:
subgraph instance: 2✓3↓flatten_1271 : 0x309131818
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @2✓3↓flatten_1271 parent: [subgraph @3↓flatten_1209]() {
  %1(CNode_1285) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1751/            return reshape_(input, (-1,))/
  %2(CNode_1286) = S_Prim_MakeTuple(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1751/            return reshape_(input, (-1,))/
  %3(CNode_1287) = S_Prim_Reshape[output_names: ["output"], input_names: ["tensor", "shape"]](%para79_фinput, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1751/            return reshape_(input, (-1,))/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1751/            return reshape_(input, (-1,))/
}
# Order:
#   1: @2✓3↓flatten_1271:CNode_1285{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @2✓3↓flatten_1271:CNode_1286{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_1285}
#   3: @2✓3↓flatten_1271:CNode_1287{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: param_фinput, [2]: CNode_1286}
#   4: @2✓3↓flatten_1271:CNode_1288{[0]: ValueNode<Primitive> Return, [1]: CNode_1287}


subgraph attr:
subgraph instance: ✗✓3↓flatten_1272 : 0x30912fa18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✗✓3↓flatten_1272 parent: [subgraph @3↓flatten_1209]() {
  %1(CNode_1290) = call @↓✓3↓flatten_1289()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1750/        if x_rank in (0, 1):/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1750/        if x_rank in (0, 1):/
}
# Order:
#   1: @✗✓3↓flatten_1272:CNode_1290{[0]: ValueNode<FuncGraph> ↓✓3↓flatten_1289}
#   2: @✗✓3↓flatten_1272:CNode_1291{[0]: ValueNode<Primitive> Return, [1]: CNode_1290}


subgraph attr:
after_block : 1
subgraph instance: 4↓flatten_1275 : 0x309125418
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @4↓flatten_1275 parent: [subgraph @3↓flatten_1209]() {
  %1(x_rank) = $(3↓flatten_1209):S_Prim_Rank(%para79_фinput)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1748/    x_rank = rank_(input)/
  %2(idx) = call @canonicalize_axis_1292(%para67_start_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1755/    start_dim = canonicalize_axis(start_dim, x_rank)/
  %3(end_dim) = call @canonicalize_axis_1292(%para68_end_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1756/    end_dim = canonicalize_axis(end_dim, x_rank)/
  %4(CNode_1294) = call @check_dim_valid_1293(%2, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1757/    check_dim_valid(start_dim, end_dim)/
  %5(CNode_1295) = StopGradient(%4)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
  %6(CNode_1296) = S_Prim_MakeTuple(I64(0), I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1759/    if x_rank in (0, 1):/
  %7(CNode_1297) = S_Prim_in(%1, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1759/    if x_rank in (0, 1):/
  %8(CNode_1298) = Cond(%7, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1759/    if x_rank in (0, 1):/
  %9(CNode_1299) = Switch(%8, @✓4↓flatten_1300, @✗4↓flatten_1301)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1759/    if x_rank in (0, 1):/
  %10(CNode_1302) = %9()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1759/    if x_rank in (0, 1):/
  %11(CNode_1303) = Depend[side_effect_propagate: I64(1)](%10, %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1759/    if x_rank in (0, 1):/
  Return(%11)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1759/    if x_rank in (0, 1):/
}
# Order:
#   1: @4↓flatten_1275:idx{[0]: ValueNode<FuncGraph> canonicalize_axis_1292, [1]: param_start_dim, [2]: x_rank}
#   2: @4↓flatten_1275:end_dim{[0]: ValueNode<FuncGraph> canonicalize_axis_1292, [1]: param_end_dim, [2]: x_rank}
#   3: @4↓flatten_1275:CNode_1294{[0]: ValueNode<FuncGraph> check_dim_valid_1293, [1]: idx, [2]: end_dim}
#   4: @4↓flatten_1275:CNode_1296{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: ValueNode<Int64Imm> 0, [2]: ValueNode<Int64Imm> 1}
#   5: @4↓flatten_1275:CNode_1297{[0]: ValueNode<DoSignaturePrimitive> S_Prim_in, [1]: x_rank, [2]: CNode_1296}
#   6: @4↓flatten_1275:CNode_1298{[0]: ValueNode<Primitive> Cond, [1]: CNode_1297, [2]: ValueNode<BoolImm> false}
#   7: @4↓flatten_1275:CNode_1299{[0]: ValueNode<Primitive> Switch, [1]: CNode_1298, [2]: ValueNode<FuncGraph> ✓4↓flatten_1300, [3]: ValueNode<FuncGraph> ✗4↓flatten_1301}
#   8: @4↓flatten_1275:CNode_1302{[0]: CNode_1299}
#   9: @4↓flatten_1275:CNode_1304{[0]: ValueNode<Primitive> Return, [1]: CNode_1303}


subgraph attr:
subgraph instance: ✓_get_cache_prim_1282 : 0x309130c18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/_primitive_cache.py:35/def _get_cache_prim(cls: Primitive) -> Primitive:/
subgraph @✓_get_cache_prim_1282 parent: [subgraph @_get_cache_prim_1262]() {
  Return(@_new_prim_for_graph_1305)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/_primitive_cache.py:89/        return _new_prim_for_graph/
}
# Order:
#   1: @✓_get_cache_prim_1282:CNode_1306{[0]: ValueNode<Primitive> Return, [1]: ValueNode<FuncGraph> _new_prim_for_graph_1305}


subgraph attr:
after_block : 1
subgraph instance: ↓✓3↓flatten_1289 : 0x309130018
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @↓✓3↓flatten_1289 parent: [subgraph @3↓flatten_1209]() {
  %1(CNode_1307) = call @_get_cache_prim_1262(ClassType)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1752/        return _get_cache_prim(P.Flatten)()(input)/
  %2(CNode_1308) = %1()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1752/        return _get_cache_prim(P.Flatten)()(input)/
  %3(CNode_1309) = %2(%para79_фinput)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1752/        return _get_cache_prim(P.Flatten)()(input)/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1752/        return _get_cache_prim(P.Flatten)()(input)/
}
# Order:
#   1: @↓✓3↓flatten_1289:CNode_1307{[0]: ValueNode<FuncGraph> _get_cache_prim_1262, [1]: ValueNode<ClassType> class 'mindspore.ops.operations.nn_ops.Flatten'}
#   2: @↓✓3↓flatten_1289:CNode_1308{[0]: CNode_1307}
#   3: @↓✓3↓flatten_1289:CNode_1309{[0]: CNode_1308, [1]: param_фinput}
#   4: @↓✓3↓flatten_1289:CNode_1310{[0]: ValueNode<Primitive> Return, [1]: CNode_1309}


subgraph attr:
subgraph instance: check_dim_valid_1293 : 0x30912dc18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1721/    def check_dim_valid(start_dim, end_dim):/
subgraph @check_dim_valid_1293(%para82_start_dim, %para83_end_dim) {
  %1(CNode_1311) = S_Prim_greater(%para82_start_dim, %para83_end_dim)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1722/        if start_dim > end_dim:/
  %2(CNode_1312) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1722/        if start_dim > end_dim:/
  %3(CNode_1313) = Switch(%2, @✓check_dim_valid_1314, @✗check_dim_valid_1315)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1722/        if start_dim > end_dim:/
  %4(CNode_1316) = %3()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1722/        if start_dim > end_dim:/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1722/        if start_dim > end_dim:/
}
# Order:
#   1: @check_dim_valid_1293:CNode_1311{[0]: ValueNode<DoSignaturePrimitive> S_Prim_greater, [1]: param_start_dim, [2]: param_end_dim}
#   2: @check_dim_valid_1293:CNode_1312{[0]: ValueNode<Primitive> Cond, [1]: CNode_1311, [2]: ValueNode<BoolImm> false}
#   3: @check_dim_valid_1293:CNode_1313{[0]: ValueNode<Primitive> Switch, [1]: CNode_1312, [2]: ValueNode<FuncGraph> ✓check_dim_valid_1314, [3]: ValueNode<FuncGraph> ✗check_dim_valid_1315}
#   4: @check_dim_valid_1293:CNode_1316{[0]: CNode_1313}
#   5: @check_dim_valid_1293:CNode_1317{[0]: ValueNode<Primitive> Return, [1]: CNode_1316}


subgraph attr:
subgraph instance: canonicalize_axis_1292 : 0x309129a18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1725/    def canonicalize_axis(axis, x_rank):/
subgraph @canonicalize_axis_1292(%para84_axis, %para85_x_rank) {
  %1(CNode_1318) = S_Prim_not_equal(%para85_x_rank, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %2(CNode_1319) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %3(CNode_1320) = Switch(%2, @↰canonicalize_axis_1321, @↱canonicalize_axis_1322)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %4(ndim) = %3()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %5(CNode_1324) = call @check_axis_valid_1323(%para84_axis, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1727/        check_axis_valid(axis, ndim)/
  %6(CNode_1325) = StopGradient(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1725/    def canonicalize_axis(axis, x_rank):/
  %7(CNode_1326) = S_Prim_greater_equal(%para84_axis, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
  %8(CNode_1327) = Cond(%7, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
  %9(CNode_1328) = Switch(%8, @↰canonicalize_axis_1329, @↱canonicalize_axis_1330)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
  %10(CNode_1331) = %9()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
  %11(CNode_1332) = Depend[side_effect_propagate: I64(1)](%10, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
  Return(%11)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
}
# Order:
#   1: @canonicalize_axis_1292:CNode_1318{[0]: ValueNode<DoSignaturePrimitive> S_Prim_not_equal, [1]: param_x_rank, [2]: ValueNode<Int64Imm> 0}
#   2: @canonicalize_axis_1292:CNode_1319{[0]: ValueNode<Primitive> Cond, [1]: CNode_1318, [2]: ValueNode<BoolImm> false}
#   3: @canonicalize_axis_1292:CNode_1320{[0]: ValueNode<Primitive> Switch, [1]: CNode_1319, [2]: ValueNode<FuncGraph> ↰canonicalize_axis_1321, [3]: ValueNode<FuncGraph> ↱canonicalize_axis_1322}
#   4: @canonicalize_axis_1292:ndim{[0]: CNode_1320}
#   5: @canonicalize_axis_1292:CNode_1324{[0]: ValueNode<FuncGraph> check_axis_valid_1323, [1]: param_axis, [2]: ndim}
#   6: @canonicalize_axis_1292:CNode_1326{[0]: ValueNode<DoSignaturePrimitive> S_Prim_greater_equal, [1]: param_axis, [2]: ValueNode<Int64Imm> 0}
#   7: @canonicalize_axis_1292:CNode_1327{[0]: ValueNode<Primitive> Cond, [1]: CNode_1326, [2]: ValueNode<BoolImm> false}
#   8: @canonicalize_axis_1292:CNode_1328{[0]: ValueNode<Primitive> Switch, [1]: CNode_1327, [2]: ValueNode<FuncGraph> ↰canonicalize_axis_1329, [3]: ValueNode<FuncGraph> ↱canonicalize_axis_1330}
#   9: @canonicalize_axis_1292:CNode_1331{[0]: CNode_1328}
#  10: @canonicalize_axis_1292:CNode_1333{[0]: ValueNode<Primitive> Return, [1]: CNode_1332}


subgraph attr:
subgraph instance: ✓4↓flatten_1300 : 0x30910b018
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✓4↓flatten_1300 parent: [subgraph @3↓flatten_1209]() {
  %1(CNode_1334) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1760/        return reshape_(input, (-1,))/
  %2(CNode_1335) = S_Prim_MakeTuple(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1760/        return reshape_(input, (-1,))/
  %3(CNode_1336) = S_Prim_Reshape[output_names: ["output"], input_names: ["tensor", "shape"]](%para79_фinput, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1760/        return reshape_(input, (-1,))/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1760/        return reshape_(input, (-1,))/
}
# Order:
#   1: @✓4↓flatten_1300:CNode_1334{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @✓4↓flatten_1300:CNode_1335{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_1334}
#   3: @✓4↓flatten_1300:CNode_1336{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: param_фinput, [2]: CNode_1335}
#   4: @✓4↓flatten_1300:CNode_1337{[0]: ValueNode<Primitive> Return, [1]: CNode_1336}


subgraph attr:
subgraph instance: ✗4↓flatten_1301 : 0x309125a18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✗4↓flatten_1301 parent: [subgraph @4↓flatten_1275]() {
  %1(CNode_1339) = call @5↓flatten_1338()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1759/    if x_rank in (0, 1):/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1759/    if x_rank in (0, 1):/
}
# Order:
#   1: @✗4↓flatten_1301:CNode_1339{[0]: ValueNode<FuncGraph> 5↓flatten_1338}
#   2: @✗4↓flatten_1301:CNode_1340{[0]: ValueNode<Primitive> Return, [1]: CNode_1339}


subgraph attr:
subgraph instance: _new_prim_for_graph_1305 : 0x309131218
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/_primitive_cache.py:67/    def _new_prim_for_graph(*args, **kwargs) -> Primitive:/
subgraph @_new_prim_for_graph_1305 parent: [subgraph @_get_cache_prim_1262](%para86_args, %para87_kwargs) {
  %1(CNode_1341) = UnpackCall_unpack_call(%para81_cls, %para86_args, %para87_kwargs)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/_primitive_cache.py:68/        return cls(*args, **kwargs)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/_primitive_cache.py:68/        return cls(*args, **kwargs)/
}
# Order:
#   1: @_new_prim_for_graph_1305:CNode_1341{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.1342, [1]: param_cls, [2]: param_args, [3]: param_kwargs}
#   2: @_new_prim_for_graph_1305:CNode_1343{[0]: ValueNode<Primitive> Return, [1]: CNode_1341}


subgraph attr:
subgraph instance: ✓check_dim_valid_1314 : 0x30912ee18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1721/    def check_dim_valid(start_dim, end_dim):/
subgraph @✓check_dim_valid_1314() {
  %1(CNode_1344) = raise[side_effect_io: Bool(1)]("ValueError", "For 'flatten', 'start_dim' cannot come after 'end_dim'.", "None")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1723/            raise ValueError("For 'flatten', 'start_dim' cannot come after 'end_dim'.")/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1723/            raise ValueError("For 'flatten', 'start_dim' cannot come after 'end_dim'.")/
}
# Order:
#   1: @✓check_dim_valid_1314:CNode_1344{[0]: ValueNode<Primitive> raise, [1]: ValueNode<StringImm> ValueError, [2]: ValueNode<StringImm> For 'flatten', 'start_dim' cannot come after 'end_dim'., [3]: ValueNode<StringImm> None}
#   2: @✓check_dim_valid_1314:CNode_1345{[0]: ValueNode<Primitive> Return, [1]: CNode_1344}


subgraph attr:
subgraph instance: ✗check_dim_valid_1315 : 0x30912e218
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1721/    def check_dim_valid(start_dim, end_dim):/
subgraph @✗check_dim_valid_1315() {
  %1(CNode_1347) = call @↓check_dim_valid_1346()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1722/        if start_dim > end_dim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1722/        if start_dim > end_dim:/
}
# Order:
#   1: @✗check_dim_valid_1315:CNode_1347{[0]: ValueNode<FuncGraph> ↓check_dim_valid_1346}
#   2: @✗check_dim_valid_1315:CNode_1348{[0]: ValueNode<Primitive> Return, [1]: CNode_1347}


subgraph attr:
subgraph instance: check_axis_valid_1323 : 0x30912b818
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1717/    def check_axis_valid(axis, ndim):/
subgraph @check_axis_valid_1323(%para88_axis, %para89_ndim) {
  %1(CNode_1349) = S_Prim_negative(%para89_ndim)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %2(CNode_1350) = S_Prim_less(%para88_axis, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %3(CNode_1351) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %4(CNode_1352) = Switch(%3, @↰check_axis_valid_1353, @↱check_axis_valid_1354)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %5(CNode_1355) = %4()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %6(CNode_1356) = Cond(%5, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %7(CNode_1357) = Switch(%6, @✓check_axis_valid_1358, @✗check_axis_valid_1359)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %8(CNode_1360) = %7()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @check_axis_valid_1323:CNode_1349{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: param_ndim}
#   2: @check_axis_valid_1323:CNode_1350{[0]: ValueNode<DoSignaturePrimitive> S_Prim_less, [1]: param_axis, [2]: CNode_1349}
#   3: @check_axis_valid_1323:CNode_1351{[0]: ValueNode<Primitive> Cond, [1]: CNode_1350, [2]: ValueNode<BoolImm> false}
#   4: @check_axis_valid_1323:CNode_1352{[0]: ValueNode<Primitive> Switch, [1]: CNode_1351, [2]: ValueNode<FuncGraph> ↰check_axis_valid_1353, [3]: ValueNode<FuncGraph> ↱check_axis_valid_1354}
#   5: @check_axis_valid_1323:CNode_1355{[0]: CNode_1352}
#   6: @check_axis_valid_1323:CNode_1356{[0]: ValueNode<Primitive> Cond, [1]: CNode_1355, [2]: ValueNode<BoolImm> false}
#   7: @check_axis_valid_1323:CNode_1357{[0]: ValueNode<Primitive> Switch, [1]: CNode_1356, [2]: ValueNode<FuncGraph> ✓check_axis_valid_1358, [3]: ValueNode<FuncGraph> ✗check_axis_valid_1359}
#   8: @check_axis_valid_1323:CNode_1360{[0]: CNode_1357}
#   9: @check_axis_valid_1323:CNode_1361{[0]: ValueNode<Primitive> Return, [1]: CNode_1360}


subgraph attr:
subgraph instance: ↰canonicalize_axis_1321 : 0x30912b218
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
subgraph @↰canonicalize_axis_1321 parent: [subgraph @canonicalize_axis_1292]() {
  Return(%para85_x_rank)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
}
# Order:
#   1: @↰canonicalize_axis_1321:CNode_1362{[0]: ValueNode<Primitive> Return, [1]: param_x_rank}


subgraph attr:
subgraph instance: ↱canonicalize_axis_1322 : 0x30912ac18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
subgraph @↱canonicalize_axis_1322() {
  Return(I64(1))
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
}
# Order:
#   1: @↱canonicalize_axis_1322:CNode_1363{[0]: ValueNode<Primitive> Return, [1]: ValueNode<Int64Imm> 1}


subgraph attr:
subgraph instance: ↰canonicalize_axis_1329 : 0x30912a618
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
subgraph @↰canonicalize_axis_1329 parent: [subgraph @canonicalize_axis_1292]() {
  Return(%para84_axis)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
}
# Order:
#   1: @↰canonicalize_axis_1329:CNode_1364{[0]: ValueNode<Primitive> Return, [1]: param_axis}


subgraph attr:
subgraph instance: ↱canonicalize_axis_1330 : 0x30912a018
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
subgraph @↱canonicalize_axis_1330 parent: [subgraph @canonicalize_axis_1292]() {
  %1(CNode_1318) = $(canonicalize_axis_1292):S_Prim_not_equal(%para85_x_rank, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %2(CNode_1319) = $(canonicalize_axis_1292):Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %3(CNode_1320) = $(canonicalize_axis_1292):Switch(%2, @↰canonicalize_axis_1321, @↱canonicalize_axis_1322)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %4(ndim) = $(canonicalize_axis_1292):%3()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %5(CNode_1365) = S_Prim_add(%para84_axis, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
}
# Order:
#   1: @↱canonicalize_axis_1330:CNode_1365{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: param_axis, [2]: ndim}
#   2: @↱canonicalize_axis_1330:CNode_1366{[0]: ValueNode<Primitive> Return, [1]: CNode_1365}


subgraph attr:
after_block : 1
subgraph instance: 5↓flatten_1338 : 0x309126018
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @5↓flatten_1338 parent: [subgraph @4↓flatten_1275]() {
  %1(x_rank) = $(3↓flatten_1209):S_Prim_Rank(%para79_фinput)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1748/    x_rank = rank_(input)/
  %2(idx) = $(4↓flatten_1275):call @canonicalize_axis_1292(%para67_start_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1755/    start_dim = canonicalize_axis(start_dim, x_rank)/
  %3(end_dim) = $(4↓flatten_1275):call @canonicalize_axis_1292(%para68_end_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1756/    end_dim = canonicalize_axis(end_dim, x_rank)/
  %4(CNode_1367) = S_Prim_equal(%2, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1762/    if start_dim == end_dim:/
  %5(CNode_1368) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1762/    if start_dim == end_dim:/
  %6(CNode_1369) = Switch(%5, @✓5↓flatten_1370, @✗5↓flatten_1371)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1762/    if start_dim == end_dim:/
  %7(CNode_1372) = %6()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1762/    if start_dim == end_dim:/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1762/    if start_dim == end_dim:/
}
# Order:
#   1: @5↓flatten_1338:CNode_1367{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: idx, [2]: end_dim}
#   2: @5↓flatten_1338:CNode_1368{[0]: ValueNode<Primitive> Cond, [1]: CNode_1367, [2]: ValueNode<BoolImm> false}
#   3: @5↓flatten_1338:CNode_1369{[0]: ValueNode<Primitive> Switch, [1]: CNode_1368, [2]: ValueNode<FuncGraph> ✓5↓flatten_1370, [3]: ValueNode<FuncGraph> ✗5↓flatten_1371}
#   4: @5↓flatten_1338:CNode_1372{[0]: CNode_1369}
#   5: @5↓flatten_1338:CNode_1373{[0]: ValueNode<Primitive> Return, [1]: CNode_1372}


subgraph attr:
after_block : 1
subgraph instance: ↓check_dim_valid_1346 : 0x30912e818
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1721/    def check_dim_valid(start_dim, end_dim):/
subgraph @↓check_dim_valid_1346() {
  Return(None)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
}
# Order:
#   1: @↓check_dim_valid_1346:CNode_1374{[0]: ValueNode<Primitive> Return, [1]: ValueNode<None> None}


subgraph attr:
subgraph instance: ✓check_axis_valid_1358 : 0x30912d618
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1717/    def check_axis_valid(axis, ndim):/
subgraph @✓check_axis_valid_1358() {
  %1(CNode_1375) = raise[side_effect_io: Bool(1)]("ValueError", "'start_dim' or 'end_dim' out of range.", "None")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1719/            raise ValueError("'start_dim' or 'end_dim' out of range.")/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1719/            raise ValueError("'start_dim' or 'end_dim' out of range.")/
}
# Order:
#   1: @✓check_axis_valid_1358:CNode_1375{[0]: ValueNode<Primitive> raise, [1]: ValueNode<StringImm> ValueError, [2]: ValueNode<StringImm> 'start_dim' or 'end_dim' out of range., [3]: ValueNode<StringImm> None}
#   2: @✓check_axis_valid_1358:CNode_1376{[0]: ValueNode<Primitive> Return, [1]: CNode_1375}


subgraph attr:
subgraph instance: ✗check_axis_valid_1359 : 0x30912ca18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1717/    def check_axis_valid(axis, ndim):/
subgraph @✗check_axis_valid_1359() {
  %1(CNode_1378) = call @↓check_axis_valid_1377()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @✗check_axis_valid_1359:CNode_1378{[0]: ValueNode<FuncGraph> ↓check_axis_valid_1377}
#   2: @✗check_axis_valid_1359:CNode_1379{[0]: ValueNode<Primitive> Return, [1]: CNode_1378}


subgraph attr:
subgraph instance: ↰check_axis_valid_1353 : 0x30912c418
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1717/    def check_axis_valid(axis, ndim):/
subgraph @↰check_axis_valid_1353 parent: [subgraph @check_axis_valid_1323]() {
  %1(CNode_1349) = $(check_axis_valid_1323):S_Prim_negative(%para89_ndim)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %2(CNode_1350) = $(check_axis_valid_1323):S_Prim_less(%para88_axis, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @↰check_axis_valid_1353:CNode_1380{[0]: ValueNode<Primitive> Return, [1]: CNode_1350}


subgraph attr:
subgraph instance: ↱check_axis_valid_1354 : 0x30912be18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1717/    def check_axis_valid(axis, ndim):/
subgraph @↱check_axis_valid_1354 parent: [subgraph @check_axis_valid_1323]() {
  %1(CNode_1381) = S_Prim_greater_equal(%para88_axis, %para89_ndim)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1718/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @↱check_axis_valid_1354:CNode_1381{[0]: ValueNode<DoSignaturePrimitive> S_Prim_greater_equal, [1]: param_axis, [2]: param_ndim}
#   2: @↱check_axis_valid_1354:CNode_1382{[0]: ValueNode<Primitive> Return, [1]: CNode_1381}


subgraph attr:
subgraph instance: ✓5↓flatten_1370 : 0x30910aa18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✓5↓flatten_1370 parent: [subgraph @3↓flatten_1209]() {
  Return(%para79_фinput)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1763/        return input/
}
# Order:
#   1: @✓5↓flatten_1370:CNode_1383{[0]: ValueNode<Primitive> Return, [1]: param_фinput}


subgraph attr:
subgraph instance: ✗5↓flatten_1371 : 0x309126618
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @✗5↓flatten_1371 parent: [subgraph @4↓flatten_1275]() {
  %1(CNode_1385) = call @6↓flatten_1384()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1762/    if start_dim == end_dim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1762/    if start_dim == end_dim:/
}
# Order:
#   1: @✗5↓flatten_1371:CNode_1385{[0]: ValueNode<FuncGraph> 6↓flatten_1384}
#   2: @✗5↓flatten_1371:CNode_1386{[0]: ValueNode<Primitive> Return, [1]: CNode_1385}


subgraph attr:
after_block : 1
subgraph instance: ↓check_axis_valid_1377 : 0x30912d018
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1717/    def check_axis_valid(axis, ndim):/
subgraph @↓check_axis_valid_1377() {
  Return(None)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
}
# Order:
#   1: @↓check_axis_valid_1377:CNode_1387{[0]: ValueNode<Primitive> Return, [1]: ValueNode<None> None}


subgraph attr:
after_block : 1
subgraph instance: 6↓flatten_1384 : 0x309126c18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @6↓flatten_1384 parent: [subgraph @4↓flatten_1275]() {
  %1(x_rank) = $(3↓flatten_1209):S_Prim_Rank(%para79_фinput)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1748/    x_rank = rank_(input)/
  %2(idx) = $(4↓flatten_1275):call @canonicalize_axis_1292(%para67_start_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1755/    start_dim = canonicalize_axis(start_dim, x_rank)/
  %3(CNode_1389) = call @↵6↓flatten_1388(%2, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1767/    while idx <= end_dim:/
}
# Order:
#   1: @6↓flatten_1384:CNode_1390{[0]: ValueNode<Primitive> Return, [1]: CNode_1389}
#   2: @6↓flatten_1384:CNode_1389{[0]: ValueNode<FuncGraph> ↵6↓flatten_1388, [1]: idx, [2]: ValueNode<Int64Imm> 1}


subgraph attr:
is_while_header : 1
subgraph instance: ↵6↓flatten_1388 : 0x309127218
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @↵6↓flatten_1388 parent: [subgraph @4↓flatten_1275](%para90_, %para91_) {
  %1(x_rank) = $(3↓flatten_1209):S_Prim_Rank(%para79_фinput)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1748/    x_rank = rank_(input)/
  %2(end_dim) = $(4↓flatten_1275):call @canonicalize_axis_1292(%para68_end_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1756/    end_dim = canonicalize_axis(end_dim, x_rank)/
  %3(CNode_1391) = S_Prim_less_equal(%para90_фidx, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1767/    while idx <= end_dim:/
  %4(force_while_cond_CNode_1391) = Cond(%3, Bool(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1767/    while idx <= end_dim:/
  %5(CNode_1392) = Switch(%4, @↻6↓flatten_1393, @7↓flatten_1394)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1767/    while idx <= end_dim:/
  %6(CNode_1395) = %5()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1767/    while idx <= end_dim:/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1767/    while idx <= end_dim:/
}
# Order:
#   1: @↵6↓flatten_1388:CNode_1391{[0]: ValueNode<DoSignaturePrimitive> S_Prim_less_equal, [1]: param_фidx, [2]: end_dim}
#   2: @↵6↓flatten_1388:force_while_cond_CNode_1391{[0]: ValueNode<Primitive> Cond, [1]: CNode_1391, [2]: ValueNode<BoolImm> true}
#   3: @↵6↓flatten_1388:CNode_1392{[0]: ValueNode<Primitive> Switch, [1]: force_while_cond_CNode_1391, [2]: ValueNode<FuncGraph> ↻6↓flatten_1393, [3]: ValueNode<FuncGraph> 7↓flatten_1394}
#   4: @↵6↓flatten_1388:CNode_1395{[0]: CNode_1392}
#   5: @↵6↓flatten_1388:CNode_1396{[0]: ValueNode<Primitive> Return, [1]: CNode_1395}


subgraph attr:
subgraph instance: ↻6↓flatten_1393 : 0x309129418
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @↻6↓flatten_1393 parent: [subgraph @↵6↓flatten_1388]() {
  %1(idx) = S_Prim_add(%para90_фidx, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1769/        idx += 1/
  %2(x_shape) = $(3↓flatten_1209):S_Prim_Shape(%para79_фinput)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1747/    x_shape = shape_(input)/
  %3(CNode_1397) = S_Prim_getitem(%2, %para90_фidx)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1768/        dim_length *= x_shape[idx]/
  %4(dim_length) = S_Prim_mul(%para91_фdim_length, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1768/        dim_length *= x_shape[idx]/
  %5(CNode_1398) = call @↵6↓flatten_1388(%1, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1767/    while idx <= end_dim:/
}
# Order:
#   1: @↻6↓flatten_1393:CNode_1397{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: x_shape, [2]: param_фidx}
#   2: @↻6↓flatten_1393:dim_length{[0]: ValueNode<DoSignaturePrimitive> S_Prim_mul, [1]: param_фdim_length, [2]: CNode_1397}
#   3: @↻6↓flatten_1393:idx{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: param_фidx, [2]: ValueNode<Int64Imm> 1}
#   4: @↻6↓flatten_1393:CNode_1399{[0]: ValueNode<Primitive> Return, [1]: CNode_1398}
#   5: @↻6↓flatten_1393:CNode_1398{[0]: ValueNode<FuncGraph> ↵6↓flatten_1388, [1]: idx, [2]: dim_length}


subgraph attr:
subgraph instance: 7↓flatten_1394 : 0x309123a18
# In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @7↓flatten_1394 parent: [subgraph @↵6↓flatten_1388]() {
  %1(x_shape) = $(3↓flatten_1209):S_Prim_Shape(%para79_фinput)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1747/    x_shape = shape_(input)/
  %2(x_rank) = $(3↓flatten_1209):S_Prim_Rank(%para79_фinput)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1748/    x_rank = rank_(input)/
  %3(idx) = $(4↓flatten_1275):call @canonicalize_axis_1292(%para67_start_dim, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1755/    start_dim = canonicalize_axis(start_dim, x_rank)/
  %4(CNode_1400) = S_Prim_make_slice(None, %3, None)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %5(CNode_1401) = S_Prim_getitem(%1, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %6(CNode_1402) = S_Prim_MakeTuple(%para91_фdim_length)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %7(CNode_1403) = S_Prim_add(%5, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %8(end_dim) = $(4↓flatten_1275):call @canonicalize_axis_1292(%para68_end_dim, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1756/    end_dim = canonicalize_axis(end_dim, x_rank)/
  %9(CNode_1404) = S_Prim_add(%8, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %10(CNode_1405) = S_Prim_make_slice(%9, None, None)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %11(CNode_1406) = S_Prim_getitem(%1, %10)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %12(new_shape) = S_Prim_add(%7, %11)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %13(CNode_1407) = S_Prim_Reshape[output_names: ["output"], input_names: ["tensor", "shape"]](%para79_фinput, %12)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1771/    return reshape_(input, new_shape)/
  Return(%13)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file /opt/anaconda3/envs/py39/lib/python3.9/site-packages/mindspore/ops/function/array_func.py:1771/    return reshape_(input, new_shape)/
}
# Order:
#   1: @7↓flatten_1394:CNode_1400{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_slice, [1]: ValueNode<None> None, [2]: idx, [3]: ValueNode<None> None}
#   2: @7↓flatten_1394:CNode_1401{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: x_shape, [2]: CNode_1400}
#   3: @7↓flatten_1394:CNode_1402{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: param_фdim_length}
#   4: @7↓flatten_1394:CNode_1403{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: CNode_1401, [2]: CNode_1402}
#   5: @7↓flatten_1394:CNode_1404{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: end_dim, [2]: ValueNode<Int64Imm> 1}
#   6: @7↓flatten_1394:CNode_1405{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_slice, [1]: CNode_1404, [2]: ValueNode<None> None, [3]: ValueNode<None> None}
#   7: @7↓flatten_1394:CNode_1406{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: x_shape, [2]: CNode_1405}
#   8: @7↓flatten_1394:new_shape{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: CNode_1403, [2]: CNode_1406}
#   9: @7↓flatten_1394:CNode_1407{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: param_фinput, [2]: new_shape}
#  10: @7↓flatten_1394:CNode_1408{[0]: ValueNode<Primitive> Return, [1]: CNode_1407}


